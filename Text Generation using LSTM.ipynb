{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import tensorflow as tf\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Stories\n",
    "Stories are automatically downloaded from https://www.cs.cmu.edu/~spok/grimmtmp/, if not detected in the disk. The total size of stories is around ~500KB. The dataset consists of 100 stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file:  stories/001.txt\n",
      "file  001.txt  already exits\n",
      "Downloading file:  stories/002.txt\n",
      "file  002.txt  already exits\n",
      "Downloading file:  stories/003.txt\n",
      "file  003.txt  already exits\n",
      "Downloading file:  stories/004.txt\n",
      "file  004.txt  already exits\n",
      "Downloading file:  stories/005.txt\n",
      "file  005.txt  already exits\n",
      "Downloading file:  stories/006.txt\n",
      "file  006.txt  already exits\n",
      "Downloading file:  stories/007.txt\n",
      "file  007.txt  already exits\n",
      "Downloading file:  stories/008.txt\n",
      "file  008.txt  already exits\n",
      "Downloading file:  stories/009.txt\n",
      "file  009.txt  already exits\n",
      "Downloading file:  stories/010.txt\n",
      "file  010.txt  already exits\n",
      "Downloading file:  stories/011.txt\n",
      "file  011.txt  already exits\n",
      "Downloading file:  stories/012.txt\n",
      "file  012.txt  already exits\n",
      "Downloading file:  stories/013.txt\n",
      "file  013.txt  already exits\n",
      "Downloading file:  stories/014.txt\n",
      "file  014.txt  already exits\n",
      "Downloading file:  stories/015.txt\n",
      "file  015.txt  already exits\n",
      "Downloading file:  stories/016.txt\n",
      "file  016.txt  already exits\n",
      "Downloading file:  stories/017.txt\n",
      "file  017.txt  already exits\n",
      "Downloading file:  stories/018.txt\n",
      "file  018.txt  already exits\n",
      "Downloading file:  stories/019.txt\n",
      "file  019.txt  already exits\n",
      "Downloading file:  stories/020.txt\n",
      "file  020.txt  already exits\n",
      "Downloading file:  stories/021.txt\n",
      "file  021.txt  already exits\n",
      "Downloading file:  stories/022.txt\n",
      "file  022.txt  already exits\n",
      "Downloading file:  stories/023.txt\n",
      "file  023.txt  already exits\n",
      "Downloading file:  stories/024.txt\n",
      "file  024.txt  already exits\n",
      "Downloading file:  stories/025.txt\n",
      "file  025.txt  already exits\n",
      "Downloading file:  stories/026.txt\n",
      "file  026.txt  already exits\n",
      "Downloading file:  stories/027.txt\n",
      "file  027.txt  already exits\n",
      "Downloading file:  stories/028.txt\n",
      "file  028.txt  already exits\n",
      "Downloading file:  stories/029.txt\n",
      "file  029.txt  already exits\n",
      "Downloading file:  stories/030.txt\n",
      "file  030.txt  already exits\n",
      "Downloading file:  stories/031.txt\n",
      "file  031.txt  already exits\n",
      "Downloading file:  stories/032.txt\n",
      "file  032.txt  already exits\n",
      "Downloading file:  stories/033.txt\n",
      "file  033.txt  already exits\n",
      "Downloading file:  stories/034.txt\n",
      "file  034.txt  already exits\n",
      "Downloading file:  stories/035.txt\n",
      "file  035.txt  already exits\n",
      "Downloading file:  stories/036.txt\n",
      "file  036.txt  already exits\n",
      "Downloading file:  stories/037.txt\n",
      "file  037.txt  already exits\n",
      "Downloading file:  stories/038.txt\n",
      "file  038.txt  already exits\n",
      "Downloading file:  stories/039.txt\n",
      "file  039.txt  already exits\n",
      "Downloading file:  stories/040.txt\n",
      "file  040.txt  already exits\n",
      "Downloading file:  stories/041.txt\n",
      "file  041.txt  already exits\n",
      "Downloading file:  stories/042.txt\n",
      "file  042.txt  already exits\n",
      "Downloading file:  stories/043.txt\n",
      "file  043.txt  already exits\n",
      "Downloading file:  stories/044.txt\n",
      "file  044.txt  already exits\n",
      "Downloading file:  stories/045.txt\n",
      "file  045.txt  already exits\n",
      "Downloading file:  stories/046.txt\n",
      "file  046.txt  already exits\n",
      "Downloading file:  stories/047.txt\n",
      "file  047.txt  already exits\n",
      "Downloading file:  stories/048.txt\n",
      "file  048.txt  already exits\n",
      "Downloading file:  stories/049.txt\n",
      "file  049.txt  already exits\n",
      "Downloading file:  stories/050.txt\n",
      "file  050.txt  already exits\n",
      "Downloading file:  stories/051.txt\n",
      "file  051.txt  already exits\n",
      "Downloading file:  stories/052.txt\n",
      "file  052.txt  already exits\n",
      "Downloading file:  stories/053.txt\n",
      "file  053.txt  already exits\n",
      "Downloading file:  stories/054.txt\n",
      "file  054.txt  already exits\n",
      "Downloading file:  stories/055.txt\n",
      "file  055.txt  already exits\n",
      "Downloading file:  stories/056.txt\n",
      "file  056.txt  already exits\n",
      "Downloading file:  stories/057.txt\n",
      "file  057.txt  already exits\n",
      "Downloading file:  stories/058.txt\n",
      "file  058.txt  already exits\n",
      "Downloading file:  stories/059.txt\n",
      "file  059.txt  already exits\n",
      "Downloading file:  stories/060.txt\n",
      "file  060.txt  already exits\n",
      "Downloading file:  stories/061.txt\n",
      "file  061.txt  already exits\n",
      "Downloading file:  stories/062.txt\n",
      "file  062.txt  already exits\n",
      "Downloading file:  stories/063.txt\n",
      "file  063.txt  already exits\n",
      "Downloading file:  stories/064.txt\n",
      "file  064.txt  already exits\n",
      "Downloading file:  stories/065.txt\n",
      "file  065.txt  already exits\n",
      "Downloading file:  stories/066.txt\n",
      "file  066.txt  already exits\n",
      "Downloading file:  stories/067.txt\n",
      "file  067.txt  already exits\n",
      "Downloading file:  stories/068.txt\n",
      "file  068.txt  already exits\n",
      "Downloading file:  stories/069.txt\n",
      "file  069.txt  already exits\n",
      "Downloading file:  stories/070.txt\n",
      "file  070.txt  already exits\n",
      "Downloading file:  stories/071.txt\n",
      "file  071.txt  already exits\n",
      "Downloading file:  stories/072.txt\n",
      "file  072.txt  already exits\n",
      "Downloading file:  stories/073.txt\n",
      "file  073.txt  already exits\n",
      "Downloading file:  stories/074.txt\n",
      "file  074.txt  already exits\n",
      "Downloading file:  stories/075.txt\n",
      "file  075.txt  already exits\n",
      "Downloading file:  stories/076.txt\n",
      "file  076.txt  already exits\n",
      "Downloading file:  stories/077.txt\n",
      "file  077.txt  already exits\n",
      "Downloading file:  stories/078.txt\n",
      "file  078.txt  already exits\n",
      "Downloading file:  stories/079.txt\n",
      "file  079.txt  already exits\n",
      "Downloading file:  stories/080.txt\n",
      "file  080.txt  already exits\n",
      "Downloading file:  stories/081.txt\n",
      "file  081.txt  already exits\n",
      "Downloading file:  stories/082.txt\n",
      "file  082.txt  already exits\n",
      "Downloading file:  stories/083.txt\n",
      "file  083.txt  already exits\n",
      "Downloading file:  stories/084.txt\n",
      "file  084.txt  already exits\n",
      "Downloading file:  stories/085.txt\n",
      "file  085.txt  already exits\n",
      "Downloading file:  stories/086.txt\n",
      "file  086.txt  already exits\n",
      "Downloading file:  stories/087.txt\n",
      "file  087.txt  already exits\n",
      "Downloading file:  stories/088.txt\n",
      "file  088.txt  already exits\n",
      "Downloading file:  stories/089.txt\n",
      "file  089.txt  already exits\n",
      "Downloading file:  stories/090.txt\n",
      "file  090.txt  already exits\n",
      "Downloading file:  stories/091.txt\n",
      "file  091.txt  already exits\n",
      "Downloading file:  stories/092.txt\n",
      "file  092.txt  already exits\n",
      "Downloading file:  stories/093.txt\n",
      "file  093.txt  already exits\n",
      "Downloading file:  stories/094.txt\n",
      "file  094.txt  already exits\n",
      "Downloading file:  stories/095.txt\n",
      "file  095.txt  already exits\n",
      "Downloading file:  stories/096.txt\n",
      "file  096.txt  already exits\n",
      "Downloading file:  stories/097.txt\n",
      "file  097.txt  already exits\n",
      "Downloading file:  stories/098.txt\n",
      "file  098.txt  already exits\n",
      "Downloading file:  stories/099.txt\n",
      "file  099.txt  already exits\n",
      "Downloading file:  stories/100.txt\n",
      "file  100.txt  already exits\n",
      "Downloading file:  stories/101.txt\n",
      "file  101.txt  already exits\n",
      "Downloading file:  stories/102.txt\n",
      "file  102.txt  already exits\n",
      "Downloading file:  stories/103.txt\n",
      "file  103.txt  already exits\n",
      "Downloading file:  stories/104.txt\n",
      "file  104.txt  already exits\n",
      "Downloading file:  stories/105.txt\n",
      "file  105.txt  already exits\n",
      "Downloading file:  stories/106.txt\n",
      "file  106.txt  already exits\n",
      "Downloading file:  stories/107.txt\n",
      "file  107.txt  already exits\n",
      "Downloading file:  stories/108.txt\n",
      "file  108.txt  already exits\n",
      "Downloading file:  stories/109.txt\n",
      "file  109.txt  already exits\n",
      "Downloading file:  stories/110.txt\n",
      "file  110.txt  already exits\n",
      "Downloading file:  stories/111.txt\n",
      "file  111.txt  already exits\n",
      "Downloading file:  stories/112.txt\n",
      "file  112.txt  already exits\n",
      "Downloading file:  stories/113.txt\n",
      "file  113.txt  already exits\n",
      "Downloading file:  stories/114.txt\n",
      "file  114.txt  already exits\n",
      "Downloading file:  stories/115.txt\n",
      "file  115.txt  already exits\n",
      "Downloading file:  stories/116.txt\n",
      "file  116.txt  already exits\n",
      "Downloading file:  stories/117.txt\n",
      "file  117.txt  already exits\n",
      "Downloading file:  stories/118.txt\n",
      "file  118.txt  already exits\n",
      "Downloading file:  stories/119.txt\n",
      "file  119.txt  already exits\n",
      "Downloading file:  stories/120.txt\n",
      "file  120.txt  already exits\n",
      "Downloading file:  stories/121.txt\n",
      "file  121.txt  already exits\n",
      "Downloading file:  stories/122.txt\n",
      "file  122.txt  already exits\n",
      "Downloading file:  stories/123.txt\n",
      "file  123.txt  already exits\n",
      "Downloading file:  stories/124.txt\n",
      "file  124.txt  already exits\n",
      "Downloading file:  stories/125.txt\n",
      "file  125.txt  already exits\n",
      "Downloading file:  stories/126.txt\n",
      "file  126.txt  already exits\n",
      "Downloading file:  stories/127.txt\n",
      "file  127.txt  already exits\n",
      "Downloading file:  stories/128.txt\n",
      "file  128.txt  already exits\n",
      "Downloading file:  stories/129.txt\n",
      "file  129.txt  already exits\n",
      "Downloading file:  stories/130.txt\n",
      "file  130.txt  already exits\n",
      "Downloading file:  stories/131.txt\n",
      "file  131.txt  already exits\n",
      "Downloading file:  stories/132.txt\n",
      "file  132.txt  already exits\n",
      "Downloading file:  stories/133.txt\n",
      "file  133.txt  already exits\n",
      "Downloading file:  stories/134.txt\n",
      "file  134.txt  already exits\n",
      "Downloading file:  stories/135.txt\n",
      "file  135.txt  already exits\n",
      "Downloading file:  stories/136.txt\n",
      "file  136.txt  already exits\n",
      "Downloading file:  stories/137.txt\n",
      "file  137.txt  already exits\n",
      "Downloading file:  stories/138.txt\n",
      "file  138.txt  already exits\n",
      "Downloading file:  stories/139.txt\n",
      "file  139.txt  already exits\n",
      "Downloading file:  stories/140.txt\n",
      "file  140.txt  already exits\n",
      "Downloading file:  stories/141.txt\n",
      "file  141.txt  already exits\n",
      "Downloading file:  stories/142.txt\n",
      "file  142.txt  already exits\n",
      "Downloading file:  stories/143.txt\n",
      "file  143.txt  already exits\n",
      "Downloading file:  stories/144.txt\n",
      "file  144.txt  already exits\n",
      "Downloading file:  stories/145.txt\n",
      "file  145.txt  already exits\n",
      "Downloading file:  stories/146.txt\n",
      "file  146.txt  already exits\n",
      "Downloading file:  stories/147.txt\n",
      "file  147.txt  already exits\n",
      "Downloading file:  stories/148.txt\n",
      "file  148.txt  already exits\n",
      "Downloading file:  stories/149.txt\n",
      "file  149.txt  already exits\n",
      "Downloading file:  stories/150.txt\n",
      "file  150.txt  already exits\n",
      "Downloading file:  stories/151.txt\n",
      "file  151.txt  already exits\n",
      "Downloading file:  stories/152.txt\n",
      "file  152.txt  already exits\n",
      "Downloading file:  stories/153.txt\n",
      "file  153.txt  already exits\n",
      "Downloading file:  stories/154.txt\n",
      "file  154.txt  already exits\n",
      "Downloading file:  stories/155.txt\n",
      "file  155.txt  already exits\n",
      "Downloading file:  stories/156.txt\n",
      "file  156.txt  already exits\n",
      "Downloading file:  stories/157.txt\n",
      "file  157.txt  already exits\n",
      "Downloading file:  stories/158.txt\n",
      "file  158.txt  already exits\n",
      "Downloading file:  stories/159.txt\n",
      "file  159.txt  already exits\n",
      "Downloading file:  stories/160.txt\n",
      "file  160.txt  already exits\n",
      "Downloading file:  stories/161.txt\n",
      "file  161.txt  already exits\n",
      "Downloading file:  stories/162.txt\n",
      "file  162.txt  already exits\n",
      "Downloading file:  stories/163.txt\n",
      "file  163.txt  already exits\n",
      "Downloading file:  stories/164.txt\n",
      "file  164.txt  already exits\n",
      "Downloading file:  stories/165.txt\n",
      "file  165.txt  already exits\n",
      "Downloading file:  stories/166.txt\n",
      "file  166.txt  already exits\n",
      "Downloading file:  stories/167.txt\n",
      "file  167.txt  already exits\n",
      "Downloading file:  stories/168.txt\n",
      "file  168.txt  already exits\n",
      "Downloading file:  stories/169.txt\n",
      "file  169.txt  already exits\n",
      "Downloading file:  stories/170.txt\n",
      "file  170.txt  already exits\n",
      "Downloading file:  stories/171.txt\n",
      "file  171.txt  already exits\n",
      "Downloading file:  stories/172.txt\n",
      "file  172.txt  already exits\n",
      "Downloading file:  stories/173.txt\n",
      "file  173.txt  already exits\n",
      "Downloading file:  stories/174.txt\n",
      "file  174.txt  already exits\n",
      "Downloading file:  stories/175.txt\n",
      "file  175.txt  already exits\n",
      "Downloading file:  stories/176.txt\n",
      "file  176.txt  already exits\n",
      "Downloading file:  stories/177.txt\n",
      "file  177.txt  already exits\n",
      "Downloading file:  stories/178.txt\n",
      "file  178.txt  already exits\n",
      "Downloading file:  stories/179.txt\n",
      "file  179.txt  already exits\n",
      "Downloading file:  stories/180.txt\n",
      "file  180.txt  already exits\n",
      "Downloading file:  stories/181.txt\n",
      "file  181.txt  already exits\n",
      "Downloading file:  stories/182.txt\n",
      "file  182.txt  already exits\n",
      "Downloading file:  stories/183.txt\n",
      "file  183.txt  already exits\n",
      "Downloading file:  stories/184.txt\n",
      "file  184.txt  already exits\n",
      "Downloading file:  stories/185.txt\n",
      "file  185.txt  already exits\n",
      "Downloading file:  stories/186.txt\n",
      "file  186.txt  already exits\n",
      "Downloading file:  stories/187.txt\n",
      "file  187.txt  already exits\n",
      "Downloading file:  stories/188.txt\n",
      "file  188.txt  already exits\n",
      "Downloading file:  stories/189.txt\n",
      "file  189.txt  already exits\n",
      "Downloading file:  stories/190.txt\n",
      "file  190.txt  already exits\n",
      "Downloading file:  stories/191.txt\n",
      "file  191.txt  already exits\n",
      "Downloading file:  stories/192.txt\n",
      "file  192.txt  already exits\n",
      "Downloading file:  stories/193.txt\n",
      "file  193.txt  already exits\n",
      "Downloading file:  stories/194.txt\n",
      "file  194.txt  already exits\n",
      "Downloading file:  stories/195.txt\n",
      "file  195.txt  already exits\n",
      "Downloading file:  stories/196.txt\n",
      "file  196.txt  already exits\n",
      "Downloading file:  stories/197.txt\n",
      "file  197.txt  already exits\n",
      "Downloading file:  stories/198.txt\n",
      "file  198.txt  already exits\n",
      "Downloading file:  stories/199.txt\n",
      "file  199.txt  already exits\n",
      "Downloading file:  stories/200.txt\n",
      "file  200.txt  already exits\n",
      "Downloading file:  stories/201.txt\n",
      "file  201.txt  already exits\n",
      "Downloading file:  stories/202.txt\n",
      "file  202.txt  already exits\n",
      "Downloading file:  stories/203.txt\n",
      "file  203.txt  already exits\n",
      "Downloading file:  stories/204.txt\n",
      "file  204.txt  already exits\n",
      "Downloading file:  stories/205.txt\n",
      "file  205.txt  already exits\n",
      "Downloading file:  stories/206.txt\n",
      "file  206.txt  already exits\n",
      "Downloading file:  stories/207.txt\n",
      "file  207.txt  already exits\n",
      "Downloading file:  stories/208.txt\n",
      "file  208.txt  already exits\n",
      "Downloading file:  stories/209.txt\n",
      "file  209.txt  already exits\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.cs.cmu.edu/~spok/grimmtmp/'\n",
    "\n",
    "#create a directory if needed\n",
    "dir_name = 'stories'\n",
    "if not os.path.exists(dir_name):\n",
    "    os.mkdir(dir_name)\n",
    "    \n",
    "def download_data(filename):\n",
    "    \"\"\"Download a file if not present\"\"\"\n",
    "    print('Downloading file: ', dir_name+os.sep+filename)\n",
    "    \n",
    "    if not os.path.exists(dir_name + os.sep+filename):\n",
    "        filename, _ = urlretrieve(url + filename, dir_name+os.sep+filename)\n",
    "    else: \n",
    "        print('file ', filename, ' already exits')\n",
    "        \n",
    "    return filename\n",
    "\n",
    "num_files = 209 \n",
    "\n",
    "filenames = [format(i, '03d')+'.txt' for i in range(1, num_files+1)]\n",
    "\n",
    "for fn in filenames:\n",
    "    download_data(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209 files found.\n"
     ]
    }
   ],
   "source": [
    "## check if the files are downloaded \n",
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
    "    assert file_exists\n",
    "print('%d files found.' %len(filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data \n",
    "Data will be stored in a list of lists where each list represents document and a document is a list of words. we will then break the text into bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file stories/001.txt\n",
      "Data size (characters) (Document 0) 3667\n",
      "sample string (Documents 0) ['in', ' o', 'ld', 'en', ' t', 'im', 'es', ' w', 'he', 'n ', 'wi', 'sh', 'in', 'g ', 'st', 'il', 'l ', 'he', 'lp', 'ed', ' o', 'ne', ', ', 'th', 'er', 'e ', 'li', 've', 'd ', 'a ', 'ki', 'ng', '\\nw', 'ho', 'se', ' d', 'au', 'gh', 'te', 'rs', ' w', 'er', 'e ', 'al', 'l ', 'be', 'au', 'ti', 'fu', 'l,']\n",
      "\n",
      "Processing file stories/002.txt\n",
      "Data size (characters) (Document 1) 4928\n",
      "sample string (Documents 1) ['ha', 'rd', ' b', 'y ', 'a ', 'gr', 'ea', 't ', 'fo', 're', 'st', ' d', 'we', 'lt', ' a', ' w', 'oo', 'd-', 'cu', 'tt', 'er', ' w', 'it', 'h ', 'hi', 's ', 'wi', 'fe', ', ', 'wh', 'o ', 'ha', 'd ', 'an', '\\no', 'nl', 'y ', 'ch', 'il', 'd,', ' a', ' l', 'it', 'tl', 'e ', 'gi', 'rl', ' t', 'hr', 'ee']\n",
      "\n",
      "Processing file stories/003.txt\n",
      "Data size (characters) (Document 2) 9745\n",
      "sample string (Documents 2) ['a ', 'ce', 'rt', 'ai', 'n ', 'fa', 'th', 'er', ' h', 'ad', ' t', 'wo', ' s', 'on', 's,', ' t', 'he', ' e', 'ld', 'er', ' o', 'f ', 'wh', 'om', ' w', 'as', ' s', 'ma', 'rt', ' a', 'nd', '\\ns', 'en', 'si', 'bl', 'e,', ' a', 'nd', ' c', 'ou', 'ld', ' d', 'o ', 'ev', 'er', 'yt', 'hi', 'ng', ', ', 'bu']\n",
      "\n",
      "Processing file stories/004.txt\n",
      "Data size (characters) (Document 3) 2852\n",
      "sample string (Documents 3) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', 'n ', 'ol', 'd ', 'go', 'at', ' w', 'ho', ' h', 'ad', ' s', 'ev', 'en', ' l', 'it', 'tl', 'e ', 'ki', 'ds', ', ', 'an', 'd\\n', 'lo', 've', 'd ', 'th', 'em', ' w', 'it', 'h ', 'al', 'l ', 'th', 'e ', 'lo', 've', ' o']\n",
      "\n",
      "Processing file stories/005.txt\n",
      "Data size (characters) (Document 4) 8189\n",
      "sample string (Documents 4) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', 'n ', 'ol', 'd ', 'ki', 'ng', ' w', 'ho', ' w', 'as', ' i', 'll', ' a', 'nd', ' t', 'ho', 'ug', 'ht', ' t', 'o\\n', 'hi', 'ms', 'el', 'f ', \"'i\", ' a', 'm ', 'ly', 'in', 'g ', 'on', ' w', 'ha', 't ', 'mu', 'st', ' b']\n",
      "\n",
      "Processing file stories/006.txt\n",
      "Data size (characters) (Document 5) 4369\n",
      "sample string (Documents 5) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'ea', 'sa', 'nt', ' w', 'ho', ' h', 'ad', ' d', 'ri', 've', 'n ', 'hi', 's ', 'co', 'w ', 'to', ' t', 'he', ' f', 'ai', 'r,', ' a', 'nd', ' s', 'ol', 'd\\n', 'he', 'r ', 'fo', 'r ', 'se', 've', 'n ', 'ta', 'le', 'rs', '. ', ' o', 'n ', 'th', 'e ']\n",
      "\n",
      "Processing file stories/007.txt\n",
      "Data size (characters) (Document 6) 5216\n",
      "sample string (Documents 6) ['th', 'er', 'e ', 'we', 're', ' o', 'nc', 'e ', 'up', 'on', ' a', ' t', 'im', 'e ', 'a ', 'ki', 'ng', ' a', 'nd', ' a', ' q', 'ue', 'en', ' w', 'ho', ' l', 'iv', 'ed', '\\nh', 'ap', 'pi', 'ly', ' t', 'og', 'et', 'he', 'r ', 'an', 'd ', 'ha', 'd ', 'tw', 'el', 've', ' c', 'hi', 'ld', 're', 'n,', ' b']\n",
      "\n",
      "Processing file stories/008.txt\n",
      "Data size (characters) (Document 7) 6097\n",
      "sample string (Documents 7) ['li', 'tt', 'le', ' b', 'ro', 'th', 'er', ' t', 'oo', 'k ', 'hi', 's ', 'li', 'tt', 'le', ' s', 'is', 'te', 'r ', 'by', ' t', 'he', ' h', 'an', 'd ', 'an', 'd ', 'sa', 'id', ', ', 'si', 'nc', 'e\\n', 'ou', 'r ', 'mo', 'th', 'er', ' d', 'ie', 'd ', 'we', ' h', 'av', 'e ', 'ha', 'd ', 'no', ' h', 'ap']\n",
      "\n",
      "Processing file stories/009.txt\n",
      "Data size (characters) (Document 8) 3699\n",
      "sample string (Documents 8) ['th', 'er', 'e ', 'we', 're', ' o', 'nc', 'e ', 'a ', 'ma', 'n ', 'an', 'd ', 'a ', 'wo', 'ma', 'n ', 'wh', 'o ', 'ha', 'd ', 'lo', 'ng', ' i', 'n ', 'va', 'in', '\\nw', 'is', 'he', 'd ', 'fo', 'r ', 'a ', 'ch', 'il', 'd.', '  ', 'at', ' l', 'en', 'gt', 'h ', 'th', 'e ', 'wo', 'ma', 'n ', 'ho', 'pe']\n",
      "\n",
      "Processing file stories/010.txt\n",
      "Data size (characters) (Document 9) 5268\n",
      "sample string (Documents 9) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' m', 'an', ' w', 'ho', 'se', ' w', 'if', 'e ', 'di', 'ed', ', ', 'an', 'd ', 'a ', 'wo', 'ma', 'n ', 'wh', 'os', 'e ', 'hu', 'sb', 'an', 'd\\n', 'di', 'ed', ', ', 'an', 'd ', 'th', 'e ', 'ma', 'n ', 'ha', 'd ', 'a ', 'da', 'ug', 'ht', 'er', ', ', 'an']\n",
      "\n",
      "Processing file stories/011.txt\n",
      "Data size (characters) (Document 10) 2377\n",
      "sample string (Documents 10) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' g', 'ir', 'l ', 'wh', 'o ', 'wa', 's ', 'id', 'le', ' a', 'nd', ' w', 'ou', 'ld', ' n', 'ot', ' s', 'pi', 'n,', ' a', 'nd', '\\nl', 'et', ' h', 'er', ' m', 'ot', 'he', 'r ', 'sa', 'y ', 'wh', 'at', ' s', 'he', ' w', 'ou', 'ld', ', ', 'sh', 'e ', 'co']\n",
      "\n",
      "Processing file stories/012.txt\n",
      "Data size (characters) (Document 11) 7695\n",
      "sample string (Documents 11) ['ha', 'rd', ' b', 'y ', 'a ', 'gr', 'ea', 't ', 'fo', 're', 'st', ' d', 'we', 'lt', ' a', ' p', 'oo', 'r ', 'wo', 'od', '-c', 'ut', 'te', 'r ', 'wi', 'th', ' h', 'is', ' w', 'if', 'e\\n', 'an', 'd ', 'hi', 's ', 'tw', 'o ', 'ch', 'il', 'dr', 'en', '. ', ' t', 'he', ' b', 'oy', ' w', 'as', ' c', 'al']\n",
      "\n",
      "Processing file stories/013.txt\n",
      "Data size (characters) (Document 12) 3665\n",
      "sample string (Documents 12) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' o', 'n ', 'a ', 'ti', 'me', ' a', ' p', 'oo', 'r ', 'ma', 'n,', ' w', 'ho', ' c', 'ou', 'ld', ' n', 'o ', 'lo', 'ng', 'er', '\\ns', 'up', 'po', 'rt', ' h', 'is', ' o', 'nl', 'y ', 'so', 'n.', '  ', 'th', 'en', ' s', 'ai', 'd ', 'th', 'e ', 'so', 'n,', ' d']\n",
      "\n",
      "Processing file stories/014.txt\n",
      "Data size (characters) (Document 13) 4178\n",
      "sample string (Documents 13) ['a ', 'lo', 'ng', ' t', 'im', 'e ', 'ag', 'o ', 'th', 'er', 'e ', 'li', 've', 'd ', 'a ', 'ki', 'ng', ' w', 'ho', ' w', 'as', ' f', 'am', 'ed', ' f', 'or', ' h', 'is', ' w', 'is', 'do', 'm\\n', 'th', 'ro', 'ug', 'h ', 'al', 'l ', 'th', 'e ', 'la', 'nd', '. ', ' n', 'ot', 'hi', 'ng', ' w', 'as', ' h']\n",
      "\n",
      "Processing file stories/015.txt\n",
      "Data size (characters) (Document 14) 8674\n",
      "sample string (Documents 14) ['on', 'e ', 'su', 'mm', 'er', \"'s\", ' m', 'or', 'ni', 'ng', ' a', ' l', 'it', 'tl', 'e ', 'ta', 'il', 'or', ' w', 'as', ' s', 'it', 'ti', 'ng', ' o', 'n ', 'hi', 's ', 'ta', 'bl', 'e\\n', 'by', ' t', 'he', ' w', 'in', 'do', 'w,', ' h', 'e ', 'wa', 's ', 'in', ' g', 'oo', 'd ', 'sp', 'ir', 'it', 's,']\n",
      "\n",
      "Processing file stories/016.txt\n",
      "Data size (characters) (Document 15) 7018\n",
      "sample string (Documents 15) ['\\tc', 'in', 'de', 're', 'll', 'a\\n', 'th', 'e ', 'wi', 'fe', ' o', 'f ', 'a ', 'ri', 'ch', ' m', 'an', ' f', 'el', 'l ', 'si', 'ck', ', ', 'an', 'd ', 'as', ' s', 'he', ' f', 'el', 't ', 'th', 'at', ' h', 'er', ' e', 'nd', '\\nw', 'as', ' d', 'ra', 'wi', 'ng', ' n', 'ea', 'r,', ' s', 'he', ' c', 'al']\n",
      "\n",
      "Processing file stories/017.txt\n",
      "Data size (characters) (Document 16) 3039\n",
      "sample string (Documents 16) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' k', 'in', \"g'\", 's ', 'so', 'n ', 'wh', 'o ', 'wa', 's ', 'se', 'iz', 'ed', ' w', 'it', 'h ', 'a ', 'de', 'si', 're', ' t', 'o ', 'tr', 'av', 'el', '\\na', 'bo', 'ut', ' t', 'he', ' w', 'or', 'ld', ', ', 'an', 'd ', 'to', 'ok', ' n', 'o ', 'on', 'e ']\n",
      "\n",
      "Processing file stories/018.txt\n",
      "Data size (characters) (Document 17) 3020\n",
      "sample string (Documents 17) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' w', 'id', 'ow', ' w', 'ho', ' h', 'ad', ' t', 'wo', ' d', 'au', 'gh', 'te', 'rs', ' -', ' o', 'ne', ' o', 'f\\n', 'wh', 'om', ' w', 'as', ' p', 're', 'tt', 'y ', 'an', 'd ', 'in', 'du', 'st', 'ri', 'ou', 's,', ' w', 'hi', 'ls', 't ', 'th', 'e ', 'ot']\n",
      "\n",
      "Processing file stories/019.txt\n",
      "Data size (characters) (Document 18) 2465\n",
      "sample string (Documents 18) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' m', 'an', ' w', 'ho', ' h', 'ad', ' s', 'ev', 'en', ' s', 'on', 's,', ' a', 'nd', ' s', 'ti', 'll', ' h', 'e ', 'ha', 'd\\n', 'no', ' d', 'au', 'gh', 'te', 'r,', ' h', 'ow', 'ev', 'er', ' m', 'uc', 'h ', 'he', ' w', 'is', 'he', 'd ', 'fo', 'r ', 'on']\n",
      "\n",
      "Processing file stories/020.txt\n",
      "Data size (characters) (Document 19) 3703\n",
      "sample string (Documents 19) ['\\tl', 'it', 'tl', 'e ', 're', 'd-', 'ca', 'p\\n', '\\no', 'nc', 'e ', 'up', 'on', ' a', ' t', 'im', 'e ', 'th', 'er', 'e ', 'wa', 's ', 'a ', 'de', 'ar', ' l', 'it', 'tl', 'e ', 'gi', 'rl', ' w', 'ho', ' w', 'as', ' l', 'ov', 'ed', '\\nb', 'y ', 'ev', 'er', 'y ', 'on', 'e ', 'wh', 'o ', 'lo', 'ok', 'ed']\n",
      "\n",
      "Processing file stories/021.txt\n",
      "Data size (characters) (Document 20) 1924\n",
      "sample string (Documents 20) ['in', ' a', ' c', 'er', 'ta', 'in', ' c', 'ou', 'nt', 'ry', ' t', 'he', 're', ' w', 'as', ' o', 'nc', 'e ', 'gr', 'ea', 't ', 'la', 'me', 'nt', 'at', 'io', 'n ', 'ov', 'er', ' a', '\\nw', 'il', 'd ', 'bo', 'ar', ' t', 'ha', 't ', 'la', 'id', ' w', 'as', 'te', ' t', 'he', ' f', 'ar', 'me', \"r'\", 's ']\n",
      "\n",
      "Processing file stories/022.txt\n",
      "Data size (characters) (Document 21) 6561\n",
      "sample string (Documents 21) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'wo', 'ma', 'n ', 'wh', 'o ', 'ga', 've', ' b', 'ir', 'th', ' t', 'o ', 'a ', 'li', 'tt', 'le', ' s', 'on', ',\\n', 'an', 'd ', 'as', ' h', 'e ', 'ca', 'me', ' i', 'nt', 'o ', 'th', 'e ', 'wo', 'rl', 'd ', 'wi', 'th', ' a', ' c', 'au']\n",
      "\n",
      "Processing file stories/023.txt\n",
      "Data size (characters) (Document 22) 5956\n",
      "sample string (Documents 22) ['a ', 'ce', 'rt', 'ai', 'n ', 'mi', 'll', 'er', ' h', 'ad', ' l', 'it', 'tl', 'e ', 'by', ' l', 'it', 'tl', 'e ', 'fa', 'll', 'en', ' i', 'nt', 'o ', 'po', 've', 'rt', 'y,', ' a', 'nd', '\\nh', 'ad', ' n', 'ot', 'hi', 'ng', ' l', 'ef', 't ', 'bu', 't ', 'hi', 's ', 'mi', 'll', ' a', 'nd', ' a', ' l']\n",
      "\n",
      "Processing file stories/024.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size (characters) (Document 23) 2529\n",
      "sample string (Documents 23) ['th', 'e ', 'mo', 'th', 'er', ' o', 'f ', 'ha', 'ns', ' s', 'ai', 'd,', ' w', 'hi', 'th', 'er', ' a', 'wa', 'y,', ' h', 'an', 's.', '  ', 'ha', 'ns', ' a', 'ns', 'we', 're', 'd,', ' t', 'o\\n', 'gr', 'et', 'el', '. ', ' b', 'eh', 'av', 'e ', 'we', 'll', ', ', 'ha', 'ns', '. ', ' o', 'h,', ' i', \"'l\"]\n",
      "\n",
      "Processing file stories/025.txt\n",
      "Data size (characters) (Document 24) 2416\n",
      "sample string (Documents 24) ['an', ' a', 'ge', 'd ', 'co', 'un', 't ', 'on', 'ce', ' l', 'iv', 'ed', ' i', 'n ', 'sw', 'it', 'ze', 'rl', 'an', 'd,', ' w', 'ho', ' h', 'ad', ' a', 'n ', 'on', 'ly', ' s', 'on', ',\\n', 'bu', 't ', 'he', ' w', 'as', ' s', 'tu', 'pi', 'd,', ' a', 'nd', ' c', 'ou', 'ld', ' l', 'ea', 'rn', ' n', 'ot']\n",
      "\n",
      "Processing file stories/026.txt\n",
      "Data size (characters) (Document 25) 3369\n",
      "sample string (Documents 25) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' m', 'an', ' w', 'ho', ' h', 'ad', ' a', ' d', 'au', 'gh', 'te', 'r ', 'wh', 'o ', 'wa', 's ', 'ca', 'll', 'ed', ' c', 'le', 've', 'r\\n', 'el', 'si', 'e.', '  ', 'an', 'd ', 'wh', 'en', ' s', 'he', ' h', 'ad', ' g', 'ro', 'wn', ' u', 'p ', 'he', 'r ']\n",
      "\n",
      "Processing file stories/027.txt\n",
      "Data size (characters) (Document 26) 10013\n",
      "sample string (Documents 26) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' t', 'ai', 'lo', 'r ', 'wh', 'o ', 'ha', 'd ', 'th', 're', 'e ', 'so', 'ns', ', ', 'an', 'd\\n', 'on', 'ly', ' o', 'ne', ' g', 'oa', 't.', '  ', 'bu', 't ', 'as', ' t', 'he', ' g', 'oa', 't ', 'su', 'pp', 'or', 'te']\n",
      "\n",
      "Processing file stories/028.txt\n",
      "Data size (characters) (Document 27) 5788\n",
      "sample string (Documents 27) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'pe', 'as', 'an', 't ', 'wh', 'o ', 'sa', 't ', 'in', ' t', 'he', ' e', 've', 'ni', 'ng', ' b', 'y ', 'th', 'e\\n', 'he', 'ar', 'th', ' a', 'nd', ' p', 'ok', 'ed', ' t', 'he', ' f', 'ir', 'e,', ' a', 'nd', ' h', 'is', ' w', 'if', 'e ']\n",
      "\n",
      "Processing file stories/029.txt\n",
      "Data size (characters) (Document 28) 1335\n",
      "sample string (Documents 28) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'se', 'rv', 'an', 't-', 'gi', 'rl', ' w', 'ho', ' w', 'as', ' i', 'nd', 'us', 'tr', 'io', 'us', ' a', 'nd', ' c', 'le', 'an', 'ly', '\\na', 'nd', ' s', 'we', 'pt', ' t', 'he', ' h', 'ou', 'se', ' e', 've', 'ry', ' d', 'ay', ', ', 'an']\n",
      "\n",
      "Processing file stories/030.txt\n",
      "Data size (characters) (Document 29) 3591\n",
      "sample string (Documents 29) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' m', 'il', 'le', 'r,', ' w', 'ho', ' h', 'ad', ' a', ' b', 'ea', 'ut', 'if', 'ul', '\\nd', 'au', 'gh', 'te', 'r,', ' a', 'nd', ' a', 's ', 'sh', 'e ', 'wa', 's ', 'gr', 'ow', 'n ', 'up', ', ', 'he', ' w', 'is', 'he']\n",
      "\n",
      "Processing file stories/031.txt\n",
      "Data size (characters) (Document 30) 1624\n",
      "sample string (Documents 30) ['a ', 'po', 'or', ' m', 'an', ' h', 'ad', ' s', 'o ', 'ma', 'ny', ' c', 'hi', 'ld', 're', 'n ', 'th', 'at', ' h', 'e ', 'ha', 'd ', 'al', 're', 'ad', 'y ', 'as', 'ke', 'd\\n', 'ev', 'er', 'yo', 'ne', ' i', 'n ', 'th', 'e ', 'wo', 'rl', 'd ', 'to', ' b', 'e ', 'go', 'df', 'at', 'he', 'r,', ' a', 'nd']\n",
      "\n",
      "Processing file stories/032.txt\n",
      "Data size (characters) (Document 31) 758\n",
      "sample string (Documents 31) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' l', 'it', 'tl', 'e ', 'gi', 'rl', ' w', 'ho', ' w', 'as', ' o', 'bs', 'ti', 'na', 'te', ' a', 'nd', ' i', 'nq', 'ui', 'si', 'ti', 've', ',\\n', 'an', 'd ', 'wh', 'en', ' h', 'er', ' p', 'ar', 'en', 'ts', ' t', 'ol', 'd ', 'he', 'r ', 'to', ' d', 'o ']\n",
      "\n",
      "Processing file stories/033.txt\n",
      "Data size (characters) (Document 32) 3121\n",
      "sample string (Documents 32) ['a ', 'po', 'or', ' m', 'an', ' h', 'ad', ' t', 'we', 'lv', 'e ', 'ch', 'il', 'dr', 'en', ' a', 'nd', ' w', 'as', ' f', 'or', 'ce', 'd ', 'to', ' w', 'or', 'k ', 'ni', 'gh', 't ', 'an', 'd\\n', 'da', 'y ', 'to', ' g', 'iv', 'e ', 'th', 'em', ' e', 've', 'n ', 'br', 'ea', 'd.', '  ', 'wh', 'en', ' t']\n",
      "\n",
      "Processing file stories/034.txt\n",
      "Data size (characters) (Document 33) 4192\n",
      "sample string (Documents 33) ['a ', 'ce', 'rt', 'ai', 'n ', 'ta', 'il', 'or', ' h', 'ad', ' a', ' s', 'on', ', ', 'wh', 'o ', 'ha', 'pp', 'en', 'ed', ' t', 'o ', 'be', ' s', 'ma', 'll', ', ', 'an', 'd\\n', 'no', ' b', 'ig', 'ge', 'r ', 'th', 'an', ' a', ' t', 'hu', 'mb', ', ', 'an', 'd ', 'on', ' t', 'hi', 's ', 'ac', 'co', 'un']\n",
      "\n",
      "Processing file stories/035.txt\n",
      "Data size (characters) (Document 34) 3650\n",
      "sample string (Documents 34) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' w', 'iz', 'ar', 'd ', 'wh', 'o ', 'us', 'ed', ' t', 'o ', 'ta', 'ke', ' t', 'he', ' f', 'or', 'm ', 'of', ' a', ' p', 'oo', 'r\\n', 'ma', 'n,', ' a', 'nd', ' w', 'en', 't ', 'to', ' h', 'ou', 'se', 's ', 'an', 'd ', 'be', 'gg', 'ed', ', ', 'an', 'd ']\n",
      "\n",
      "Processing file stories/036.txt\n",
      "Data size (characters) (Document 35) 8219\n",
      "sample string (Documents 35) ['it', ' i', 's ', 'no', 'w ', 'lo', 'ng', ' a', 'go', ', ', 'qu', 'it', 'e ', 'tw', 'o ', 'th', 'ou', 'sa', 'nd', ' y', 'ea', 'rs', ', ', 'si', 'nc', 'e ', 'th', 'er', 'e ', 'wa', 's\\n', 'a ', 'ri', 'ch', ' m', 'an', ' w', 'ho', ' h', 'ad', ' a', ' b', 'ea', 'ut', 'if', 'ul', ' a', 'nd', ' p', 'io']\n",
      "\n",
      "Processing file stories/037.txt\n",
      "Data size (characters) (Document 36) 2151\n",
      "sample string (Documents 36) ['a ', 'fa', 'rm', 'er', ' o', 'nc', 'e ', 'ha', 'd ', 'a ', 'fa', 'it', 'hf', 'ul', ' d', 'og', ' c', 'al', 'le', 'd ', 'su', 'lt', 'an', ', ', 'wh', 'o ', 'ha', 'd ', 'gr', 'ow', 'n\\n', 'ol', 'd,', ' a', 'nd', ' l', 'os', 't ', 'al', 'l ', 'hi', 's ', 'te', 'et', 'h,', ' s', 'o ', 'th', 'at', ' h']\n",
      "\n",
      "Processing file stories/038.txt\n",
      "Data size (characters) (Document 37) 5129\n",
      "sample string (Documents 37) ['on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ', ', 'a ', 'ce', 'rt', 'ai', 'n ', 'ki', 'ng', ' w', 'as', ' h', 'un', 'ti', 'ng', ' i', 'n ', 'a ', 'gr', 'ea', 't ', 'fo', 're', 'st', ',\\n', 'an', 'd ', 'he', ' c', 'ha', 'se', 'd ', 'a ', 'wi', 'ld', ' b', 'ea', 'st', ' s', 'o ', 'ea', 'ge', 'rl']\n",
      "\n",
      "Processing file stories/039.txt\n",
      "Data size (characters) (Document 38) 3472\n",
      "sample string (Documents 38) ['\\tb', 'ri', 'ar', '-r', 'os', 'e\\n', '\\na', ' l', 'on', 'g ', 'ti', 'me', ' a', 'go', ' t', 'he', 're', ' w', 'er', 'e ', 'a ', 'ki', 'ng', ' a', 'nd', ' q', 'ue', 'en', ' w', 'ho', ' s', 'ai', 'd ', 'ev', 'er', 'y\\n', 'da', 'y,', ' a', 'h,', ' i', 'f ', 'on', 'ly', ' w', 'e ', 'ha', 'd ', 'a ', 'ch']\n",
      "\n",
      "Processing file stories/040.txt\n",
      "Data size (characters) (Document 39) 2490\n",
      "sample string (Documents 39) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' f', 'or', 'es', 'te', 'r ', 'wh', 'o ', 'we', 'nt', ' i', 'nt', 'o ', 'th', 'e ', 'fo', 're', 'st', ' t', 'o ', 'hu', 'nt', ',\\n', 'an', 'd ', 'as', ' h', 'e ', 'en', 'te', 're', 'd ', 'it', ' h', 'e ', 'he', 'ar', 'd ', 'a ', 'so', 'un', 'd ', 'of']\n",
      "\n",
      "Processing file stories/041.txt\n",
      "Data size (characters) (Document 40) 4273\n",
      "sample string (Documents 40) ['a ', 'ki', 'ng', ' h', 'ad', ' a', ' d', 'au', 'gh', 'te', 'r ', 'wh', 'o ', 'wa', 's ', 'be', 'au', 'ti', 'fu', 'l ', 'be', 'yo', 'nd', ' a', 'll', ' m', 'ea', 'su', 're', ',\\n', 'bu', 't ', 'so', ' p', 'ro', 'ud', ' a', 'nd', ' h', 'au', 'gh', 'ty', ' w', 'it', 'ha', 'l ', 'th', 'at', ' n', 'o ']\n",
      "\n",
      "Processing file stories/042.txt\n",
      "Data size (characters) (Document 41) 8327\n",
      "sample string (Documents 41) ['\\ts', 'no', 'w ', 'wh', 'it', 'e ', 'an', 'd ', 'th', 'e ', 'se', 've', 'n ', 'dw', 'ar', 'fs', '\\n\\n', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' i', 'n ', 'th', 'e ', 'mi', 'dd', 'le', ' o', 'f ', 'wi', 'nt', 'er', ', ', 'wh', 'en', ' t', 'he', ' f', 'la', 'ke', 's ', 'of', '\\ns', 'no', 'w ']\n",
      "\n",
      "Processing file stories/043.txt\n",
      "Data size (characters) (Document 42) 6128\n",
      "sample string (Documents 42) ['th', 'er', 'e ', 'we', 're', ' o', 'nc', 'e ', 'th', 're', 'e ', 'br', 'ot', 'he', 'rs', ' w', 'ho', ' h', 'ad', ' f', 'al', 'le', 'n ', 'de', 'ep', 'er', ' a', 'nd', ' d', 'ee', 'pe', 'r ', 'in', 'to', '\\np', 'ov', 'er', 'ty', ', ', 'an', 'd ', 'at', ' l', 'as', 't ', 'th', 'ei', 'r ', 'ne', 'ed']\n",
      "\n",
      "Processing file stories/044.txt\n",
      "Data size (characters) (Document 43) 2819\n",
      "sample string (Documents 43) ['\\tr', 'um', 'pe', 'ls', 'ti', 'lt', 'sk', 'in', '\\n\\n', 'on', 'ce', ' t', 'he', 're', ' w', 'as', ' a', ' m', 'il', 'le', 'r ', 'wh', 'o ', 'wa', 's ', 'po', 'or', ', ', 'bu', 't ', 'wh', 'o ', 'ha', 'd ', 'a ', 'be', 'au', 'ti', 'fu', 'l\\n', 'da', 'ug', 'ht', 'er', '. ', ' n', 'ow', ' i', 't ', 'ha']\n",
      "\n",
      "Processing file stories/045.txt\n",
      "Data size (characters) (Document 44) 3822\n",
      "sample string (Documents 44) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' w', 'om', 'an', ' w', 'ho', ' w', 'as', ' a', ' r', 'ea', 'l ', 'wi', 'tc', 'h ', 'an', 'd ', 'ha', 'd ', 'tw', 'o\\n', 'da', 'ug', 'ht', 'er', 's,', ' o', 'ne', ' u', 'gl', 'y ', 'an', 'd ', 'wi', 'ck', 'ed', ', ']\n",
      "\n",
      "Processing file stories/046.txt\n",
      "Data size (characters) (Document 45) 7772\n",
      "sample string (Documents 45) ['in', ' o', 'ld', 'en', ' t', 'im', 'es', ' t', 'he', 're', ' w', 'as', ' a', ' k', 'in', 'g,', ' w', 'ho', ' h', 'ad', ' b', 'eh', 'in', 'd ', 'hi', 's ', 'pa', 'la', 'ce', ' a', '\\nb', 'ea', 'ut', 'if', 'ul', ' p', 'le', 'as', 'ur', 'e-', 'ga', 'rd', 'en', ' i', 'n ', 'wh', 'ic', 'h ', 'th', 'er']\n",
      "\n",
      "Processing file stories/047.txt\n",
      "Data size (characters) (Document 46) 22158\n",
      "sample string (Documents 46) ['th', 'er', 'e ', 'we', 're', ' o', 'nc', 'e ', 'up', 'on', ' a', ' t', 'im', 'e ', 'tw', 'o ', 'br', 'ot', 'he', 'rs', ', ', 'on', 'e ', 'ri', 'ch', ' a', 'nd', ' t', 'he', ' o', 'th', 'er', '\\np', 'oo', 'r.', '  ', 'th', 'e ', 'ri', 'ch', ' o', 'ne', ' w', 'as', ' a', ' g', 'ol', 'ds', 'mi', 'th']\n",
      "\n",
      "Processing file stories/048.txt\n",
      "Data size (characters) (Document 47) 2169\n",
      "sample string (Documents 47) ['tw', 'o ', 'ki', 'ng', \"s'\", ' s', 'on', 's ', 'on', 'ce', ' w', 'en', 't ', 'ou', 't ', 'in', ' s', 'ea', 'rc', 'h ', 'of', ' a', 'dv', 'en', 'tu', 're', 's,', ' a', 'nd', ' f', 'el', 'l ', 'in', 'to', '\\na', ' w', 'il', 'd,', ' d', 'is', 'or', 'de', 'rl', 'y ', 'wa', 'y ', 'of', ' l', 'iv', 'in']\n",
      "\n",
      "Processing file stories/049.txt\n",
      "Data size (characters) (Document 48) 2822\n",
      "sample string (Documents 48) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' k', 'in', 'g ', 'wh', 'o ', 'ha', 'd ', 'th', 're', 'e ', 'so', 'ns', ', ', 'of', ' w', 'ho', 'm ', 'tw', 'o\\n', 'we', 're', ' c', 'le', 've', 'r ', 'an', 'd ', 'wi', 'se', ', ', 'bu', 't ', 'th', 'e ', 'th', 'ir']\n",
      "\n",
      "Processing file stories/050.txt\n",
      "Data size (characters) (Document 49) 4034\n",
      "sample string (Documents 49) ['th', 'er', 'e ', 'wa', 's ', 'a ', 'ma', 'n ', 'wh', 'o ', 'ha', 'd ', 'th', 're', 'e ', 'so', 'ns', ', ', 'th', 'e ', 'yo', 'un', 'ge', 'st', ' o', 'f ', 'wh', 'om', ' w', 'as', ' c', 'al', 'le', 'd\\n', 'du', 'mm', 'li', 'ng', ', ', 'an', 'd ', 'wa', 's ', 'de', 'sp', 'is', 'ed', ', ', 'mo', 'ck']\n",
      "\n",
      "Processing file stories/051.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size (characters) (Document 50) 5608\n",
      "sample string (Documents 50) ['\\ta', 'll', 'er', 'le', 'ir', 'au', 'h\\n', '\\nt', 'he', 're', ' w', 'as', ' o', 'nc', 'e ', 'up', 'on', ' a', ' t', 'im', 'e ', 'a ', 'ki', 'ng', ' w', 'ho', ' h', 'ad', ' a', ' w', 'if', 'e ', 'wi', 'th', ' g', 'ol', 'de', 'n ', 'ha', 'ir', ',\\n', 'an', 'd ', 'sh', 'e ', 'wa', 's ', 'so', ' b', 'ea']\n",
      "\n",
      "Processing file stories/052.txt\n",
      "Data size (characters) (Document 51) 1287\n",
      "sample string (Documents 51) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' w', 'om', 'an', ' a', 'nd', ' h', 'er', ' d', 'au', 'gh', 'te', 'r ', 'wh', 'o ', 'li', 've', 'd ', 'in', ' a', '\\np', 're', 'tt', 'y ', 'ga', 'rd', 'en', ' w', 'it', 'h ', 'ca', 'bb', 'ag', 'es', '. ', ' a', 'nd', ' a', ' l', 'it', 'tl', 'e ', 'ha']\n",
      "\n",
      "Processing file stories/053.txt\n",
      "Data size (characters) (Document 52) 2841\n",
      "sample string (Documents 52) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' k', 'in', \"g'\", 's ', 'so', 'n ', 'wh', 'o ', 'ha', 'd ', 'a ', 'br', 'id', 'e ', 'wh', 'om', ' h', 'e ', 'lo', 've', 'd ', 've', 'ry', ' m', 'uc', 'h.', '\\na', 'nd', ' w', 'he', 'n ', 'he', ' w', 'as', ' s', 'it', 'ti', 'ng', ' b', 'es', 'id', 'e ']\n",
      "\n",
      "Processing file stories/054.txt\n",
      "Data size (characters) (Document 53) 1922\n",
      "sample string (Documents 53) ['ha', 'ns', ' w', 'is', 'he', 'd ', 'to', ' p', 'ut', ' h', 'is', ' s', 'on', ' t', 'o ', 'le', 'ar', 'n ', 'a ', 'tr', 'ad', 'e,', ' s', 'o ', 'he', ' w', 'en', 't ', 'in', 'to', ' t', 'he', '\\nc', 'hu', 'rc', 'h ', 'an', 'd ', 'pr', 'ay', 'ed', ' t', 'o ', 'ou', 'r ', 'lo', 'rd', ' g', 'od', ' t']\n",
      "\n",
      "Processing file stories/055.txt\n",
      "Data size (characters) (Document 54) 2573\n",
      "sample string (Documents 54) ['a ', 'fa', 'th', 'er', ' o', 'nc', 'e ', 'ca', 'll', 'ed', ' h', 'is', ' t', 'hr', 'ee', ' s', 'on', 's ', 'be', 'fo', 're', ' h', 'im', ', ', 'an', 'd ', 'he', ' g', 'av', 'e ', 'to', ' t', 'he', '\\nf', 'ir', 'st', ' a', ' c', 'oc', 'k,', ' t', 'o ', 'th', 'e ', 'se', 'co', 'nd', ' a', ' s', 'cy']\n",
      "\n",
      "Processing file stories/056.txt\n",
      "Data size (characters) (Document 55) 5285\n",
      "sample string (Documents 55) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' m', 'an', ' w', 'ho', ' u', 'nd', 'er', 'st', 'oo', 'd ', 'al', 'l ', 'ki', 'nd', 's ', 'of', ' a', 'rt', 's.', '  ', 'he', ' s', 'er', 've', 'd ', 'in', '\\nw', 'ar', ', ', 'an', 'd ', 'be', 'ha', 've', 'd ', 'we', 'll', ' a', 'nd', ' b', 'ra', 've']\n",
      "\n",
      "Processing file stories/057.txt\n",
      "Data size (characters) (Document 56) 971\n",
      "sample string (Documents 56) ['th', 'e ', 'sh', 'e-', 'wo', 'lf', ' b', 'ro', 'ug', 'ht', ' i', 'nt', 'o ', 'th', 'e ', 'wo', 'rl', 'd ', 'a ', 'yo', 'un', 'g ', 'on', 'e,', ' a', 'nd', ' i', 'nv', 'it', 'ed', ' t', 'he', ' f', 'ox', '\\nt', 'o ', 'be', ' g', 'od', 'fa', 'th', 'er', '. ', ' a', 'ft', 'er', ' a', 'll', ', ', 'he']\n",
      "\n",
      "Processing file stories/058.txt\n",
      "Data size (characters) (Document 57) 4538\n",
      "sample string (Documents 57) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' q', 'ue', 'en', ' t', 'o ', 'wh', 'om', ' g', 'od', ' h', 'ad', ' g', 'iv', 'en', ' n', 'o ', 'ch', 'il', 'dr', 'en', '.\\n', 'ev', 'er', 'y ', 'mo', 'rn', 'in', 'g ', 'sh', 'e ', 'we', 'nt', ' i', 'nt', 'o ', 'th']\n",
      "\n",
      "Processing file stories/059.txt\n",
      "Data size (characters) (Document 58) 636\n",
      "sample string (Documents 58) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' v', 'er', 'y ', 'ol', 'd ', 'ma', 'n,', ' w', 'ho', 'se', ' e', 'ye', 's ', 'ha', 'd ', 'be', 'co', 'me', ' d', 'im', ', ', 'hi', 's ', 'ea', 'rs', '\\nd', 'ul', 'l ', 'of', ' h', 'ea', 'ri', 'ng', ', ', 'hi', 's ', 'kn', 'ee', 's ', 'tr', 'em', 'bl']\n",
      "\n",
      "Processing file stories/060.txt\n",
      "Data size (characters) (Document 59) 786\n",
      "sample string (Documents 59) ['a ', 'li', 'tt', 'le', ' b', 'ro', 'th', 'er', ' a', 'nd', ' s', 'is', 'te', 'r ', 'we', 're', ' o', 'nc', 'e ', 'pl', 'ay', 'in', 'g ', 'by', ' a', ' w', 'el', 'l,', ' a', 'nd', ' w', 'hi', 'le', '\\nt', 'he', 'y ', 'we', 're', ' t', 'hu', 's ', 'pl', 'ay', 'in', 'g,', ' t', 'he', 'y ', 'bo', 'th']\n",
      "\n",
      "Processing file stories/061.txt\n",
      "Data size (characters) (Document 60) 10687\n",
      "sample string (Documents 60) ['th', 'er', 'e ', 'wa', 's ', 'on', 'e ', 'up', 'on', ' a', ' t', 'im', 'e ', 'a ', 'gr', 'ea', 't ', 'wa', 'r,', ' a', 'nd', ' w', 'he', 'n ', 'it', ' c', 'am', 'e ', 'to', ' a', 'n ', 'en', 'd,', '\\nm', 'an', 'y ', 'so', 'ld', 'ie', 'rs', ' w', 'er', 'e ', 'di', 'sc', 'ha', 'rg', 'ed', '. ', ' t']\n",
      "\n",
      "Processing file stories/062.txt\n",
      "Data size (characters) (Document 61) 5105\n",
      "sample string (Documents 61) ['ha', 'ns', ' h', 'ad', ' s', 'er', 've', 'd ', 'hi', 's ', 'ma', 'st', 'er', ' f', 'or', ' s', 'ev', 'en', ' y', 'ea', 'rs', ', ', 'so', ' h', 'e ', 'sa', 'id', ' t', 'o ', 'hi', 'm,', '\\nm', 'as', 'te', 'r,', ' m', 'y ', 'ti', 'me', ' i', 's ', 'up', ', ', 'no', 'w ', 'i ', 'sh', 'ou', 'ld', ' b']\n",
      "\n",
      "Processing file stories/063.txt\n",
      "Data size (characters) (Document 62) 1127\n",
      "sample string (Documents 62) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' y', 'ou', 'ng', ' p', 'ea', 'sa', 'nt', ' n', 'am', 'ed', ' h', 'an', 's,', ' w', 'ho', 'se', ' u', 'nc', 'le', '\\nw', 'an', 'te', 'd ', 'to', ' f', 'in', 'd ', 'hi', 'm ', 'a ', 'ri', 'ch', ' w', 'if', 'e.', '  ']\n",
      "\n",
      "Processing file stories/064.txt\n",
      "Data size (characters) (Document 63) 4981\n",
      "sample string (Documents 63) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'ma', 'n ', 'an', 'd ', 'a ', 'po', 'or', ' w', 'om', 'an', ' w', 'ho', ' h', 'ad', ' n', 'ot', 'hi', 'ng', ' b', 'ut', ' a', '\\nl', 'it', 'tl', 'e ', 'co', 'tt', 'ag', 'e,', ' a', 'nd', ' w', 'ho', ' e', 'ar', 'ne', 'd ', 'th', 'ei']\n",
      "\n",
      "Processing file stories/065.txt\n",
      "Data size (characters) (Document 64) 6006\n",
      "sample string (Documents 64) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' m', 'an', ' w', 'ho', ' w', 'as', ' a', 'bo', 'ut', ' t', 'o ', 'se', 't ', 'ou', 't ', 'on', ' a', ' l', 'on', 'g\\n', 'jo', 'ur', 'ne', 'y,', ' a', 'nd', ' o', 'n ', 'pa', 'rt', 'in', 'g ', 'he', ' a', 'sk', 'ed']\n",
      "\n",
      "Processing file stories/066.txt\n",
      "Data size (characters) (Document 65) 5900\n",
      "sample string (Documents 65) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', 'n ', 'ol', 'd ', 'qu', 'ee', 'n ', 'wh', 'os', 'e ', 'hu', 'sb', 'an', 'd ', 'ha', 'd ', 'be', 'en', ' d', 'ea', 'd\\n', 'fo', 'r ', 'ma', 'ny', ' y', 'ea', 'rs', ', ', 'an', 'd ', 'sh', 'e ', 'ha', 'd ', 'a ', 'be']\n",
      "\n",
      "Processing file stories/067.txt\n",
      "Data size (characters) (Document 66) 7837\n",
      "sample string (Documents 66) ['on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' c', 'ou', 'nt', 'ry', 'ma', 'n ', 'ha', 'd ', 'a ', 'so', 'n ', 'wh', 'o ', 'wa', 's ', 'as', ' b', 'ig', ' a', 's ', 'a ', 'th', 'um', 'b,', '\\na', 'nd', ' d', 'id', ' n', 'ot', ' b', 'ec', 'om', 'e ', 'an', 'y ', 'bi', 'gg', 'er', ', ', 'an']\n",
      "\n",
      "Processing file stories/068.txt\n",
      "Data size (characters) (Document 67) 4717\n",
      "sample string (Documents 67) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' r', 'ic', 'h ', 'ki', 'ng', ' w', 'ho', ' h', 'ad', ' t', 'hr', 'ee', ' d', 'au', 'gh', 'te', 'rs', ', ', 'wh', 'o\\n', 'da', 'il', 'y ', 'we', 'nt', ' t', 'o ', 'wa', 'lk', ' i', 'n ', 'th', 'e ', 'pa', 'la', 'ce']\n",
      "\n",
      "Processing file stories/069.txt\n",
      "Data size (characters) (Document 68) 6233\n",
      "sample string (Documents 68) ['th', 'er', 'e ', 'wa', 's ', 'a ', 'ce', 'rt', 'ai', 'n ', 'me', 'rc', 'ha', 'nt', ' w', 'ho', ' h', 'ad', ' t', 'wo', ' c', 'hi', 'ld', 're', 'n,', ' a', ' b', 'oy', ' a', 'nd', ' a', ' g', 'ir', 'l,', '\\nt', 'he', 'y ', 'we', 're', ' b', 'ot', 'h ', 'yo', 'un', 'g,', ' a', 'nd', ' c', 'ou', 'ld']\n",
      "\n",
      "Processing file stories/070.txt\n",
      "Data size (characters) (Document 69) 5664\n",
      "sample string (Documents 69) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' q', 'ue', 'en', ' w', 'ho', ' h', 'ad', ' a', ' l', 'it', 'tl', 'e ', 'da', 'ug', 'ht', 'er', ' w', 'ho', '\\nw', 'as', ' s', 'ti', 'll', ' s', 'o ', 'yo', 'un', 'g ', 'th', 'at', ' s', 'he', ' h', 'ad', ' t', 'o ']\n",
      "\n",
      "Processing file stories/071.txt\n",
      "Data size (characters) (Document 70) 3569\n",
      "sample string (Documents 70) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'pe', 'as', 'an', 't ', 'wh', 'o ', 'ha', 'd ', 'no', ' l', 'an', 'd,', ' b', 'ut', ' o', 'nl', 'y ', 'a ', 'sm', 'al', 'l\\n', 'ho', 'us', 'e,', ' a', 'nd', ' o', 'ne', ' d', 'au', 'gh', 'te', 'r.', '  ', 'th', 'en', ' s', 'ai', 'd ']\n",
      "\n",
      "Processing file stories/072.txt\n",
      "Data size (characters) (Document 71) 3793\n",
      "sample string (Documents 71) ['ab', 'ou', 't ', 'a ', 'th', 'ou', 'sa', 'nd', ' o', 'r ', 'mo', 're', ' y', 'ea', 'rs', ' a', 'go', ', ', 'th', 'er', 'e ', 'we', 're', ' i', 'n ', 'th', 'is', '\\nc', 'ou', 'nt', 'ry', ' n', 'ot', 'hi', 'ng', ' b', 'ut', ' s', 'ma', 'll', ' k', 'in', 'gs', ', ', 'an', 'd ', 'on', 'e ', 'of', ' t']\n",
      "\n",
      "Processing file stories/073.txt\n",
      "Data size (characters) (Document 72) 5980\n",
      "sample string (Documents 72) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' k', 'in', 'g ', 'wh', 'o ', 'ha', 'd ', 'an', ' i', 'll', 'ne', 'ss', ', ', 'an', 'd ', 'no', ' o', 'ne', ' b', 'el', 'ie', 've', 'd ', 'th', 'at', ' h', 'e\\n', 'wo', 'ul', 'd ', 'co', 'me', ' o', 'ut', ' o', 'f ', 'it', ' w', 'it', 'h ', 'hi', 's ']\n",
      "\n",
      "Processing file stories/074.txt\n",
      "Data size (characters) (Document 73) 4518\n",
      "sample string (Documents 73) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'wo', 'od', 'cu', 'tt', 'er', ' w', 'ho', ' t', 'oi', 'le', 'd ', 'fr', 'om', ' e', 'ar', 'ly', '\\nm', 'or', 'ni', 'ng', ' t', 'il', 'l ', 'la', 'te', ' a', 't ', 'ni', 'gh', 't.', '  ', 'wh', 'en', ' a', 't ', 'la', 'st', ' h', 'e ']\n",
      "\n",
      "Processing file stories/075.txt\n",
      "Data size (characters) (Document 74) 3247\n",
      "sample string (Documents 74) ['a ', 'di', 'sc', 'ha', 'rg', 'ed', ' s', 'ol', 'di', 'er', ' h', 'ad', ' n', 'ot', 'hi', 'ng', ' t', 'o ', 'li', 've', ' o', 'n,', ' a', 'nd', ' d', 'id', ' n', 'ot', ' k', 'no', 'w ', 'ho', 'w ', 'to', '\\nm', 'ak', 'e ', 'hi', 's ', 'wa', 'y.', '  ', 'so', ' h', 'e ', 'we', 'nt', ' o', 'ut', ' i']\n",
      "\n",
      "Processing file stories/076.txt\n",
      "Data size (characters) (Document 75) 5130\n",
      "sample string (Documents 75) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' y', 'ou', 'ng', ' f', 'el', 'lo', 'w ', 'wh', 'o ', 'en', 'li', 'st', 'ed', ' a', 's ', 'a ', 'so', 'ld', 'ie', 'r,', ' c', 'on', 'du', 'ct', 'ed', '\\nh', 'im', 'se', 'lf', ' b', 'ra', 've', 'ly', ', ', 'an', 'd ', 'wa', 's ', 'al', 'wa', 'ys', ' t']\n",
      "\n",
      "Processing file stories/077.txt\n",
      "Data size (characters) (Document 76) 2401\n",
      "sample string (Documents 76) ['on', 'ce', ' i', 'n ', 'su', 'mm', 'er', '-t', 'im', 'e ', 'th', 'e ', 'be', 'ar', ' a', 'nd', ' t', 'he', ' w', 'ol', 'f ', 'we', 're', ' w', 'al', 'ki', 'ng', ' i', 'n ', 'th', 'e ', 'fo', 're', 'st', ',\\n', 'an', 'd ', 'th', 'e ', 'be', 'ar', ' h', 'ea', 'rd', ' a', ' b', 'ir', 'd ', 'si', 'ng']\n",
      "\n",
      "Processing file stories/078.txt\n",
      "Data size (characters) (Document 77) 624\n",
      "sample string (Documents 77) ['th', 'er', 'e ', 'wa', 's ', 'a ', 'po', 'or', ' b', 'ut', ' g', 'oo', 'd ', 'li', 'tt', 'le', ' g', 'ir', 'l ', 'wh', 'o ', 'li', 've', 'd ', 'al', 'on', 'e ', 'wi', 'th', ' h', 'er', '\\nm', 'ot', 'he', 'r,', ' a', 'nd', ' t', 'he', 'y ', 'no', ' l', 'on', 'ge', 'r ', 'ha', 'd ', 'an', 'yt', 'hi']\n",
      "\n",
      "Processing file stories/079.txt\n",
      "Data size (characters) (Document 78) 3991\n",
      "sample string (Documents 78) ['on', 'e ', 'da', 'y ', 'a ', 'pe', 'as', 'an', 't ', 'to', 'ok', ' h', 'is', ' g', 'oo', 'd ', 'ha', 'ze', 'l-', 'st', 'ic', 'k ', 'ou', 't ', 'of', ' t', 'he', ' c', 'or', 'ne', 'r\\n', 'an', 'd ', 'sa', 'id', ' t', 'o ', 'hi', 's ', 'wi', 'fe', ', ', 'tr', 'in', 'a,', ' i', ' a', 'm ', 'go', 'in']\n",
      "\n",
      "Processing file stories/080.txt\n",
      "Data size (characters) (Document 79) 1426\n",
      "sample string (Documents 79) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' l', 'it', 'tl', 'e ', 'ch', 'il', 'd ', 'wh', 'os', 'e ', 'mo', 'th', 'er', ' g', 'av', 'e ', 'he', 'r ', 'ev', 'er', 'y\\n', 'af', 'te', 'rn', 'oo', 'n ', 'a ', 'sm', 'al', 'l ', 'bo', 'wl', ' o', 'f ', 'mi', 'lk', ' a', 'nd', ' b', 're', 'ad', ', ']\n",
      "\n",
      "Processing file stories/081.txt\n",
      "Data size (characters) (Document 80) 3574\n",
      "sample string (Documents 80) ['in', ' a', ' c', 'er', 'ta', 'in', ' m', 'il', 'l ', 'li', 've', 'd ', 'an', ' o', 'ld', ' m', 'il', 'le', 'r ', 'wh', 'o ', 'ha', 'd ', 'ne', 'it', 'he', 'r ', 'wi', 'fe', ' n', 'or', ' c', 'hi', 'ld', ',\\n', 'an', 'd ', 'th', 're', 'e ', 'ap', 'pr', 'en', 'ti', 'ce', 's ', 'se', 'rv', 'ed', ' u']\n",
      "\n",
      "Processing file stories/082.txt\n",
      "Data size (characters) (Document 81) 10822\n",
      "sample string (Documents 81) ['hi', 'll', ' a', 'nd', ' v', 'al', 'e ', 'do', ' n', 'ot', ' m', 'ee', 't,', ' b', 'ut', ' t', 'he', ' c', 'hi', 'ld', 're', 'n ', 'of', ' m', 'en', ' d', 'o,', ' g', 'oo', 'd ', 'an', 'd ', 'ba', 'd.', '\\ni', 'n ', 'th', 'is', ' w', 'ay', ' a', ' s', 'ho', 'em', 'ak', 'er', ' a', 'nd', ' a', ' t']\n",
      "\n",
      "Processing file stories/083.txt\n",
      "Data size (characters) (Document 82) 5480\n",
      "sample string (Documents 82) ['\\th', 'an', 's ', 'th', 'e ', 'he', 'dg', 'eh', 'og', '\\n\\n', 'th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' c', 'ou', 'nt', 'ry', ' m', 'an', ' w', 'ho', ' h', 'ad', ' m', 'on', 'ey', ' a', 'nd', ' l', 'an', 'd ', 'in', ' p', 'le', 'nt', 'y,', ' b', 'ut', '\\nh', 'ow', 'ev', 'er', ' r', 'ic', 'h ']\n",
      "\n",
      "Processing file stories/084.txt\n",
      "Data size (characters) (Document 83) 658\n",
      "sample string (Documents 83) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' m', 'ot', 'he', 'r ', 'wh', 'o ', 'ha', 'd ', 'a ', 'li', 'tt', 'le', ' b', 'oy', ' o', 'f ', 'se', 've', 'n ', 'ye', 'ar', 's ', 'ol', 'd,', ' w', 'ho', '\\nw', 'as', ' s', 'o ', 'ha', 'nd', 'so', 'me', ' a', 'nd', ' l', 'ov', 'ab', 'le', ' t', 'ha']\n",
      "\n",
      "Processing file stories/085.txt\n",
      "Data size (characters) (Document 84) 5989\n",
      "sample string (Documents 84) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' y', 'ou', 'ng', ' f', 'el', 'lo', 'w ', 'wh', 'o ', 'ha', 'd ', 'le', 'ar', 'nt', ' t', 'he', ' t', 'ra', 'de', ' o', 'f ', 'lo', 'ck', 'sm', 'it', 'h,', '\\na', 'nd', ' t', 'ol', 'd ', 'hi', 's ', 'fa', 'th', 'er', ' h', 'e ', 'wo', 'ul', 'd ', 'no']\n",
      "\n",
      "Processing file stories/086.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size (characters) (Document 85) 8758\n",
      "sample string (Documents 85) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' k', 'in', 'g ', 'wh', 'o ', 'ha', 'd ', 'a ', 'li', 'tt', 'le', ' b', 'oy', ' i', 'n ', 'wh', 'os', 'e ', 'st', 'ar', 's\\n', 'it', ' h', 'ad', ' b', 'ee', 'n ', 'fo', 're', 'to', 'ld', ' t', 'ha', 't ', 'he', ' s']\n",
      "\n",
      "Processing file stories/087.txt\n",
      "Data size (characters) (Document 86) 3109\n",
      "sample string (Documents 86) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' p', 'ri', 'nc', 'es', 's ', 'wh', 'o ', 'wa', 's ', 'ex', 'tr', 'em', 'el', 'y ', 'pr', 'ou', 'd.', ' i', 'f ', 'a\\n', 'wo', 'oe', 'r ', 'ca', 'me', ' s', 'he', ' g', 'av', 'e ', 'hi', 'm ', 'so', 'me', ' r', 'id']\n",
      "\n",
      "Processing file stories/088.txt\n",
      "Data size (characters) (Document 87) 1365\n",
      "sample string (Documents 87) ['a ', 'ta', 'il', 'or', \"'s\", ' a', 'pp', 're', 'nt', 'ic', 'e ', 'wa', 's ', 'tr', 'av', 'el', 'in', 'g ', 'ab', 'ou', 't ', 'th', 'e ', 'wo', 'rl', 'd ', 'in', ' s', 'ea', 'rc', 'h ', 'of', '\\nw', 'or', 'k,', ' a', 'nd', ' a', 't ', 'on', 'e ', 'ti', 'me', ' h', 'e ', 'co', 'ul', 'd ', 'fi', 'nd']\n",
      "\n",
      "Processing file stories/089.txt\n",
      "Data size (characters) (Document 88) 4538\n",
      "sample string (Documents 88) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' o', 'n ', 'a ', 'ti', 'me', ' a', ' s', 'ol', 'di', 'er', ' w', 'ho', ' f', 'or', ' m', 'an', 'y ', 'ye', 'ar', 's ', 'ha', 'd ', 'se', 'rv', 'ed', ' t', 'he', '\\nk', 'in', 'g ', 'fa', 'it', 'hf', 'ul', 'ly', ', ', 'bu', 't ', 'wh', 'en', ' t', 'he', ' w']\n",
      "\n",
      "Processing file stories/090.txt\n",
      "Data size (characters) (Document 89) 345\n",
      "sample string (Documents 89) ['on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' t', 'he', 're', ' w', 'as', ' a', ' c', 'hi', 'ld', ' w', 'ho', ' w', 'as', ' w', 'il', 'lf', 'ul', ', ', 'an', 'd ', 'wo', 'ul', 'd ', 'no', 't ', 'do', '\\nw', 'ha', 't ', 'he', 'r ', 'mo', 'th', 'er', ' w', 'is', 'he', 'd.', '  ', 'fo', 'r ', 'th']\n",
      "\n",
      "Processing file stories/091.txt\n",
      "Data size (characters) (Document 90) 5460\n",
      "sample string (Documents 90) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' k', 'in', \"g'\", 's ', 'so', 'n,', ' w', 'ho', ' w', 'as', ' n', 'o ', 'lo', 'ng', 'er', ' c', 'on', 'te', 'nt', ' t', 'o ', 'st', 'ay', ' a', 't\\n', 'ho', 'me', ' i', 'n ', 'hi', 's ', 'fa', 'th', 'er', \"'s\", ' h', 'ou', 'se', ', ', 'an', 'd ', 'as']\n",
      "\n",
      "Processing file stories/092.txt\n",
      "Data size (characters) (Document 91) 6854\n",
      "sample string (Documents 91) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' y', 'ou', 'ng', ' h', 'un', 'ts', 'ma', 'n ', 'wh', 'o ', 'we', 'nt', ' i', 'nt', 'o ', 'th', 'e ', 'fo', 're', 'st', ' t', 'o ', 'li', 'e ', 'in', '\\nw', 'ai', 't.', '  ', 'he', ' h', 'ad', ' a', ' f', 're', 'sh', ' a', 'nd', ' j', 'oy', 'ou', 's ']\n",
      "\n",
      "Processing file stories/093.txt\n",
      "Data size (characters) (Document 92) 2314\n",
      "sample string (Documents 92) ['a ', 'po', 'or', ' s', 'er', 'va', 'nt', '-g', 'ir', 'l ', 'wa', 's ', 'on', 'ce', ' t', 'ra', 've', 'li', 'ng', ' w', 'it', 'h ', 'th', 'e ', 'fa', 'mi', 'ly', ' w', 'it', 'h ', 'wh', 'ic', 'h ', 'sh', 'e\\n', 'wa', 's ', 'in', ' s', 'er', 'vi', 'ce', ', ', 'th', 'ro', 'ug', 'h ', 'a ', 'gr', 'ea']\n",
      "\n",
      "Processing file stories/094.txt\n",
      "Data size (characters) (Document 93) 1706\n",
      "sample string (Documents 93) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' m', 'an', ' w', 'ho', ' h', 'ad', ' t', 'hr', 'ee', ' s', 'on', 's,', ' a', 'nd', ' n', 'ot', 'hi', 'ng', ' e', 'ls', 'e ', 'in', ' t', 'he', '\\nw', 'or', 'ld', ' b', 'ut', ' t', 'he', ' h', 'ou', 'se', ' i', 'n ', 'wh', 'ic', 'h ', 'he', ' l', 'iv']\n",
      "\n",
      "Processing file stories/095.txt\n",
      "Data size (characters) (Document 94) 3229\n",
      "sample string (Documents 94) ['th', 'er', 'e ', 'wa', 's ', 'a ', 'gr', 'ea', 't ', 'wa', 'r,', ' a', 'nd', ' t', 'he', ' k', 'in', 'g ', 'ha', 'd ', 'ma', 'ny', ' s', 'ol', 'di', 'er', 's,', ' b', 'ut', ' g', 'av', 'e ', 'th', 'em', '\\ns', 'ma', 'll', ' p', 'ay', ', ', 'so', ' s', 'ma', 'll', ' t', 'ha', 't ', 'th', 'ey', ' c']\n",
      "\n",
      "Processing file stories/096.txt\n",
      "Data size (characters) (Document 95) 4954\n",
      "sample string (Documents 95) ['on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' l', 'iv', 'ed', ' a', ' m', 'an', ' a', 'nd', ' a', ' w', 'om', 'an', ' w', 'ho', ' s', 'o ', 'lo', 'ng', ' a', 's ', 'th', 'ey', ' w', 'er', 'e\\n', 'ri', 'ch', ' h', 'ad', ' n', 'o ', 'ch', 'il', 'dr', 'en', ', ', 'bu', 't ', 'wh', 'en', ' t', 'he']\n",
      "\n",
      "Processing file stories/097.txt\n",
      "Data size (characters) (Document 96) 5732\n",
      "sample string (Documents 96) ['in', ' t', 'he', ' d', 'ay', 's ', 'wh', 'en', ' w', 'is', 'hi', 'ng', ' w', 'as', ' s', 'ti', 'll', ' o', 'f ', 'so', 'me', ' u', 'se', ', ', 'a ', 'ki', 'ng', \"'s\", ' s', 'on', ' w', 'as', '\\nb', 'ew', 'it', 'ch', 'ed', ' b', 'y ', 'an', ' o', 'ld', ' w', 'it', 'ch', ', ', 'an', 'd ', 'sh', 'ut']\n",
      "\n",
      "Processing file stories/098.txt\n",
      "Data size (characters) (Document 97) 4334\n",
      "sample string (Documents 97) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'ma', 'n ', 'wh', 'o ', 'ha', 'd ', 'fo', 'ur', ' s', 'on', 's,', ' a', 'nd', ' w', 'he', 'n ', 'th', 'ey', ' w', 'er', 'e ', 'gr', 'ow', 'n\\n', 'up', ', ', 'he', ' s', 'ai', 'd ', 'to', ' t', 'he', 'm,', ' \"', 'my', ' d', 'ea', 'r ']\n",
      "\n",
      "Processing file stories/099.txt\n",
      "Data size (characters) (Document 98) 7090\n",
      "sample string (Documents 98) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' w', 'om', 'an', ' w', 'ho', ' h', 'ad', ' t', 'hr', 'ee', ' d', 'au', 'gh', 'te', 'rs', ', ', 'th', 'e ', 'el', 'de', 'st', ' o', 'f ', 'wh', 'om', '\\nw', 'as', ' c', 'al', 'le', 'd ', 'on', 'e-', 'ey', 'e,', ' b', 'ec', 'au', 'se', ' s', 'he', ' h']\n",
      "\n",
      "Processing file stories/100.txt\n",
      "Data size (characters) (Document 99) 1007\n",
      "sample string (Documents 99) ['\"g', 'oo', 'd-', 'da', 'y,', ' f', 'at', 'he', 'r ', 'ho', 'll', 'en', 'th', 'e.', '\" ', '\"m', 'an', 'y ', 'th', 'an', 'ks', ', ', 'pi', 'f-', 'pa', 'f-', 'po', 'lt', 'ri', 'e.', '\" ', '\"m', 'ay', ' i', '\\nb', 'e ', 'al', 'lo', 'we', 'd ', 'to', ' h', 'av', 'e ', 'yo', 'ur', ' d', 'au', 'gh', 'te']\n",
      "\n",
      "Processing file stories/101.txt\n",
      "Data size (characters) (Document 100) 3891\n",
      "sample string (Documents 100) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' k', 'in', 'g ', 'wh', 'o ', 'ha', 'd ', 'tw', 'el', 've', ' d', 'au', 'gh', 'te', 'rs', ', ', 'ea', 'ch', ' o', 'ne', '\\nm', 'or', 'e ', 'be', 'au', 'ti', 'fu', 'l ', 'th', 'an', ' t', 'he', ' o', 'th', 'er', '. ']\n",
      "\n",
      "Processing file stories/102.txt\n",
      "Data size (characters) (Document 101) 6953\n",
      "sample string (Documents 101) ['in', ' o', 'ld', 'en', ' t', 'im', 'es', ' t', 'he', 're', ' l', 'iv', 'ed', ' a', 'n ', 'ag', 'ed', ' q', 'ue', 'en', ' w', 'ho', ' w', 'as', ' a', ' s', 'or', 'ce', 're', 'ss', ', ', 'an', 'd ', 'he', 'r\\n', 'da', 'ug', 'ht', 'er', ' w', 'as', ' t', 'he', ' m', 'os', 't ', 'be', 'au', 'ti', 'fu']\n",
      "\n",
      "Processing file stories/103.txt\n",
      "Data size (characters) (Document 102) 4093\n",
      "sample string (Documents 102) ['a ', 'wo', 'ma', 'n ', 'wa', 's ', 'wa', 'lk', 'in', 'g ', 'ab', 'ou', 't ', 'th', 'e ', 'fi', 'el', 'ds', ' w', 'it', 'h ', 'he', 'r ', 'da', 'ug', 'ht', 'er', ' a', 'nd', ' h', 'er', '\\ns', 'te', 'p-', 'da', 'ug', 'ht', 'er', ' c', 'ut', 'ti', 'ng', ' f', 'od', 'de', 'r,', ' w', 'he', 'n ', 'th']\n",
      "\n",
      "Processing file stories/104.txt\n",
      "Data size (characters) (Document 103) 8259\n",
      "sample string (Documents 103) ['**', '*t', 'he', 're', ' w', 'as', ' o', 'nc', 'e ', 'up', 'on', ' a', ' t', 'im', 'e ', 'a ', 'ki', 'ng', ' w', 'ho', ' h', 'ad', ' a', ' g', 're', 'at', ' f', 'or', 'es', 't ', 'ne', 'ar', '\\nh', 'is', ' p', 'al', 'ac', 'e,', ' f', 'ul', 'l ', 'of', ' a', 'll', ' k', 'in', 'ds', ' o', 'f ', 'wi']\n",
      "\n",
      "Processing file stories/105.txt\n",
      "Data size (characters) (Document 104) 1789\n",
      "sample string (Documents 104) ['ea', 'st', ' i', 'nd', 'ia', ' w', 'as', ' b', 'es', 'ie', 'ge', 'd ', 'by', ' a', 'n ', 'en', 'em', 'y ', 'wh', 'o ', 'wo', 'ul', 'd ', 'no', 't ', 're', 'ti', 're', ' u', 'nt', 'il', '\\nh', 'e ', 'ha', 'd ', 're', 'ce', 'iv', 'ed', ' s', 'ix', ' h', 'un', 'dr', 'ed', ' d', 'ol', 'la', 'rs', '. ']\n",
      "\n",
      "Processing file stories/106.txt\n",
      "Data size (characters) (Document 105) 393\n",
      "sample string (Documents 105) ['be', 'tw', 'ee', 'n ', 'we', 'rr', 'el', ' a', 'nd', ' s', 'oi', 'st', ' t', 'he', 're', ' l', 'iv', 'ed', ' a', ' m', 'an', ' w', 'ho', 'se', ' n', 'am', 'e ', 'wa', 's ', 'kn', 'oi', 'st', ',\\n', 'an', 'd ', 'he', ' h', 'ad', ' t', 'hr', 'ee', ' s', 'on', 's.', '  ', 'on', 'e ', 'wa', 's ', 'bl']\n",
      "\n",
      "Processing file stories/107.txt\n",
      "Data size (characters) (Document 106) 385\n",
      "sample string (Documents 106) ['a ', 'gi', 'rl', ' f', 'ro', 'm ', 'br', 'ak', 'el', ' o', 'nc', 'e ', 'we', 'nt', ' t', 'o ', 'st', '. ', 'an', 'ne', \"'s\", ' c', 'ha', 'pe', 'l ', 'at', ' t', 'he', ' f', 'oo', 't\\n', 'of', ' t', 'he', ' h', 'in', 'ne', 'nb', 'er', 'g,', ' a', 'nd', ' a', 's ', 'sh', 'e ', 'wa', 'nt', 'ed', ' t']\n",
      "\n",
      "Processing file stories/108.txt\n",
      "Data size (characters) (Document 107) 448\n",
      "sample string (Documents 107) ['wh', 'it', 'he', 'r ', 'do', ' y', 'ou', ' g', 'o.', '  ', 'to', ' w', 'al', 'pe', '. ', ' i', ' t', 'o ', 'wa', 'lp', 'e,', ' y', 'ou', ' t', 'o ', 'wa', 'lp', 'e,', ' s', 'o,', '\\ns', 'o,', ' t', 'og', 'et', 'he', 'r ', 'we', \"'l\", 'l ', 'go', '.\\n', 'ha', 've', ' y', 'ou', ' a', ' m', 'an', '. ']\n",
      "\n",
      "Processing file stories/109.txt\n",
      "Data size (characters) (Document 108) 1722\n",
      "sample string (Documents 108) ['th', 'er', 'e ', 'we', 're', ' o', 'nc', 'e ', 'a ', 'li', 'tt', 'le', ' b', 'ro', 'th', 'er', ' a', 'nd', ' a', ' l', 'it', 'tl', 'e ', 'si', 'st', 'er', ', ', 'wh', 'o ', 'lo', 've', 'd\\n', 'ea', 'ch', ' o', 'th', 'er', ' w', 'it', 'h ', 'al', 'l ', 'th', 'ei', 'r ', 'he', 'ar', 'ts', '. ', ' t']\n",
      "\n",
      "Processing file stories/110.txt\n",
      "Data size (characters) (Document 109) 2264\n",
      "sample string (Documents 109) ['th', 'er', 'e ', 'we', 're', ' o', 'nc', 'e ', 'tw', 'o ', 'br', 'ot', 'he', 'rs', ', ', 'th', 'e ', 'on', 'e ', 'ri', 'ch', ', ', 'th', 'e ', 'ot', 'he', 'r ', 'po', 'or', '.\\n', 'th', 'e ', 'ri', 'ch', ' o', 'ne', ', ', 'ho', 'we', 've', 'r,', ' g', 'av', 'e ', 'no', 'th', 'in', 'g ', 'to', ' t']\n",
      "\n",
      "Processing file stories/111.txt\n",
      "Data size (characters) (Document 110) 1018\n",
      "sample string (Documents 110) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'wo', 'ma', 'n ', 'wh', 'o ', 'ha', 'd ', 'a ', 'so', 'n,', ' w', 'ho', ' m', 'uc', 'h ', 'wi', 'sh', 'ed', ' t', 'o\\n', 'tr', 'av', 'el', ', ', 'bu', 't ', 'hi', 's ', 'mo', 'th', 'er', ' s', 'ai', 'd,', ' h', 'ow', ' c', 'an', ' y']\n",
      "\n",
      "Processing file stories/112.txt\n",
      "Data size (characters) (Document 111) 3286\n",
      "sample string (Documents 111) ['on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' t', 'he', 're', ' l', 'iv', 'ed', ' a', ' k', 'in', 'g ', 'an', 'd ', 'a ', 'qu', 'ee', 'n,', ' w', 'ho', ' w', 'er', 'e ', 'ri', 'ch', ',\\n', 'an', 'd ', 'ha', 'd ', 'ev', 'er', 'yt', 'hi', 'ng', ' t', 'he', 'y ', 'wa', 'nt', 'ed', ', ', 'bu', 't ']\n",
      "\n",
      "Processing file stories/113.txt\n",
      "Data size (characters) (Document 112) 405\n",
      "sample string (Documents 112) ['a ', 'ma', 'n ', 'an', 'd ', 'hi', 's ', 'wi', 'fe', ' w', 'er', 'e ', 'on', 'ce', ' s', 'it', 'ti', 'ng', ' b', 'y ', 'th', 'e ', 'do', 'or', ' o', 'f ', 'th', 'ei', 'r ', 'ho', 'us', 'e,', '\\na', 'nd', ' t', 'he', 'y ', 'ha', 'd ', 'a ', 'ro', 'as', 'te', 'd ', 'ch', 'ic', 'ke', 'n ', 'se', 't ']\n",
      "\n",
      "Processing file stories/114.txt\n",
      "Data size (characters) (Document 113) 2914\n",
      "sample string (Documents 113) ['th', 'er', 'e ', 'we', 're', ' o', 'nc', 'e ', 'tw', 'o ', 'br', 'ot', 'he', 'rs', ' w', 'ho', ' b', 'ot', 'h ', 'se', 'rv', 'ed', ' a', 's ', 'so', 'ld', 'ie', 'rs', ', ', 'on', 'e ', 'of', '\\nt', 'he', 'm ', 'wa', 's ', 'ri', 'ch', ', ', 'an', 'd ', 'th', 'e ', 'ot', 'he', 'r ', 'po', 'or', '. ']\n",
      "\n",
      "Processing file stories/115.txt\n",
      "Data size (characters) (Document 114) 1433\n",
      "sample string (Documents 114) ['at', ' t', 'he', ' t', 'im', 'e ', 'wh', 'en', ' o', 'ur', ' l', 'or', 'd ', 'st', 'il', 'l ', 'wa', 'lk', 'ed', ' t', 'hi', 's ', 'ea', 'rt', 'h,', ' h', 'e ', 'an', 'd ', 'st', '.\\n', 'pe', 'te', 'r ', 'st', 'op', 'pe', 'd ', 'on', 'e ', 'ev', 'en', 'in', 'g ', 'at', ' a', ' s', 'mi', 'th', \"'s\"]\n",
      "\n",
      "Processing file stories/116.txt\n",
      "Data size (characters) (Document 115) 545\n",
      "sample string (Documents 115) ['a ', 'ce', 'rt', 'ai', 'n ', 'ki', 'ng', ' h', 'ad', ' t', 'hr', 'ee', ' s', 'on', 's ', 'wh', 'o ', 'we', 're', ' a', 'll', ' e', 'qu', 'al', 'ly', ' d', 'ea', 'r ', 'to', '\\nh', 'im', ', ', 'an', 'd ', 'he', ' d', 'id', ' n', 'ot', ' k', 'no', 'w ', 'wh', 'ic', 'h ', 'of', ' t', 'he', 'm ', 'to']\n",
      "\n",
      "Processing file stories/117.txt\n",
      "Data size (characters) (Document 116) 877\n",
      "sample string (Documents 116) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' s', 'he', 'ph', 'er', 'd ', 'bo', 'y ', 'wh', 'os', 'e ', 'fa', 'me', ' s', 'pr', 'ea', 'd\\n', 'fa', 'r ', 'an', 'd ', 'wi', 'de', ' b', 'ec', 'au', 'se', ' o', 'f ', 'th', 'e ', 'wi', 'se', ' a', 'ns', 'we', 'rs']\n",
      "\n",
      "Processing file stories/118.txt\n",
      "Data size (characters) (Document 117) 801\n",
      "sample string (Documents 117) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' l', 'it', 'tl', 'e ', 'gi', 'rl', ' w', 'ho', 'se', ' f', 'at', 'he', 'r ', 'an', 'd ', 'mo', 'th', 'er', '\\nw', 'er', 'e ', 'de', 'ad', ', ', 'an', 'd ', 'sh', 'e ', 'wa', 's ', 'so', ' p', 'oo', 'r ', 'th', 'at']\n",
      "\n",
      "Processing file stories/119.txt\n",
      "Data size (characters) (Document 118) 889\n",
      "sample string (Documents 118) ['a ', 'fa', 'th', 'er', ' w', 'as', ' o', 'ne', ' d', 'ay', ' s', 'it', 'ti', 'ng', ' a', 't ', 'di', 'nn', 'er', ' w', 'it', 'h ', 'hi', 's ', 'wi', 'fe', ' a', 'nd', ' h', 'is', '\\nc', 'hi', 'ld', 're', 'n,', ' a', 'nd', ' a', ' g', 'oo', 'd ', 'fr', 'ie', 'nd', ' w', 'ho', ' h', 'ad', ' c', 'om']\n",
      "\n",
      "Processing file stories/120.txt\n",
      "Data size (characters) (Document 119) 404\n",
      "sample string (Documents 119) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' y', 'ou', 'ng', ' s', 'he', 'ph', 'er', 'd ', 'wh', 'o ', 'wa', 'nt', 'ed', ' v', 'er', 'y ', 'mu', 'ch', ' t', 'o ', 'ma', 'rr', 'y,', '\\na', 'nd', ' w', 'as', ' a', 'cq', 'ua', 'in', 'te', 'd ', 'wi', 'th', ' t', 'hr', 'ee', ' s', 'is', 'te', 'rs']\n",
      "\n",
      "Processing file stories/121.txt\n",
      "Data size (characters) (Document 120) 2545\n",
      "sample string (Documents 120) ['a ', 'sp', 'ar', 'ro', 'w ', 'ha', 'd ', 'fo', 'ur', ' y', 'ou', 'ng', ' o', 'ne', 's ', 'in', ' a', ' s', 'wa', 'll', 'ow', \"'s\", ' n', 'es', 't.', '  ', 'wh', 'en', ' t', 'he', 'y\\n', 'we', 're', ' f', 'le', 'dg', 'ed', ', ', 'so', 'me', ' n', 'au', 'gh', 'ty', ' b', 'oy', 's ', 'pu', 'll', 'ed']\n",
      "\n",
      "Processing file stories/122.txt\n",
      "Data size (characters) (Document 121) 6387\n",
      "sample string (Documents 121) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'wi', 'do', 'w ', 'wh', 'o ', 'li', 've', 'd ', 'in', ' a', ' l', 'on', 'el', 'y ', 'co', 'tt', 'ag', 'e.', '  ', 'in', '\\nf', 'ro', 'nt', ' o', 'f ', 'th', 'e ', 'co', 'tt', 'ag', 'e ', 'wa', 's ', 'a ', 'ga', 'rd', 'en', ' w', 'he']\n",
      "\n",
      "Processing file stories/123.txt\n",
      "Data size (characters) (Document 122) 6215\n",
      "sample string (Documents 122) ['le', 't ', 'no', ' o', 'ne', ' e', 've', 'r ', 'sa', 'y ', 'th', 'at', ' a', ' p', 'oo', 'r ', 'ta', 'il', 'or', ' c', 'an', 'no', 't ', 'do', ' g', 're', 'at', ' t', 'hi', 'ng', 's\\n', 'an', 'd ', 'wi', 'n ', 'hi', 'gh', ' h', 'on', 'or', 's.', '  ', 'al', 'l ', 'th', 'at', ' i', 's ', 'ne', 'ed']\n",
      "\n",
      "Processing file stories/124.txt\n",
      "Data size (characters) (Document 123) 2567\n",
      "sample string (Documents 123) ['ha', 'rr', 'y ', 'wa', 's ', 'la', 'zy', ', ', 'an', 'd ', 'al', 'th', 'ou', 'gh', ' h', 'e ', 'ha', 'd ', 'no', 'th', 'in', 'g ', 'el', 'se', ' t', 'o ', 'do', ' b', 'ut', '\\nd', 'ri', 've', ' h', 'is', ' g', 'oa', 't ', 'da', 'il', 'y ', 'to', ' p', 'as', 'tu', 're', ', ', 'he', ' n', 'ev', 'er']\n",
      "\n",
      "Processing file stories/125.txt\n",
      "Data size (characters) (Document 124) 7174\n",
      "sample string (Documents 124) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' k', 'in', 'g,', ' b', 'ut', ' w', 'he', 're', ' h', 'e ', 're', 'ig', 'ne', 'd ', 'an', 'd ', 'wh', 'at', '\\nh', 'e ', 'wa', 's ', 'ca', 'll', 'ed', ', ', 'i ', 'do', ' n', 'ot', ' k', 'no', 'w.', '  ', 'he', ' h']\n",
      "\n",
      "Processing file stories/126.txt\n",
      "Data size (characters) (Document 125) 6823\n",
      "sample string (Documents 125) ['th', 'er', 'e ', 'we', 're', ' o', 'nc', 'e ', 'a ', 'ma', 'n ', 'an', 'd ', 'a ', 'wo', 'ma', 'n ', 'wh', 'o ', 'ha', 'd ', 'an', ' o', 'nl', 'y ', 'ch', 'il', 'd,', ' a', 'nd', '\\nl', 'iv', 'ed', ' q', 'ui', 'te', ' a', 'lo', 'ne', ' i', 'n ', 'a ', 'so', 'li', 'ta', 'ry', ' v', 'al', 'le', 'y.']\n",
      "\n",
      "Processing file stories/127.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size (characters) (Document 126) 5064\n",
      "sample string (Documents 126) ['a ', 'po', 'or', ' w', 'oo', 'd-', 'cu', 'tt', 'er', ' l', 'iv', 'ed', ' w', 'it', 'h ', 'hi', 's ', 'wi', 'fe', ' a', 'nd', ' t', 'hr', 'ee', ' d', 'au', 'gh', 'te', 'rs', ' i', 'n\\n', 'a ', 'li', 'tt', 'le', ' h', 'ut', ' o', 'n ', 'th', 'e ', 'ed', 'ge', ' o', 'f ', 'a ', 'lo', 'ne', 'ly', ' f']\n",
      "\n",
      "Processing file stories/128.txt\n",
      "Data size (characters) (Document 127) 10096\n",
      "sample string (Documents 127) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' v', 'er', 'y ', 'ol', 'd ', 'wo', 'ma', 'n,', ' w', 'ho', ' l', 'iv', 'ed', ' w', 'it', 'h ', 'he', 'r\\n', 'fl', 'oc', 'k ', 'of', ' g', 'ee', 'se', ' i', 'n ', 'a ', 're', 'mo', 'te', ' c', 'le', 'ar', 'in', 'g ']\n",
      "\n",
      "Processing file stories/129.txt\n",
      "Data size (characters) (Document 128) 1845\n",
      "sample string (Documents 128) ['wh', 'en', ' a', 'da', 'm ', 'an', 'd ', 'ev', 'e ', 'we', 're', ' d', 'ri', 've', 'n ', 'ou', 't ', 'of', ' p', 'ar', 'ad', 'is', 'e,', ' t', 'he', 'y ', 'we', 're', '\\nc', 'om', 'pe', 'll', 'ed', ' t', 'o ', 'bu', 'il', 'd ', 'a ', 'ho', 'us', 'e ', 'fo', 'r ', 'th', 'em', 'se', 'lv', 'es', ' o']\n",
      "\n",
      "Processing file stories/130.txt\n",
      "Data size (characters) (Document 129) 5643\n",
      "sample string (Documents 129) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' m', 'il', 'le', 'r ', 'wh', 'o ', 'li', 've', 'd ', 'wi', 'th', ' h', 'is', ' w', 'if', 'e\\n', 'in', ' g', 're', 'at', ' c', 'on', 'te', 'nt', 'me', 'nt', '. ', ' t', 'he', 'y ', 'ha', 'd ', 'mo', 'ne', 'y ', 'an']\n",
      "\n",
      "Processing file stories/131.txt\n",
      "Data size (characters) (Document 130) 3172\n",
      "sample string (Documents 130) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'sh', 'ep', 'he', 'rd', '-b', 'oy', ' w', 'ho', 'se', ' f', 'at', 'he', 'r ', 'an', 'd ', 'mo', 'th', 'er', '\\nw', 'er', 'e ', 'de', 'ad', ', ', 'an', 'd ', 'he', ' w', 'as', ' p', 'la', 'ce', 'd ', 'by', ' t', 'he', ' a', 'ut', 'ho']\n",
      "\n",
      "Processing file stories/132.txt\n",
      "Data size (characters) (Document 131) 7003\n",
      "sample string (Documents 131) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' g', 'ir', 'l ', 'wh', 'o ', 'wa', 's ', 'yo', 'un', 'g ', 'an', 'd ', 'be', 'au', 'ti', 'fu', 'l,', ' b', 'ut', '\\ns', 'he', ' h', 'ad', ' l', 'os', 't ', 'he', 'r ', 'mo', 'th', 'er', ' w', 'he', 'n ', 'sh', 'e ']\n",
      "\n",
      "Processing file stories/133.txt\n",
      "Data size (characters) (Document 132) 2910\n",
      "sample string (Documents 132) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' g', 'ir', 'l ', 'wh', 'os', 'e ', 'fa', 'th', 'er', ' a', 'nd', ' m', 'ot', 'he', 'r ', 'di', 'ed', ' w', 'hi', 'le', '\\ns', 'he', ' w', 'as', ' s', 'ti', 'll', ' a', ' l', 'it', 'tl', 'e ', 'ch', 'il', 'd.', '  ', 'al', 'l ', 'al', 'on', 'e,', ' i']\n",
      "\n",
      "Processing file stories/134.txt\n",
      "Data size (characters) (Document 133) 3316\n",
      "sample string (Documents 133) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' p', 'ri', 'nc', 'es', 's,', ' w', 'ho', ', ', 'hi', 'gh', ' u', 'nd', 'er', ' t', 'he', '\\nb', 'at', 'tl', 'em', 'en', 'ts', ' i', 'n ', 'he', 'r ', 'ca', 'st', 'le', ', ', 'ha', 'd ', 'an', ' a', 'pa', 'rt', 'me']\n",
      "\n",
      "Processing file stories/135.txt\n",
      "Data size (characters) (Document 134) 7859\n",
      "sample string (Documents 134) ['on', 'e ', 'da', 'y ', 'an', ' o', 'ld', ' m', 'an', ' a', 'nd', ' h', 'is', ' w', 'if', 'e ', 'we', 're', ' s', 'it', 'ti', 'ng', ' i', 'n ', 'fr', 'on', 't ', 'of', ' a', '\\nm', 'is', 'er', 'ab', 'le', ' h', 'ou', 'se', ' r', 'es', 'ti', 'ng', ' a', ' w', 'hi', 'le', ' f', 'ro', 'm ', 'th', 'ei']\n",
      "\n",
      "Processing file stories/136.txt\n",
      "Data size (characters) (Document 135) 9842\n",
      "sample string (Documents 135) ['a ', 'yo', 'un', 'g ', 'dr', 'um', 'me', 'r ', 'we', 'nt', ' o', 'ut', ' q', 'ui', 'te', ' a', 'lo', 'ne', ' o', 'ne', ' e', 've', 'ni', 'ng', ' i', 'nt', 'o ', 'th', 'e ', 'co', 'un', 'tr', 'y,', '\\na', 'nd', ' c', 'am', 'e ', 'to', ' a', ' l', 'ak', 'e ', 'on', ' t', 'he', ' s', 'ho', 're', ' o']\n",
      "\n",
      "Processing file stories/137.txt\n",
      "Data size (characters) (Document 136) 637\n",
      "sample string (Documents 136) ['in', ' f', 'or', 'me', 'r ', 'ti', 'me', 's,', ' w', 'he', 'n ', 'go', 'd ', 'hi', 'ms', 'el', 'f ', 'st', 'il', 'l ', 'wa', 'lk', 'ed', ' t', 'he', ' e', 'ar', 'th', ', ', 'th', 'e\\n', 'fr', 'ui', 'tf', 'ul', 'ne', 'ss', ' o', 'f ', 'th', 'e ', 'so', 'il', ' w', 'as', ' m', 'uc', 'h ', 'gr', 'ea']\n",
      "\n",
      "Processing file stories/138.txt\n",
      "Data size (characters) (Document 137) 1984\n",
      "sample string (Documents 137) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' k', 'in', 'g ', 'wh', 'o ', 'ha', 'd ', 'a ', 'da', 'ug', 'ht', 'er', ',\\n', 'an', 'd ', 'he', ' c', 'au', 'se', 'd ', 'a ', 'gl', 'as', 's ', 'mo', 'un', 'ta', 'in', ' t', 'o ', 'be', ' m', 'ad', 'e,', ' a', 'nd']\n",
      "\n",
      "Processing file stories/139.txt\n",
      "Data size (characters) (Document 138) 2746\n",
      "sample string (Documents 138) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', 'n ', 'en', 'ch', 'an', 'tr', 'es', 's,', ' w', 'ho', ' h', 'ad', ' t', 'hr', 'ee', ' s', 'on', 's ', 'wh', 'o ', 'lo', 've', 'd ', 'ea', 'ch', '\\no', 'th', 'er', ' a', 's ', 'br', 'ot', 'he', 'rs', ', ', 'bu', 't ', 'th', 'e ', 'ol', 'd ', 'wo', 'ma']\n",
      "\n",
      "Processing file stories/140.txt\n",
      "Data size (characters) (Document 139) 5265\n",
      "sample string (Documents 139) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' k', 'in', 'g ', 'wh', 'o ', 'ha', 'd ', 'a ', 'so', 'n ', 'wh', 'o ', 'as', 'ke', 'd ', 'in', ' m', 'ar', 'ri', 'ag', 'e ', 'th', 'e\\n', 'da', 'ug', 'ht', 'er', ' o', 'f ', 'a ', 'mi', 'gh', 'ty', ' k', 'in', 'g,', ' s', 'he', ' w', 'as', ' c', 'al']\n",
      "\n",
      "Processing file stories/141.txt\n",
      "Data size (characters) (Document 140) 2943\n",
      "sample string (Documents 140) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' o', 'n ', 'a ', 'ti', 'me', ' a', ' m', 'ot', 'he', 'r ', 'wh', 'o ', 'ha', 'd ', 'th', 're', 'e ', 'da', 'ug', 'ht', 'er', 's,', '\\nt', 'he', ' e', 'ld', 'es', 't ', 'of', ' w', 'ho', 'm ', 'wa', 's ', 'ru', 'de', ' a', 'nd', ' w', 'ic', 'ke', 'd,', ' t']\n",
      "\n",
      "Processing file stories/142.txt\n",
      "Data size (characters) (Document 141) 1053\n",
      "sample string (Documents 141) ['th', 're', 'e ', 'hu', 'nd', 're', 'd ', 'ye', 'ar', 's ', 'be', 'fo', 're', ' t', 'he', ' b', 'ir', 'th', ' o', 'f ', 'th', 'e ', 'lo', 'rd', ' c', 'hr', 'is', 't,', ' t', 'he', 're', '\\nl', 'iv', 'ed', ' a', ' m', 'ot', 'he', 'r ', 'wh', 'o ', 'ha', 'd ', 'tw', 'el', 've', ' s', 'on', 's,', ' b']\n",
      "\n",
      "Processing file stories/143.txt\n",
      "Data size (characters) (Document 142) 398\n",
      "sample string (Documents 142) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'wo', 'ma', 'n ', 'wh', 'o ', 'ha', 'd ', 'tw', 'o ', 'ch', 'il', 'dr', 'en', '. ', ' t', 'he', ' y', 'ou', 'ng', 'es', 't\\n', 'ha', 'd ', 'to', ' g', 'o ', 'ev', 'er', 'y ', 'da', 'y ', 'in', 'to', ' t', 'he', ' f', 'or', 'es', 't ']\n",
      "\n",
      "Processing file stories/144.txt\n",
      "Data size (characters) (Document 143) 1251\n",
      "sample string (Documents 143) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' k', 'in', \"g'\", 's ', 'so', 'n ', 'wh', 'o ', 'we', 'nt', ' o', 'ut', ' i', 'nt', 'o ', 'th', 'e ', 'wo', 'rl', 'd,', ' a', 'nd', '\\nh', 'e ', 'wa', 's ', 'fu', 'll', ' o', 'f ', 'th', 'ou', 'gh', 't ', 'an', 'd ', 'sa', 'd.', '  ', 'he', ' l', 'oo']\n",
      "\n",
      "Processing file stories/145.txt\n",
      "Data size (characters) (Document 144) 639\n",
      "sample string (Documents 144) ['th', 'er', 'e ', 'we', 're', ' o', 'nc', 'e ', 'up', 'on', ' a', ' t', 'im', 'e ', 'tw', 'o ', 'si', 'st', 'er', 's,', ' o', 'ne', ' o', 'f ', 'wh', 'om', ' h', 'ad', ' n', 'o\\n', 'ch', 'il', 'dr', 'en', ' a', 'nd', ' w', 'as', ' r', 'ic', 'h,', ' a', 'nd', ' t', 'he', ' o', 'th', 'er', ' h', 'ad']\n",
      "\n",
      "Processing file stories/146.txt\n",
      "Data size (characters) (Document 145) 2451\n",
      "sample string (Documents 145) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' h', 'er', 'mi', 't ', 'wh', 'o ', 'li', 've', 'd ', 'in', ' a', ' f', 'or', 'es', 't ', 'at', ' t', 'he', '\\nf', 'oo', 't ', 'of', ' a', ' m', 'ou', 'nt', 'ai', 'n,', ' a', 'nd', ' p', 'as', 'se', 'd ', 'hi', 's ']\n",
      "\n",
      "Processing file stories/147.txt\n",
      "Data size (characters) (Document 146) 887\n",
      "sample string (Documents 146) ['in', ' a', ' l', 'ar', 'ge', ' t', 'ow', 'n ', 'th', 'er', 'e ', 'wa', 's ', 'an', ' o', 'ld', ' w', 'om', 'an', ' w', 'ho', ' s', 'at', ' i', 'n ', 'th', 'e ', 'ev', 'en', 'in', 'g ', 'al', 'on', 'e\\n', 'in', ' h', 'er', ' r', 'oo', 'm ', 'th', 'in', 'ki', 'ng', ' h', 'ow', ' s', 'he', ' h', 'ad']\n",
      "\n",
      "Processing file stories/148.txt\n",
      "Data size (characters) (Document 147) 566\n",
      "sample string (Documents 147) ['on', 'e ', 'af', 'te', 'rn', 'oo', 'n ', 'th', 'e ', 'ch', 'ri', 'st', '-c', 'hi', 'ld', ' h', 'ad', ' l', 'ai', 'd ', 'hi', 'ms', 'el', 'f ', 'in', ' h', 'is', ' c', 'ra', 'dl', 'e-', 'be', 'd\\n', 'an', 'd ', 'ha', 'd ', 'fa', 'll', 'en', ' a', 'sl', 'ee', 'p.', '  ', 'th', 'en', ' h', 'is', ' m']\n",
      "\n",
      "Processing file stories/149.txt\n",
      "Data size (characters) (Document 148) 2517\n",
      "sample string (Documents 148) ['a ', 'ce', 'rt', 'ai', 'n ', 'ca', 't ', 'ha', 'd ', 'ma', 'de', ' t', 'he', ' a', 'cq', 'ua', 'in', 'ta', 'nc', 'e ', 'of', ' a', ' m', 'ou', 'se', ', ', 'an', 'd ', 'ha', 'd\\n', 'sa', 'id', ' s', 'o ', 'mu', 'ch', ' t', 'o ', 'he', 'r ', 'ab', 'ou', 't ', 'th', 'e ', 'gr', 'ea', 't ', 'lo', 've']\n",
      "\n",
      "Processing file stories/150.txt\n",
      "Data size (characters) (Document 149) 2765\n",
      "sample string (Documents 149) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' w', 'on', 'de', 'rf', 'ul', ' m', 'us', 'ic', 'ia', 'n,', ' w', 'ho', ' w', 'en', 't ', 'qu', 'it', 'e ', 'fo', 'rl', 'or', 'n\\n', 'th', 'ro', 'ug', 'h ', 'a ', 'fo', 're', 'st', ' a', 'nd', ' t', 'ho', 'ug', 'ht', ' o', 'f ', 'al', 'l ', 'ma', 'nn']\n",
      "\n",
      "Processing file stories/151.txt\n",
      "Data size (characters) (Document 150) 2060\n",
      "sample string (Documents 150) ['th', 'e ', 'co', 'ck', ' o', 'nc', 'e ', 'sa', 'id', ' t', 'o ', 'th', 'e ', 'he', 'n,', ' i', 't ', 'is', ' n', 'ow', ' t', 'he', ' t', 'im', 'e ', 'wh', 'en', ' t', 'he', ' n', 'ut', 's\\n', 'ar', 'e ', 'ri', 'pe', ', ', 'so', ' l', 'et', ' u', 's ', 'go', ' t', 'o ', 'th', 'e ', 'hi', 'll', ' t']\n",
      "\n",
      "Processing file stories/152.txt\n",
      "Data size (characters) (Document 151) 1331\n",
      "sample string (Documents 151) ['in', ' a', ' v', 'il', 'la', 'ge', ' d', 'we', 'lt', ' a', ' p', 'oo', 'r ', 'ol', 'd ', 'wo', 'ma', 'n,', ' w', 'ho', ' h', 'ad', ' g', 'at', 'he', 're', 'd ', 'to', 'ge', 'th', 'er', ' a', '\\nd', 'is', 'h ', 'of', ' b', 'ea', 'ns', ' a', 'nd', ' w', 'an', 'te', 'd ', 'to', ' c', 'oo', 'k ', 'th']\n",
      "\n",
      "Processing file stories/153.txt\n",
      "Data size (characters) (Document 152) 7907\n",
      "sample string (Documents 152) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' f', 'is', 'he', 'rm', 'an', ' w', 'ho', ' l', 'iv', 'ed', ' w', 'it', 'h ', 'hi', 's ', 'wi', 'fe', '\\ni', 'n ', 'a ', 'pi', 'g-', 'st', 'ye', ' c', 'lo', 'se', ' b', 'y ', 'th', 'e ', 'se', 'a,', ' a', 'nd', ' e']\n",
      "\n",
      "Processing file stories/154.txt\n",
      "Data size (characters) (Document 153) 1611\n",
      "sample string (Documents 153) ['on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' m', 'ou', 'se', ', ', 'a ', 'bi', 'rd', ', ', 'an', 'd ', 'a ', 'sa', 'us', 'ag', 'e ', 'be', 'ca', 'me', '\\nc', 'om', 'pa', 'ni', 'on', 's,', ' k', 'ep', 't ', 'ho', 'us', 'e ', 'to', 'ge', 'th', 'er', ', ', 'li', 've', 'd ', 'we', 'll', ' a']\n",
      "\n",
      "Processing file stories/155.txt\n",
      "Data size (characters) (Document 154) 3244\n",
      "sample string (Documents 154) ['a ', 'ce', 'rt', 'ai', 'n ', 'ma', 'n ', 'ha', 'd ', 'a ', 'do', 'nk', 'ey', ', ', 'wh', 'ic', 'h ', 'ha', 'd ', 'ca', 'rr', 'ie', 'd ', 'th', 'e ', 'co', 'rn', '-s', 'ac', 'ks', '\\nt', 'o ', 'th', 'e ', 'mi', 'll', ' i', 'nd', 'ef', 'at', 'ig', 'ab', 'ly', ' f', 'or', ' m', 'an', 'y ', 'a ', 'lo']\n",
      "\n",
      "Processing file stories/156.txt\n",
      "Data size (characters) (Document 155) 1407\n",
      "sample string (Documents 155) ['a ', 'lo', 'us', 'e ', 'an', 'd ', 'a ', 'fl', 'ea', ' k', 'ep', 't ', 'ho', 'us', 'e ', 'to', 'ge', 'th', 'er', ' a', 'nd', ' w', 'er', 'e ', 'br', 'ew', 'in', 'g ', 'be', 'er', '\\ni', 'n ', 'an', ' e', 'gg', '-s', 'he', 'll', '. ', ' t', 'he', 'n ', 'th', 'e ', 'li', 'tt', 'le', ' l', 'ou', 'se']\n",
      "\n",
      "Processing file stories/157.txt\n",
      "Data size (characters) (Document 156) 2016\n",
      "sample string (Documents 156) ['on', 'e ', 've', 'ry', ' f', 'in', 'e ', 'da', 'y ', 'it', ' c', 'am', 'e ', 'to', ' p', 'as', 's ', 'th', 'at', ' t', 'he', ' g', 'oo', 'd ', 'go', 'd ', 'wi', 'sh', 'ed', ' t', 'o ', 'en', 'jo', 'y\\n', 'hi', 'ms', 'el', 'f ', 'in', ' t', 'he', ' h', 'ea', 've', 'nl', 'y ', 'ga', 'rd', 'en', ', ']\n",
      "\n",
      "Processing file stories/158.txt\n",
      "Data size (characters) (Document 157) 2073\n",
      "sample string (Documents 157) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', 'n ', 'ol', 'd ', 'fo', 'x ', 'wi', 'th', ' n', 'in', 'e ', 'ta', 'il', 's,', ' w', 'ho', '\\nb', 'el', 'ie', 've', 'd ', 'th', 'at', ' h', 'is', ' w', 'if', 'e ', 'wa', 's ', 'no', 't ', 'fa', 'it', 'hf', 'ul', ' t']\n",
      "\n",
      "Processing file stories/159.txt\n",
      "Data size (characters) (Document 158) 1700\n",
      "sample string (Documents 158) ['a ', 'sh', 'oe', 'ma', 'ke', 'r,', ' b', 'y ', 'no', ' f', 'au', 'lt', ' o', 'f ', 'hi', 's ', 'ow', 'n,', ' h', 'ad', ' b', 'ec', 'om', 'e ', 'so', ' p', 'oo', 'r ', 'th', 'at', ' a', 't\\n', 'la', 'st', ' h', 'e ', 'ha', 'd ', 'no', 'th', 'in', 'g ', 'le', 'ft', ' b', 'ut', ' l', 'ea', 'th', 'er']\n",
      "\n",
      "Processing file stories/160.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size (characters) (Document 159) 989\n",
      "sample string (Documents 159) ['th', 'er', 'e ', 'we', 're', ' o', 'nc', 'e ', 'a ', 'co', 'ck', ' a', 'nd', ' a', ' h', 'en', ' w', 'ho', ' w', 'an', 'te', 'd ', 'to', ' t', 'ak', 'e ', 'a ', 'jo', 'ur', 'ne', 'y\\n', 'to', 'ge', 'th', 'er', '. ', ' s', 'o ', 'th', 'e ', 'co', 'ck', ' b', 'ui', 'lt', ' a', ' b', 'ea', 'ut', 'if']\n",
      "\n",
      "Processing file stories/161.txt\n",
      "Data size (characters) (Document 160) 3114\n",
      "sample string (Documents 160) ['a ', 'sh', 'ee', 'p-', 'do', 'g ', 'ha', 'd ', 'no', 't ', 'a ', 'go', 'od', ' m', 'as', 'te', 'r,', ' b', 'ut', ', ', 'on', ' t', 'he', ' c', 'on', 'tr', 'ar', 'y,', ' o', 'ne', ' w', 'ho', '\\nl', 'et', ' h', 'im', ' s', 'uf', 'fe', 'r ', 'hu', 'ng', 'er', '. ', ' a', 's ', 'he', ' c', 'ou', 'ld']\n",
      "\n",
      "Processing file stories/162.txt\n",
      "Data size (characters) (Document 161) 6731\n",
      "sample string (Documents 161) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' m', 'an', ' w', 'ho', ' w', 'as', ' c', 'al', 'le', 'd ', 'fr', 'ed', 'er', 'ic', 'k\\n', 'an', 'd ', 'a ', 'wo', 'ma', 'n ', 'ca', 'll', 'ed', ' c', 'at', 'he', 'ri', 'ne', ', ', 'wh', 'o ', 'ha', 'd ', 'ma', 'rr']\n",
      "\n",
      "Processing file stories/163.txt\n",
      "Data size (characters) (Document 162) 5357\n",
      "sample string (Documents 162) ['th', 'er', 'e ', 'wa', 's ', 'a ', 'ce', 'rt', 'ai', 'n ', 'vi', 'll', 'ag', 'e ', 'wh', 'er', 'ei', 'n ', 'no', ' o', 'ne', ' l', 'iv', 'ed', ' b', 'ut', ' r', 'ea', 'll', 'y ', 'ri', 'ch', '\\np', 'ea', 'sa', 'nt', 's,', ' a', 'nd', ' j', 'us', 't ', 'on', 'e ', 'po', 'or', ' o', 'ne', ', ', 'wh']\n",
      "\n",
      "Processing file stories/164.txt\n",
      "Data size (characters) (Document 163) 2546\n",
      "sample string (Documents 163) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', 'n ', 'ol', 'd ', 'ca', 'st', 'le', ' i', 'n ', 'th', 'e ', 'mi', 'ds', 't ', 'of', ' a', ' l', 'ar', 'ge', ' a', 'nd', ' d', 'en', 'se', '\\nf', 'or', 'es', 't,', ' a', 'nd', ' i', 'n ', 'it', ' a', 'n ', 'ol', 'd ', 'wo', 'ma', 'n ', 'wh', 'o ', 'wa']\n",
      "\n",
      "Processing file stories/165.txt\n",
      "Data size (characters) (Document 164) 1018\n",
      "sample string (Documents 164) ['on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' t', 'he', ' f', 'ox', ' w', 'as', ' t', 'al', 'ki', 'ng', ' t', 'o ', 'th', 'e ', 'wo', 'lf', ' o', 'f ', 'th', 'e\\n', 'st', 're', 'ng', 'th', ' o', 'f ', 'ma', 'n.', '  ', 'ho', 'w ', 'no', ' a', 'ni', 'ma', 'l ', 'co', 'ul', 'd ', 'wi', 'th', 'st']\n",
      "\n",
      "Processing file stories/166.txt\n",
      "Data size (characters) (Document 165) 1827\n",
      "sample string (Documents 165) ['th', 'e ', 'wo', 'lf', ' h', 'ad', ' t', 'he', ' f', 'ox', ' w', 'it', 'h ', 'hi', 'm,', ' a', 'nd', ' w', 'ha', 'ts', 'oe', 've', 'r ', 'th', 'e ', 'wo', 'lf', '\\nw', 'is', 'he', 'd,', ' t', 'ha', 't ', 'th', 'e ', 'fo', 'x ', 'wa', 's ', 'co', 'mp', 'el', 'le', 'd ', 'to', ' d', 'o,', ' f', 'or']\n",
      "\n",
      "Processing file stories/167.txt\n",
      "Data size (characters) (Document 166) 734\n",
      "sample string (Documents 166) ['it', ' h', 'ap', 'pe', 'ne', 'd ', 'th', 'at', ' t', 'he', ' c', 'at', ' m', 'et', ' t', 'he', ' f', 'ox', ' i', 'n ', 'a ', 'fo', 're', 'st', ', ', 'an', 'd ', 'as', ' s', 'he', '\\nt', 'ho', 'ug', 'ht', ' t', 'o ', 'he', 'rs', 'el', 'f,', ' h', 'e ', 'is', ' c', 'le', 've', 'r ', 'an', 'd ', 'fu']\n",
      "\n",
      "Processing file stories/168.txt\n",
      "Data size (characters) (Document 167) 2408\n",
      "sample string (Documents 167) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' c', 'oo', 'k ', 'na', 'me', 'd ', 'gr', 'et', 'el', ', ', 'wh', 'o ', 'wo', 're', ' s', 'ho', 'es', ' w', 'it', 'h\\n', 're', 'd ', 'he', 'el', 's,', ' a', 'nd', ' w', 'he', 'n ', 'sh', 'e ', 'wa', 'lk', 'ed', ' o', 'ut', ' w', 'it', 'h ', 'th', 'em']\n",
      "\n",
      "Processing file stories/169.txt\n",
      "Data size (characters) (Document 168) 1644\n",
      "sample string (Documents 168) ['on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' t', 'he', ' l', 'it', 'tl', 'e ', 'he', 'n ', 'we', 'nt', ' w', 'it', 'h ', 'th', 'e ', 'li', 'tt', 'le', ' c', 'oc', 'k\\n', 'to', ' t', 'he', ' n', 'ut', '-h', 'il', 'l,', ' a', 'nd', ' t', 'he', 'y ', 'ag', 're', 'ed', ' t', 'og', 'et', 'he', 'r ']\n",
      "\n",
      "Processing file stories/170.txt\n",
      "Data size (characters) (Document 169) 2544\n",
      "sample string (Documents 169) ['on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' t', 'he', 're', ' w', 'as', ' a', ' m', 'an', ' w', 'ho', ' d', 'id', ' n', 'ot', 'hi', 'ng', ' b', 'ut', ' g', 'am', 'bl', 'e,', ' a', 'nd', '\\nf', 'or', ' t', 'ha', 't ', 're', 'as', 'on', ' p', 'eo', 'pl', 'e ', 'ne', 've', 'r ', 'ca', 'll', 'ed']\n",
      "\n",
      "Processing file stories/171.txt\n",
      "Data size (characters) (Document 170) 575\n",
      "sample string (Documents 170) ['th', 'e ', 'fo', 'x ', 'on', 'ce', ' c', 'am', 'e ', 'to', ' a', ' m', 'ea', 'do', 'w ', 'in', ' w', 'hi', 'ch', ' s', 'at', ' a', ' f', 'lo', 'ck', ' o', 'f ', 'fi', 'ne', ' f', 'at', '\\ng', 'ee', 'se', ', ', 'on', ' w', 'hi', 'ch', ' h', 'e ', 'sm', 'il', 'ed', ' a', 'nd', ' s', 'ai', 'd,', ' i']\n",
      "\n",
      "Processing file stories/172.txt\n",
      "Data size (characters) (Document 171) 4341\n",
      "sample string (Documents 171) ['in', ' o', 'ld', 'en', ' t', 'im', 'es', ', ', 'wh', 'en', ' t', 'he', ' l', 'or', 'd ', 'hi', 'ms', 'el', 'f ', 'st', 'il', 'l ', 'us', 'ed', ' t', 'o ', 'wa', 'lk', ' a', 'bo', 'ut', ' o', 'n\\n', 'th', 'is', ' e', 'ar', 'th', ' a', 'mo', 'ng', 'st', ' m', 'en', ', ', 'it', ' o', 'nc', 'e ', 'ha']\n",
      "\n",
      "Processing file stories/173.txt\n",
      "Data size (characters) (Document 172) 3480\n",
      "sample string (Documents 172) ['on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' l', 'iv', 'ed', ' a', ' p', 'ea', 'sa', 'nt', ' a', 'nd', ' h', 'is', ' w', 'if', 'e,', ' a', 'nd', ' t', 'he', ' p', 'ar', 'so', 'n\\n', 'of', ' t', 'he', ' v', 'il', 'la', 'ge', ' h', 'ad', ' a', ' f', 'an', 'cy', ' f', 'or', ' t', 'he', ' w', 'if']\n",
      "\n",
      "Processing file stories/174.txt\n",
      "Data size (characters) (Document 173) 2003\n",
      "sample string (Documents 173) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' p', 'oo', 'r ', 'pe', 'as', 'an', 't ', 'ca', 'll', 'ed', ' c', 'ra', 'bb', ', ', 'wh', 'o ', 'dr', 'ov', 'e\\n', 'wi', 'th', ' t', 'wo', ' o', 'xe', 'n ', 'a ', 'lo', 'ad', ' o', 'f ', 'wo', 'od', ' t', 'o ', 'th']\n",
      "\n",
      "Processing file stories/175.txt\n",
      "Data size (characters) (Document 174) 4494\n",
      "sample string (Documents 174) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' r', 'ic', 'h ', 'ma', 'n,', ' w', 'ho', ' h', 'ad', ' a', ' s', 'er', 'va', 'nt', ' w', 'ho', ' s', 'er', 've', 'd ', 'hi', 'm\\n', 'di', 'li', 'ge', 'nt', 'ly', ' a', 'nd', ' h', 'on', 'es', 'tl', 'y.', '  ', 'he', ' w', 'as', ' e', 've', 'ry', ' m']\n",
      "\n",
      "Processing file stories/176.txt\n",
      "Data size (characters) (Document 175) 932\n",
      "sample string (Documents 175) ['a ', 'co', 'un', 'tr', 'ym', 'an', ' w', 'as', ' o', 'nc', 'e ', 'go', 'in', 'g ', 'ou', 't ', 'to', ' p', 'lo', 'ug', 'h ', 'wi', 'th', ' a', ' p', 'ai', 'r ', 'of', ' o', 'xe', 'n.', '\\nw', 'he', 'n ', 'he', ' g', 'ot', ' t', 'o ', 'th', 'e ', 'fi', 'el', 'd,', ' b', 'ot', 'h ', 'th', 'e ', 'an']\n",
      "\n",
      "Processing file stories/177.txt\n",
      "Data size (characters) (Document 176) 2833\n",
      "sample string (Documents 176) ['th', 're', 'e ', 'ar', 'my', ' s', 'ur', 'ge', 'on', 's ', 'wh', 'o ', 'th', 'ou', 'gh', 't ', 'th', 'ey', ' k', 'ne', 'w ', 'th', 'ei', 'r ', 'ar', 't ', 'pe', 'rf', 'ec', 'tl', 'y\\n', 'we', 're', ' t', 'ra', 've', 'li', 'ng', ' a', 'bo', 'ut', ' t', 'he', ' w', 'or', 'ld', ', ', 'an', 'd ', 'th']\n",
      "\n",
      "Processing file stories/178.txt\n",
      "Data size (characters) (Document 177) 2821\n",
      "sample string (Documents 177) ['se', 've', 'n ', 'sw', 'ab', 'ia', 'ns', ' w', 'er', 'e ', 'on', 'ce', ' t', 'og', 'et', 'he', 'r.', '  ', 'th', 'e ', 'fi', 'rs', 't ', 'wa', 's ', 'ma', 'st', 'er', '\\ns', 'ch', 'ul', 'z,', ' t', 'he', ' s', 'ec', 'on', 'd,', ' j', 'ac', 'kl', 'i,', ' t', 'he', ' t', 'hi', 'rd', ', ', 'ma', 'rl']\n",
      "\n",
      "Processing file stories/179.txt\n",
      "Data size (characters) (Document 178) 3010\n",
      "sample string (Documents 178) ['th', 'er', 'e ', 'we', 're', ' o', 'nc', 'e ', 'th', 're', 'e ', 'ap', 'pr', 'en', 'ti', 'ce', 's,', ' w', 'ho', ' h', 'ad', ' a', 'gr', 'ee', 'd ', 'to', ' k', 'ee', 'p ', 'al', 'wa', 'ys', '\\nt', 'og', 'et', 'he', 'r ', 'wh', 'il', 'e ', 'tr', 'av', 'el', 'in', 'g,', ' a', 'nd', ' a', 'lw', 'ay']\n",
      "\n",
      "Processing file stories/180.txt\n",
      "Data size (characters) (Document 179) 1904\n",
      "sample string (Documents 179) ['in', ' a', ' c', 'er', 'ta', 'in', ' v', 'il', 'la', 'ge', ' t', 'he', 're', ' o', 'nc', 'e ', 'li', 've', 'd ', 'a ', 'ma', 'n ', 'an', 'd ', 'hi', 's ', 'wi', 'fe', ', ', 'an', 'd\\n', 'th', 'e ', 'wi', 'fe', ' w', 'as', ' s', 'o ', 'la', 'zy', ' t', 'ha', 't ', 'sh', 'e ', 'wo', 'ul', 'd ', 'ne']\n",
      "\n",
      "Processing file stories/181.txt\n",
      "Data size (characters) (Document 180) 1185\n",
      "sample string (Documents 180) ['a ', 'pe', 'as', 'an', 't ', 'ha', 'd ', 'a ', 'fa', 'it', 'hf', 'ul', ' h', 'or', 'se', ' w', 'hi', 'ch', ' h', 'ad', ' g', 'ro', 'wn', ' o', 'ld', ' a', 'nd', ' c', 'ou', 'ld', '\\nd', 'o ', 'no', ' m', 'or', 'e ', 'wo', 'rk', ', ', 'so', ' h', 'is', ' m', 'as', 'te', 'r ', 'wo', 'ul', 'd ', 'no']\n",
      "\n",
      "Processing file stories/182.txt\n",
      "Data size (characters) (Document 181) 955\n",
      "sample string (Documents 181) ['th', 'e ', 'lo', 'rd', ' g', 'od', ' h', 'ad', ' c', 're', 'at', 'ed', ' a', 'll', ' a', 'ni', 'ma', 'ls', ', ', 'an', 'd ', 'ha', 'd ', 'ch', 'os', 'en', ' o', 'ut', ' t', 'he', ' w', 'ol', 'f ', 'to', '\\nb', 'e ', 'hi', 's ', 'do', 'g,', ' b', 'ut', ' h', 'e ', 'ha', 'd ', 'fo', 'rg', 'ot', 'te']\n",
      "\n",
      "Processing file stories/183.txt\n",
      "Data size (characters) (Document 182) 723\n",
      "sample string (Documents 182) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' s', 'or', 'ce', 're', 'r ', 'wh', 'o ', 'wa', 's ', 'st', 'an', 'di', 'ng', ' i', 'n ', 'th', 'e ', 'mi', 'ds', 't ', 'of', ' a', '\\ng', 're', 'at', ' c', 'ro', 'wd', ' o', 'f ', 'pe', 'op', 'le', ' p', 'er', 'fo', 'rm', 'in', 'g ', 'hi', 's ', 'wo']\n",
      "\n",
      "Processing file stories/184.txt\n",
      "Data size (characters) (Document 183) 411\n",
      "sample string (Documents 183) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', 'n ', 'ol', 'd ', 'wo', 'ma', 'n,', ' b', 'ut', ' y', 'ou', ' h', 'av', 'e ', 'su', 're', 'ly', ' s', 'ee', 'n ', 'an', ' o', 'ld', '\\nw', 'om', 'an', ' g', 'o ', 'a-', 'be', 'gg', 'in', 'g ', 'be', 'fo', 're', ' n', 'ow', '. ', ' t', 'hi', 's ', 'wo']\n",
      "\n",
      "Processing file stories/185.txt\n",
      "Data size (characters) (Document 184) 509\n",
      "sample string (Documents 184) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' o', 'n ', 'a ', 'ti', 'me', ' a', ' m', 'ai', 'de', 'n ', 'wh', 'o ', 'wa', 's ', 'pr', 'et', 'ty', ', ', 'bu', 't ', 'id', 'le', '\\na', 'nd', ' n', 'eg', 'li', 'ge', 'nt', '. ', ' w', 'he', 'n ', 'sh', 'e ', 'ha', 'd ', 'to', ' s', 'pi', 'n ', 'sh', 'e ']\n",
      "\n",
      "Processing file stories/186.txt\n",
      "Data size (characters) (Document 185) 877\n",
      "sample string (Documents 185) ['in', ' t', 'he', ' t', 'im', 'e ', 'of', ' s', 'ch', 'la', 'ur', 'af', 'fe', 'n ', 'i ', 'we', 'nt', ' f', 'or', 'th', ' a', 'nd', ' s', 'aw', ' r', 'om', 'e ', 'an', 'd ', 'th', 'e\\n', 'la', 'te', 'ra', 'n ', 'ha', 'ng', 'in', 'g ', 'by', ' a', ' s', 'ma', 'll', ' s', 'il', 'ke', 'n ', 'th', 're']\n",
      "\n",
      "Processing file stories/187.txt\n",
      "Data size (characters) (Document 186) 534\n",
      "sample string (Documents 186) ['i ', 'wi', 'll', ' t', 'el', 'l ', 'yo', 'u ', 'so', 'me', 'th', 'in', 'g.', '  ', 'i ', 'sa', 'w ', 'tw', 'o ', 'ro', 'as', 'te', 'd ', 'fo', 'wl', 's ', 'fl', 'yi', 'ng', ',\\n', 'th', 'ey', ' f', 'le', 'w ', 'qu', 'ic', 'kl', 'y ', 'an', 'd ', 'ha', 'd ', 'th', 'ei', 'r ', 'br', 'ea', 'st', 's ']\n",
      "\n",
      "Processing file stories/188.txt\n",
      "Data size (characters) (Document 187) 327\n",
      "sample string (Documents 187) ['th', 're', 'e ', 'wo', 'me', 'n ', 'we', 're', ' t', 'ra', 'ns', 'fo', 'rm', 'ed', ' i', 'nt', 'o ', 'fl', 'ow', 'er', 's ', 'wh', 'ic', 'h ', 'gr', 'ew', ' i', 'n\\n', 'th', 'e ', 'fi', 'el', 'd,', ' b', 'ut', ' o', 'ne', ' o', 'f ', 'th', 'em', ' w', 'as', ' a', 'll', 'ow', 'ed', ' t', 'o ', 'be']\n",
      "\n",
      "Processing file stories/189.txt\n",
      "Data size (characters) (Document 188) 603\n",
      "sample string (Documents 188) ['ho', 'w ', 'fo', 'rt', 'un', 'at', 'e ', 'is', ' t', 'he', ' m', 'as', 'te', 'r,', ' a', 'nd', ' h', 'ow', ' w', 'el', 'l ', 'al', 'l ', 'go', 'es', ' i', 'n ', 'hi', 's\\n', 'ho', 'us', 'e,', ' w', 'he', 'n ', 'he', ' h', 'as', ' a', ' w', 'is', 'e ', 'se', 'rv', 'an', 't ', 'wh', 'o ', 'li', 'st']\n",
      "\n",
      "Processing file stories/190.txt\n",
      "Data size (characters) (Document 189) 655\n",
      "sample string (Documents 189) ['on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' p', 'oo', 'r ', 'pi', 'ou', 's ', 'pe', 'as', 'an', 't ', 'di', 'ed', ', ', 'an', 'd ', 'ar', 'ri', 've', 'd ', 'be', 'fo', 're', '\\nt', 'he', ' g', 'at', 'e ', 'of', ' h', 'ea', 've', 'n.', '  ', 'at', ' t', 'he', ' s', 'am', 'e ', 'ti', 'me']\n",
      "\n",
      "Processing file stories/191.txt\n",
      "Data size (characters) (Document 190) 1131\n",
      "sample string (Documents 190) ['le', 'an', ' l', 'is', 'a ', 'wa', 's ', 'of', ' a', ' v', 'er', 'y ', 'di', 'ff', 'er', 'en', 't ', 'wa', 'y ', 'of', ' t', 'hi', 'nk', 'in', 'g ', 'fr', 'om', ' l', 'az', 'y\\n', 'ha', 'rr', 'y ', 'an', 'd ', 'fa', 't ', 'tr', 'in', 'a,', ' w', 'ho', ' n', 'ev', 'er', ' l', 'et', ' a', 'ny', 'th']\n",
      "\n",
      "Processing file stories/192.txt\n",
      "Data size (characters) (Document 191) 983\n",
      "sample string (Documents 191) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' t', 'ai', 'lo', 'r,', ' w', 'ho', ' w', 'as', ' a', ' q', 'ua', 'rr', 'el', 'so', 'me', ' f', 'el', 'lo', 'w,', ' a', 'nd', '\\nh', 'is', ' w', 'if', 'e,', ' w', 'ho', ' w', 'as', ' g', 'oo', 'd,', ' i', 'nd', 'us', 'tr', 'io', 'us', ', ', 'an', 'd ']\n",
      "\n",
      "Processing file stories/193.txt\n",
      "Data size (characters) (Document 192) 3027\n",
      "sample string (Documents 192) ['in', ' o', 'ld', 'en', ' t', 'im', 'es', ' e', 've', 'ry', ' s', 'ou', 'nd', ' s', 'ti', 'll', ' h', 'ad', ' i', 'ts', ' m', 'ea', 'ni', 'ng', ' a', 'nd', ' s', 'ig', 'ni', 'fi', 'ca', 'nc', 'e.', '\\nw', 'he', 'n ', 'th', 'e ', 'sm', 'it', \"h'\", 's ', 'ha', 'mm', 'er', ' r', 'es', 'ou', 'nd', 'ed']\n",
      "\n",
      "Processing file stories/194.txt\n",
      "Data size (characters) (Document 193) 648\n",
      "sample string (Documents 193) ['th', 'e ', 'fi', 'sh', 'es', ' h', 'ad', ' f', 'or', ' a', ' l', 'on', 'g ', 'ti', 'me', ' b', 'ee', 'n ', 'di', 'sc', 'on', 'te', 'nt', 'ed', ' b', 'ec', 'au', 'se', ' n', 'o\\n', 'or', 'de', 'r ', 'pr', 'ev', 'ai', 'le', 'd ', 'in', ' t', 'he', 'ir', ' k', 'in', 'gd', 'om', '. ', ' n', 'on', 'e ']\n",
      "\n",
      "Processing file stories/195.txt\n",
      "Data size (characters) (Document 194) 620\n",
      "sample string (Documents 194) ['wh', 'er', 'e ', 'do', ' y', 'ou', ' l', 'ik', 'e ', 'be', 'st', ' t', 'o ', 'fe', 'ed', ' y', 'ou', 'r ', 'fl', 'oc', 'ks', ', ', 'sa', 'id', ' a', ' m', 'an', '\\nt', 'o ', 'an', ' o', 'ld', ' c', 'ow', 'he', 'rd', '. ', ' h', 'er', 'e,', ' s', 'ir', ', ', 'wh', 'er', 'e ', 'th', 'e ', 'gr', 'as']\n",
      "\n",
      "Processing file stories/196.txt\n",
      "Data size (characters) (Document 195) 2246\n",
      "sample string (Documents 195) ['tw', 'o ', 'or', ' t', 'hr', 'ee', ' h', 'un', 'dr', 'ed', ' y', 'ea', 'rs', ' a', 'go', ', ', 'wh', 'en', ' p', 'eo', 'pl', 'e ', 'we', 're', ' f', 'ar', ' f', 'ro', 'm ', 'be', 'in', 'g ', 'so', '\\nc', 'ra', 'ft', 'y ', 'an', 'd ', 'cu', 'nn', 'in', 'g ', 'as', ' t', 'he', 'y ', 'ar', 'e ', 'no']\n",
      "\n",
      "Processing file stories/197.txt\n",
      "Data size (characters) (Document 196) 2081\n",
      "sample string (Documents 196) ['in', ' d', 'ay', 's ', 'go', 'ne', ' b', 'y ', 'th', 'er', 'e ', 'wa', 's ', 'a ', 'la', 'nd', ' w', 'he', 're', ' t', 'he', ' n', 'ig', 'ht', 's ', 'we', 're', ' a', 'lw', 'ay', 's ', 'da', 'rk', ',\\n', 'an', 'd ', 'th', 'e ', 'sk', 'y ', 'sp', 're', 'ad', ' o', 've', 'r ', 'it', ' l', 'ik', 'e ']\n",
      "\n",
      "Processing file stories/198.txt\n",
      "Data size (characters) (Document 197) 1488\n",
      "sample string (Documents 197) ['wh', 'en', ' g', 'od', ' c', 're', 'at', 'ed', ' t', 'he', ' w', 'or', 'ld', ' a', 'nd', ' w', 'as', ' a', 'bo', 'ut', ' t', 'o ', 'fi', 'x ', 'th', 'e ', 'le', 'ng', 'th', ' o', 'f\\n', 'ea', 'ch', ' c', 're', 'at', 'ur', \"e'\", 's ', 'li', 'fe', ', ', 'th', 'e ', 'as', 's ', 'ca', 'me', ' a', 'nd']\n",
      "\n",
      "Processing file stories/199.txt\n",
      "Data size (characters) (Document 198) 1529\n",
      "sample string (Documents 198) ['in', ' a', 'nc', 'ie', 'nt', ' t', 'im', 'es', ' a', ' g', 'ia', 'nt', ' w', 'as', ' o', 'nc', 'e ', 'tr', 'av', 'el', 'in', 'g ', 'on', ' a', ' g', 're', 'at', ' h', 'ig', 'hw', 'ay', ',\\n', 'wh', 'en', ' s', 'ud', 'de', 'nl', 'y ', 'an', ' u', 'nk', 'no', 'wn', ' m', 'an', ' s', 'pr', 'an', 'g ']\n",
      "\n",
      "Processing file stories/200.txt\n",
      "Data size (characters) (Document 199) 3745\n",
      "sample string (Documents 199) ['ma', 'st', 'er', ' p', 'fr', 'ie', 'm ', 'wa', 's ', 'a ', 'sh', 'or', 't,', ' t', 'hi', 'n,', ' b', 'ut', ' l', 'iv', 'el', 'y ', 'ma', 'n,', ' w', 'ho', ' n', 'ev', 'er', '\\nr', 'es', 'te', 'd ', 'a ', 'mo', 'me', 'nt', '. ', ' h', 'is', ' f', 'ac', 'e,', ' o', 'f ', 'wh', 'ic', 'h ', 'hi', 's ']\n",
      "\n",
      "Processing file stories/201.txt\n",
      "Data size (characters) (Document 200) 2599\n",
      "sample string (Documents 200) ['a ', 'ta', 'il', 'or', ' a', 'nd', ' a', ' g', 'ol', 'ds', 'mi', 'th', ' w', 'er', 'e ', 'tr', 'av', 'el', 'in', 'g ', 'to', 'ge', 'th', 'er', ', ', 'an', 'd ', 'on', 'e ', 'ev', 'en', 'in', 'g\\n', 'wh', 'en', ' t', 'he', ' s', 'un', ' h', 'ad', ' s', 'un', 'k ', 'be', 'hi', 'nd', ' t', 'he', ' m']\n",
      "\n",
      "Processing file stories/202.txt\n",
      "Data size (characters) (Document 201) 1783\n",
      "sample string (Documents 201) ['a ', 'ce', 'rt', 'ai', 'n ', 'ta', 'il', 'or', ' w', 'ho', ' w', 'as', ' g', 're', 'at', ' a', 't ', 'bo', 'as', 'ti', 'ng', ' b', 'ut', ' i', 'll', ' a', 't ', 'do', 'in', 'g,', '\\nt', 'oo', 'k ', 'it', ' i', 'nt', 'o ', 'hi', 's ', 'he', 'ad', ' t', 'o ', 'go', ' a', 'br', 'oa', 'd ', 'fo', 'r ']\n",
      "\n",
      "Processing file stories/203.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size (characters) (Document 202) 681\n",
      "sample string (Documents 202) ['a ', 'me', 'rc', 'ha', 'nt', ' h', 'ad', ' d', 'on', 'e ', 'go', 'od', ' b', 'us', 'in', 'es', 's ', 'at', ' t', 'he', ' f', 'ai', 'r.', '  ', 'he', ' h', 'ad', ' s', 'ol', 'd ', 'hi', 's\\n', 'wa', 're', 's,', ' a', 'nd', ' l', 'in', 'ed', ' h', 'is', ' m', 'on', 'ey', '-b', 'ag', 's ', 'wi', 'th']\n",
      "\n",
      "Processing file stories/204.txt\n",
      "Data size (characters) (Document 203) 3501\n",
      "sample string (Documents 203) ['th', 'is', ' s', 'to', 'ry', ', ', 'my', ' d', 'ea', 'r ', 'yo', 'un', 'g ', 'fo', 'lk', 's,', ' s', 'ee', 'ms', ' t', 'o ', 'be', ' f', 'al', 'se', ', ', 'bu', 't ', 'it', ' r', 'ea', 'll', 'y\\n', 'is', ' t', 'ru', 'e,', ' f', 'or', ' m', 'y ', 'gr', 'an', 'df', 'at', 'he', 'r,', ' f', 'ro', 'm ']\n",
      "\n",
      "Processing file stories/205.txt\n",
      "Data size (characters) (Document 204) 978\n",
      "sample string (Documents 204) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' f', 'ar', '-s', 'ig', 'ht', 'ed', ', ', 'cr', 'af', 'ty', ' p', 'ea', 'sa', 'nt', ' w', 'ho', 'se', '\\nt', 'ri', 'ck', 's ', 'we', 're', ' m', 'uc', 'h ', 'ta', 'lk', 'ed', ' a', 'bo', 'ut', '. ', ' t', 'he', ' b']\n",
      "\n",
      "Processing file stories/206.txt\n",
      "Data size (characters) (Document 205) 457\n",
      "sample string (Documents 205) ['ge', 'or', 'ge', ' o', 'ne', ' d', 'ay', ' s', 'ai', 'd ', 'to', ' h', 'is', ' l', 'it', 'tl', 'e ', 'ch', 'ic', 'ke', 'ns', ', ', 'co', 'me', ' i', 'nt', 'o ', 'th', 'e\\n', 'pa', 'rl', 'or', ' a', 'nd', ' e', 'nj', 'oy', ' y', 'ou', 'rs', 'el', 've', 's,', ' a', 'nd', ' p', 'ic', 'k ', 'up', ' t']\n",
      "\n",
      "Processing file stories/207.txt\n",
      "Data size (characters) (Document 206) 3822\n",
      "sample string (Documents 206) ['a ', 'ri', 'ch', ' f', 'ar', 'me', 'r ', 'wa', 's ', 'on', 'e ', 'da', 'y ', 'st', 'an', 'di', 'ng', ' i', 'n ', 'hi', 's ', 'ya', 'rd', ' i', 'ns', 'pe', 'ct', 'in', 'g ', 'hi', 's\\n', 'fi', 'el', 'ds', ' a', 'nd', ' g', 'ar', 'de', 'ns', '. ', ' t', 'he', ' c', 'or', 'n ', 'wa', 's ', 'gr', 'ow']\n",
      "\n",
      "Processing file stories/208.txt\n",
      "Data size (characters) (Document 207) 3783\n",
      "sample string (Documents 207) ['a ', 'so', 'ld', 'ie', 'r ', 'wh', 'o ', 'is', ' a', 'fr', 'ai', 'd ', 'of', ' n', 'ot', 'hi', 'ng', ', ', 'tr', 'ou', 'bl', 'es', ' h', 'im', 'se', 'lf', ' a', 'bo', 'ut', '\\nn', 'ot', 'hi', 'ng', '. ', ' o', 'ne', ' o', 'f ', 'th', 'is', ' k', 'in', 'd ', 'ha', 'd ', 're', 'ce', 'iv', 'ed', ' h']\n",
      "\n",
      "Processing file stories/209.txt\n",
      "Data size (characters) (Document 208) 461\n",
      "sample string (Documents 208) ['\\ni', 'n ', 'th', 'e ', 'wi', 'nt', 'er', ' t', 'im', 'e,', ' w', 'he', 'n ', 'de', 'ep', ' s', 'no', 'w ', 'la', 'y ', 'on', ' t', 'he', ' g', 'ro', 'un', 'd,', ' a', ' p', 'oo', 'r ', 'bo', 'y ', '\\nw', 'as', ' f', 'or', 'ce', 'd ', 'to', ' g', 'o ', 'ou', 't ', 'on', ' a', ' s', 'le', 'dg', 'e ']\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    \n",
    "    with open(filename) as f:\n",
    "        data = tf.compat.as_str(f.read())\n",
    "        #make all the words lowercaser\n",
    "        data = data.lower()\n",
    "        data = list(data)\n",
    "    return data\n",
    "\n",
    "documents = []\n",
    "global documents\n",
    "for i in range(num_files):\n",
    "    print('\\nProcessing file %s' %os.path.join(dir_name, filenames[i]))\n",
    "    chars = read_data(os.path.join(dir_name, filenames[i]))\n",
    "    \n",
    "    #break the data into bigrams\n",
    "    two_grams = [''.join(chars[ch_i:ch_i+2]) for ch_i in range(0, len(chars)-2, 2)]\n",
    "    #create a list of lists with bigrams\n",
    "    documents.append(two_grams)\n",
    "    print('Data size (characters) (Document %d) %d' %(i, len(two_grams)))\n",
    "    print('sample string (Documents %d) %s' %(i, two_grams[:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Dictionaries (Bigrams)\n",
    "Build the following. to understand each of these elements, let use also assume the test \"I like to go to school\"\n",
    "- dictionary : maps a string word to an ID . ({I:O, like:1, to:2, go:3, school:4})\n",
    "- reverse_dictionary: maps ID to words({0:I, 1:like, 2:to, 3:go, 4:school})\n",
    "- count: list of list of (word, frequency) elements (e.g [(I,1), (to,2), (go,1), (school,1)])\n",
    "- data: Contain the string of text we read, where string words are replaced with word IDs e.g [0, 1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "727505 Characters found.\n",
      "Most common words (+UNK) [('e ', 24554), ('he', 24203), (' t', 21726), ('th', 21031), ('d ', 16995)]\n",
      "Least common words (+UNK) [('md', 1), ('dt', 1), ('xu', 1), ('x-', 1), ('-.', 1), ('tp', 1), ('-j', 1), ('lg', 1), ('uj', 1), ('kd', 1), ('z.', 1), ('kt', 1), ('oj', 1), ('c-', 1), ('!\"', 1)]\n",
      "Sample data [16, 27, 88, 26, 3, 96, 72, 11, 2, 17]\n",
      "Sample data [23, 157, 25, 36, 78, 183, 42, 9, 87, 19]\n",
      "Vocabulary:  572\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(documents):\n",
    "    chars = []\n",
    "    # This is going to be a list of lists\n",
    "    # Where the outer list denote each document\n",
    "    # and the inner lists denote words in a given document\n",
    "    data_list = []\n",
    "  \n",
    "    for d in documents:\n",
    "        chars.extend(d)\n",
    "    print('%d Characters found.'%len(chars))\n",
    "    count = []\n",
    "    # Get the bigram sorted by their frequency (Highest comes first)\n",
    "    count.extend(collections.Counter(chars).most_common())\n",
    "    \n",
    "    # Create an ID for each bigram by giving the current length of the dictionary\n",
    "    # And adding that item to the dictionary\n",
    "    # Start with 'UNK' that is assigned to too rare words\n",
    "    dictionary = dict({'UNK':0})\n",
    "    for char, c in count:\n",
    "        # Only add a bigram to dictionary if its frequency is more than 10\n",
    "        if c > 10:\n",
    "            dictionary[char] = len(dictionary)    \n",
    "    \n",
    "    unk_count = 0\n",
    "    # Traverse through all the text we have\n",
    "    # to replace each string word with the ID of the word\n",
    "    for d in documents:\n",
    "        data = list()\n",
    "        for char in d:\n",
    "            # If word is in the dictionary use the word ID,\n",
    "            # else use the ID of the special token \"UNK\"\n",
    "            if char in dictionary:\n",
    "                index = dictionary[char]        \n",
    "            else:\n",
    "                index = dictionary['UNK']\n",
    "                unk_count += 1\n",
    "            data.append(index)\n",
    "            \n",
    "        data_list.append(data)\n",
    "        \n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "    return data_list, count, dictionary, reverse_dictionary\n",
    "\n",
    "global data_list, count, dictionary, reverse_dictionary,vocabulary_size\n",
    "\n",
    "# Print some statistics about data\n",
    "data_list, count, dictionary, reverse_dictionary = build_dataset(documents)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Least common words (+UNK)', count[-15:])\n",
    "print('Sample data', data_list[0][:10])\n",
    "print('Sample data', data_list[1][:10])\n",
    "print('Vocabulary: ',len(dictionary))\n",
    "vocabulary_size = len(dictionary)\n",
    "del documents  # To reduce memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Batches of Data\n",
    "The following object generates a batch of data which will be used to train the LSTM. More specifically the generator breaks a given sequence of words into batch_size segments. we also maintain a cursor of each segment. so whenever we create a bacth of data, we sample one item from each segment and update the cursor of each segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Unrolled index 0\n",
      "\tInputs:\n",
      "\te  (1), \tki (152), \t d (49), \t w (11), \tbe (69), \n",
      "\tOutput:\n",
      "\tli (98), \tng (34), \tau (215), \ter (13), \tau (215), \n",
      "\n",
      "Unrolled index 1\n",
      "\tInputs:\n",
      "\tli (98), \tng (34), \tau (215), \ter (13), \tau (215), \n",
      "\tOutput:\n",
      "\tve (43), \t\n",
      "w (167), \tgh (109), \te  (1), \tti (112), \n",
      "\n",
      "Unrolled index 2\n",
      "\tInputs:\n",
      "\tve (43), \t\n",
      "w (167), \tgh (109), \te  (1), \tti (112), \n",
      "\tOutput:\n",
      "\td  (5), \tho (61), \tte (62), \tal (80), \tfu (235), \n",
      "\n",
      "Unrolled index 3\n",
      "\tInputs:\n",
      "\td  (5), \tho (61), \tte (62), \tal (80), \tfu (235), \n",
      "\tOutput:\n",
      "\ta  (78), \tse (56), \trs (138), \tl  (57), \tl, (260), \n",
      "\n",
      "Unrolled index 4\n",
      "\tInputs:\n",
      "\ta  (78), \tse (56), \trs (138), \tl  (57), \tbe (69), \n",
      "\tOutput:\n",
      "\tki (152), \t d (49), \t w (11), \tbe (69), \tau (215), "
     ]
    }
   ],
   "source": [
    "class DataGeneratorOHE(object):\n",
    "    \n",
    "    def __init__(self, text, batch_size, num_unroll):\n",
    "        #Text where a bigram is denoted by its ID\n",
    "        self._text = text\n",
    "        #Number of bigrams in the text\n",
    "        self._text_size = len(self._text)\n",
    "        #number of data points ina batch of data\n",
    "        self._batch_size = batch_size\n",
    "        #Num unroll is the number of steps we unroll the RNN in a single training step\n",
    "        self._num_unroll = num_unroll\n",
    "        #we break the text into several segments and the batch of data is sampled by samplying a single item from a single segemnt\n",
    "        self._segments = self._text_size // self._batch_size\n",
    "        self._cursor = [offset * self._segments for offset in range(self._batch_size)]\n",
    "    \n",
    "    def next_batch(self):\n",
    "        '''Generates a single batch of data'''\n",
    "        #train inputs (one-hot-encoded)  and train outputs (one-hot-encode)\n",
    "        batch_data = np.zeros((self._batch_size, vocabulary_size), dtype=np.float32)\n",
    "        batch_labels = np.zeros((self._batch_size, vocabulary_size), dtype=np.float32)\n",
    "        \n",
    "        #fill in the batch datapoint by datapoint\n",
    "        for b in range(self._batch_size):\n",
    "            #if the cursor of a given segment exceeds the segment length\n",
    "            #we reset the cursor back to the beginning of that segment\n",
    "            if self._cursor[b]+1 >= self._text_size:\n",
    "                self._cursor[b] = b * self._segments\n",
    "                \n",
    "            #add the text at the cursor as the input\n",
    "            batch_data[b, self._text[self._cursor[b]]] = 1.0\n",
    "            #add the preceeding bigram as the label to be predicted\n",
    "            batch_labels[b, self._text[self._cursor[b]+1]] = 1.0\n",
    "            #update the cursor\n",
    "            self._cursor[b] = (self._cursor[b]+1)%self._text_size\n",
    "            \n",
    "        return batch_data, batch_labels\n",
    "    \n",
    "    def unroll_batches(self):\n",
    "        '''This produces a list of num_unroll batches as required by a single step of training the RNN'''\n",
    "        \n",
    "        unroll_data, unroll_labels = [], []\n",
    "        for ui in range(self._num_unroll):\n",
    "            data, labels= self.next_batch()\n",
    "            unroll_data.append(data)\n",
    "            unroll_labels.append(labels)\n",
    "            \n",
    "        return unroll_data, unroll_labels\n",
    "    \n",
    "    def reset_indices(self):\n",
    "        '''used to reset all the cursors if needed'''\n",
    "        self._cursor = [offset * self._segments for offset in range(self._batch_size)]\n",
    "        \n",
    "# running a tiny set to see if things are correct\n",
    "dg = DataGeneratorOHE(data_list[0][25:50], 5, 5)\n",
    "u_data, u_labels =dg.unroll_batches()\n",
    "\n",
    "# Iterate through each data batch in the unrolled set of batches\n",
    "for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):   \n",
    "    print('\\n\\nUnrolled index %d'%ui)\n",
    "    dat_ind = np.argmax(dat,axis=1)\n",
    "    lbl_ind = np.argmax(lbl,axis=1)\n",
    "    print('\\tInputs:')\n",
    "    for sing_dat in dat_ind:\n",
    "        print('\\t%s (%d)'%(reverse_dictionary[sing_dat],sing_dat),end=\", \")\n",
    "    print('\\n\\tOutput:')\n",
    "    for sing_lbl in lbl_ind:        \n",
    "        print('\\t%s (%d)'%(reverse_dictionary[sing_lbl],sing_lbl),end=\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the LSTM\n",
    "This is a standard LSTM. the LSTM has 5 main component\n",
    "- Cell state\n",
    "- Hidden state\n",
    "- Input gate\n",
    "- Forget gate\n",
    "- output gate\n",
    "\n",
    "Each gate has three sets of weights (1 set for the current input, 1 set for the previous hidden state and 1 bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining hyperparameters\n",
    "Here we define several hyperparameters. However additionally we use dropout; a technique that helps to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of neurons in the hidden state varibles\n",
    "num_nodes = 128\n",
    "\n",
    "#number of data points in a batch we proces\n",
    "batch_size = 64\n",
    "\n",
    "#number of times steps we unroll for during optimization\n",
    "num_unrollings = 50\n",
    "\n",
    "dropout = 0.0 \n",
    "\n",
    "#use this in the scv filename when saving \n",
    "filename_extension = ''\n",
    "if dropout > 0.0:\n",
    "    filename_extension = '_dropout'\n",
    "\n",
    "filename_to_save = 'lstm'+filename_extension+'.csv' # use to save perplexity values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Inputs and Outputs\n",
    "\n",
    "In the code we define two different types of inputs. \n",
    "* Training inputs (The stories we downloaded) (batch_size > 1 with unrolling)\n",
    "* Validation inputs (An unseen validation dataset) (bach_size =1, no unrolling)\n",
    "* Test input (New story we are going to generate) (batch_size=1, no unrolling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#training input data.\n",
    "train_inputs, train_labels = [],[]\n",
    "\n",
    "#defining unrolled training inputs\n",
    "for ui in range(num_unrollings):\n",
    "    train_inputs.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size], name='train_inputs_%d'%ui))\n",
    "    train_labels.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size], name='train_labels_%d'%ui))\n",
    "    \n",
    "    #validation data placeholders\n",
    "    valid_inputs = tf.placeholder(tf.float32, shape=[1, vocabulary_size], name='valid_inputs')\n",
    "    valid_labels = tf.placeholder(tf.float32, shape=[1, vocabulary_size], name='valid_labels')\n",
    "    \n",
    "    #text generation: batch 1, no unrolling\n",
    "    test_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size], name='test_input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Model Parameters\n",
    "\n",
    "Now we define model parameters. Compared to RNNs, LSTMs have a large number of parameters. Each gate (input, forget, memory and output) has three different sets of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input gate (i_t) - How much memory to write to cell state\n",
    "#connect the current input to the input gate\n",
    "ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.02))\n",
    "#connects the prvious hidden state to the input gate\n",
    "im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.02))\n",
    "#bais of the input gate\n",
    "ib = tf.Variable(tf.random_uniform([1, num_nodes], -0.02, 0.02))\n",
    "\n",
    "#Forget gate (f_t) - How much memoery is discard from the cell state\n",
    "# connect the current input to the forget gate\n",
    "fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.02))\n",
    "#connects the prvious hidden state to the forget gate\n",
    "fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.02))\n",
    "#bais of the forget gate\n",
    "fb = tf.Variable(tf.random_uniform([1, num_nodes], -0.02, 0.02))\n",
    "\n",
    "#Candidate value (c~_t) - used to compute the current cell state\n",
    "# connect the current input to the candidate\n",
    "cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.02))\n",
    "#connects the prvious hidden state to the candidate\n",
    "cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.02))\n",
    "#bais of the candidate\n",
    "cb = tf.Variable(tf.random_uniform([1, num_nodes], -0.02, 0.02))\n",
    "\n",
    "\n",
    "# Output gate (o_t) - How much memory to output from the cell state\n",
    "# Connects the current input to the output gate\n",
    "ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.02))\n",
    "# Connects the previous hidden state to the output gate\n",
    "om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.02))\n",
    "# Bias of the output gate\n",
    "ob = tf.Variable(tf.random_uniform([1, num_nodes],-0.02,0.02))\n",
    "\n",
    "#softmax Classifier weights and biases.\n",
    "w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], stddev=0.02))\n",
    "b = tf.Variable(tf.random_uniform([1, num_nodes], -0.02, 0.02))\n",
    "\n",
    "# Variables saving state across unrollings.\n",
    "# Hidden state\n",
    "saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False, name='train_hidden')\n",
    "# Cell state\n",
    "saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False, name='train_cell')\n",
    "\n",
    "# Same variables for validation phase\n",
    "saved_valid_output = tf.Variable(tf.zeros([1, num_nodes]),trainable=False, name='valid_hidden')\n",
    "saved_valid_state = tf.Variable(tf.zeros([1, num_nodes]),trainable=False, name='valid_cell')\n",
    "\n",
    "# Same variables for testing phase\n",
    "saved_test_output = tf.Variable(tf.zeros([1, num_nodes]),trainable=False, name='test_hidden')\n",
    "saved_test_state = tf.Variable(tf.zeros([1, num_nodes]),trainable=False, name='test_cell')\n",
    "\n",
    "\n",
    "# Softmax Classifier weights and biases.\n",
    "w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], stddev=0.02)) \n",
    "b = tf.Variable(tf.random_uniform([vocabulary_size],-0.02,0.02))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining LSTM Computations\n",
    "Here first we define the LSTM cell computations as a consice function. Then we use this function to define training and test-time inference logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definition of the cell computation\n",
    "def lstm_cell(i, o, state):\n",
    "    \"\"\"create an lstm cell\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i,ox) + tf.matmul(o, om) + ob)\n",
    "    \n",
    "    return output_gate * tf.tanh(state), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Softmax_16:0\", shape=(1, 572), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "#Training related inference logic\n",
    "\n",
    "# Keeps the calculated state outputs in all the unrollings\n",
    "# Used to calculate loss\n",
    "outputs = list()\n",
    "\n",
    "#These two python variables are iteratively updated at each step of unrolling\n",
    "output = saved_output\n",
    "state = saved_state\n",
    "\n",
    "#compute the hidden state (output) and cell state (state)\n",
    "# recursively for all the teps in unrolling\n",
    "for i in train_inputs: \n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    output = tf.nn.dropout(output, keep_prob=1.0-dropout)\n",
    "    #append eachcomputed output vale\n",
    "    outputs.append(output)\n",
    "    \n",
    "#calculate the score values\n",
    "logits = tf.matmul(tf.concat(axis=0, values=outputs), w) + b\n",
    "\n",
    "# Compute predictions.\n",
    "train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Compute training perplexity\n",
    "train_perplexity_without_exp = tf.reduce_sum(tf.concat(train_labels,0)*-tf.log(tf.concat(train_prediction,0)+1e-10))/(num_unrollings*batch_size)\n",
    "\n",
    "# =====================================================================\n",
    "# validation phase related inference logic\n",
    "\n",
    "#compute the LSTM cell output for validation data\n",
    "valid_output, valid_state = lstm_cell(valid_inputs, saved_valid_output, saved_valid_state)\n",
    "\n",
    "#compute the logits\n",
    "valid_logits = tf.nn.xw_plus_b(valid_output, w, b)\n",
    "\n",
    "#compute training perplexity \n",
    "train_perplexity_without_exp = tf.reduce_sum(tf.concat(train_labels,0)*-tf.log(tf.concat(train_prediction,0)+1e-10))/(num_unrollings*batch_size)\n",
    "\n",
    "#Make sure that the state variables are updated\n",
    "#before moving on to the next iteration of generation\n",
    "with tf.control_dependencies([saved_valid_output.assign(valid_output), \n",
    "                            saved_valid_state.assign(valid_state)]):\n",
    "    valid_prediction = tf.nn.softmax(valid_logits)\n",
    "\n",
    "\n",
    "# Compute validation perplexity\n",
    "valid_perplexity_without_exp = tf.reduce_sum(valid_labels*-tf.log(valid_prediction+1e-10))\n",
    "\n",
    "# ========================================================================\n",
    "# Testing phase related inference logic\n",
    "\n",
    "# Compute the LSTM cell output for testing data\n",
    "test_output, test_state = lstm_cell(test_input, saved_test_output, saved_test_state)\n",
    "\n",
    "#compute test logit\n",
    "test_logits = tf.nn.xw_plus_b(test_output, w, b)\n",
    "\n",
    "#Make sure that the state variables are updated\n",
    "#before moving on to the next iteration of generation\n",
    "with tf.control_dependencies([saved_test_output.assign(test_output),\n",
    "                            saved_test_state.assign(test_state)]):\n",
    "    test_prediction = tf.nn.softmax(test_logits)\n",
    "print(test_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating LSTM Loss\n",
    "we calculate the training loss of the LSTM. It's a typical cross entropy loss calculated over all the scores we obtained for training data(loss) '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add_3259:0\", shape=(3200, 572), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# before calculating the training loss,\n",
    "#save the hidden state and the cell state to their respective Tensorflow variables\n",
    "with tf.control_dependencies([saved_output.assign(output), \n",
    "                              saved_state.assign(state)]):\n",
    "    #calculate the training loss by concatenation the results form all the unrolled time steps\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits = logits, labels=tf.concat(axis=0, values=train_labels))\n",
    "    )\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Learning Rate and the Optimizer with Gradient Clipping\n",
    "Here we define the learning rate and the optimizer we're going to use. We will be using Adam optimizer. We use gradient clipping to prevent any gradient explosions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"Adam_1\"\n",
      "op: \"NoOp\"\n",
      "input: \"^Adam_1/update_Variable_16/ApplyAdam\"\n",
      "input: \"^Adam_1/update_Variable_17/ApplyAdam\"\n",
      "input: \"^Adam_1/update_Variable_18/ApplyAdam\"\n",
      "input: \"^Adam_1/update_Variable_19/ApplyAdam\"\n",
      "input: \"^Adam_1/update_Variable_20/ApplyAdam\"\n",
      "input: \"^Adam_1/update_Variable_21/ApplyAdam\"\n",
      "input: \"^Adam_1/update_Variable_22/ApplyAdam\"\n",
      "input: \"^Adam_1/update_Variable_23/ApplyAdam\"\n",
      "input: \"^Adam_1/update_Variable_24/ApplyAdam\"\n",
      "input: \"^Adam_1/update_Variable_25/ApplyAdam\"\n",
      "input: \"^Adam_1/update_Variable_26/ApplyAdam\"\n",
      "input: \"^Adam_1/update_Variable_27/ApplyAdam\"\n",
      "input: \"^Adam_1/update_Variable_30/ApplyAdam\"\n",
      "input: \"^Adam_1/update_Variable_31/ApplyAdam\"\n",
      "input: \"^Adam_1/Assign\"\n",
      "input: \"^Adam_1/Assign_1\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#learning rate decay\n",
    "gstep  = tf.Variable(0, trainable=False, name='global_step')\n",
    "\n",
    "#Running this operation will cause the value of gstep to increase, while in turn reducing the learning rate\n",
    "inc_gstep = tf.assign(gstep, gstep+1)\n",
    "\n",
    "# decays learning rate everytime the gstep increase\n",
    "tf_learning_rate = tf.train.exponential_decay(0.001, gstep,\n",
    "                                             decay_steps=1, decay_rate=0.5)\n",
    "#Adam optimizer and gradient clipping\n",
    "optimizer = tf.train.AdamOptimizer(tf_learning_rate)\n",
    "gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "#clipping gradients\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "\n",
    "optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v)\n",
    ")\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resetting Operations for Resetting Hidden States\n",
    "Sometimes the state variable needs to be reset (e.g. when starting predictions at a beginning of a new epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset train state\n",
    "reset_train_state = tf.group(tf.assign(saved_state, tf.zeros([batch_size, num_nodes])),\n",
    "                            tf.assign(saved_output, tf.zeros([batch_size, num_nodes])))\n",
    "\n",
    "#Reset valid state\n",
    "reset_valid_state = tf.group(tf.assign(saved_valid_state, tf.zeros([1, num_nodes])),\n",
    "                            tf.assign(saved_valid_output, tf.zeros([1, num_nodes])))\n",
    "# Reset test state\n",
    "reset_test_state = tf.group(tf.assign(saved_test_output.assign(tf.random_normal([1, num_nodes], stddev=0.05)),\n",
    "                                     saved_test_state.assign(tf.random_normal([1, num_nodes], stddev=0.05))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy Sampling to Break the Repetition\n",
    "Here we write some simple logic to break the repetition in text. Specifically instead of always getting the word that gave this highest prediction probability, we sample randomly where the probability of being selected given by their prediction probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(distribution):\n",
    "    '''Greedy sampling\n",
    "    We pick the three best predictions given by the LSTM and sample one of them with very high \n",
    "    probability of picking the best one'''\n",
    "    \n",
    "    best_inds = np.argsort(distribution)[-3:]\n",
    "    best_probs = distribution[best_inds] / np.sum(distribution[best_inds])\n",
    "    best_idx = np.random.choice(best_inds, p=best_probs)\n",
    "    return best_idx\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the LSTM to Generate Text\n",
    "Here we train the LSTM on the available data and generate text using the trained LSTM for several steps. From each document we extract text for steps_per_document steps to train the LSTM on. We also report the train perplexity at the end of each step. Finally we test the LSTM by asking it to generate some new text starting from a randomly picked bigram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate Decay Logic\n",
    "Here we define the logic to decrease learning rate whenever the validation perplexity does not decrease\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate decay related\n",
    "# If valid perpelxity does not decrease\n",
    "# continuously for this many epochs\n",
    "# decrease the learning rate\n",
    "decay_threshold = 5\n",
    "# Keep counting perplexity increases\n",
    "decay_count = 0\n",
    "\n",
    "min_perplexity = 1e10\n",
    "\n",
    "# Learning rate decay logic\n",
    "def decay_learning_rate(session, v_perplexity):\n",
    "    global decay_threshold, decay_count, min_perplexity  \n",
    "    # Decay learning rate\n",
    "    if v_perplexity < min_perplexity:\n",
    "        decay_count = 0\n",
    "        min_perplexity= v_perplexity\n",
    "    else: \n",
    "        decay_count += 1\n",
    "    \n",
    "    if decay_count >= decay_threshold:\n",
    "        print('\\t Reducing learning rate')\n",
    "        decay_count = 0\n",
    "        \n",
    "        session.run(inc_gstep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/online1/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py:1714: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Global Variables \n",
      "Training (Step: 0) (36).(98).(73).(94).(61).(88).(47).(45).(21).(24).\n",
      "Average loss at step 1: 4.407770\n",
      "\tPerplexity at step 1: 82.086177\n",
      "\n",
      "Valid Perplexity: 67.70\n",
      "\n",
      "Generated Text after epoch 0 ... \n",
      "======================== New text Segment ==========================\n",
      "\t irstard ther, and the to then the poped said, and the pome, and the came, and ther, but he was he pome to be had the did, and now he had the dredersed the was he pome the was his was they the poped they his whould the took the was his his was the was he pome he the pome, and there the poped him the pope, which the pope, and the pord, and that then he whoulders, what his his he pome the was then he had to the pome that the to to then his his had he took to be to be to the to to be the was the do the to as the pord, and not the to the was the dide he was his to the hen he was his the don, and the pome then the pome his to the pord of he his was he the to as the\n",
      "dere, and said, and the pope.  and did, but the poped the pome he the to the was his had the was he hen in to then, what he poped the poped the hereing the card, and\n",
      "him the pome, and said him what his in he pome, and said the was he the ders, and the pord, and not the poped the pome, and that it the pord, and said he pope, and the \n",
      "====================================================================\n",
      "\n",
      "Training (Step: 1) (29).(18).(1).(61).(2).(72).(34).(21).(5).(90).\n",
      "Average loss at step 2: 3.167796\n",
      "\tPerplexity at step 2: 23.755081\n",
      "\n",
      "Valid Perplexity: 42.11\n",
      "\n",
      "Generated Text after epoch 1 ... \n",
      "======================== New text Segment ==========================\n",
      "\t nd him, said him, and wants the ded had his little and the wasant, and and then the devil into beat have her and staked the king they to to be bece of she staing the was some for thereat then the\n",
      "devils.  they day daughtered him and and as daughter, and when the wast light it was set with reat and there, and when the was standing was the great him, and then the king to the king him, said, and when his had but the will that he devil that still said that his had and said the king himsever the wated him to him to the king then him the greek that she him ans son thered him, and they ing the great him little was stained, and the ded way of thereing the greet they saway, when he was stlet that his thereed, and wanted that the great hall and to the greet that the waster that his to the king to on, and then he said the will that said the great reme threw he was solemned, and when he devil the devils whim to they daugh, and and the said, \"then stime and said his son the great rejoicing the king t\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 2) (45).(36).(14).(44).(2).(21).(77).(30).(22).(81).\n",
      "Average loss at step 3: 2.534515\n",
      "\tPerplexity at step 3: 12.610317\n",
      "\n",
      "Valid Perplexity: 39.69\n",
      "\n",
      "Generated Text after epoch 2 ... \n",
      "======================== New text Segment ==========================\n",
      "\t , saw a beof shole, and\n",
      "rough the king tim and thereat the sto to your bade for that into the made cansed it, the killed the stork came to the canded to the god and of the mento it.  and were was to the there, and asked the king and there was forest, and tailor to his forse.  and the sailor, into the king and that was a was into the town to to be all stailor to the king as one of her, and not the king had eat the thim to him, and the killed him, and the king and thereater thered back, and said,\n",
      "he, the\n",
      "home to ough he has now his head so once that the king and stork whole were disticked the little tailor to the whild on his with the should not was a find and time.\n",
      "\n",
      "he could had back, the whole was the stown he was not ing ass that of the men the king, who the tailor had wile begarrow, and that the the stailor, and not was head back hered the\n",
      "tailor, and not the tailor, and now him and said the said to him.  he was to time the tailor hade took he had not the tailor, and the tailor the sho\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 3) (45).(13).(52).(9).(81).(88).(63).(94).(10).(11).\n",
      "Average loss at step 4: 2.279176\n",
      "\tPerplexity at step 4: 9.768623\n",
      "\n",
      "Valid Perplexity: 38.57\n",
      "\n",
      "Generated Text after epoch 3 ... \n",
      "======================== New text Segment ==========================\n",
      "\t read.  they were no bettle they were of it, she hansel.  when she weved now stit, but\n",
      "they had night, she were a but in the ground, answered to children or fait, the woman, and head nother, and now that leat her walked, and\n",
      "they threw away sat doore.  then she cood to her which apple, she\n",
      "was in the old woman which awas and gretel, and they had no more perce and my were to the gretel which which it is spinning beards, therried to the room, and now the hansel, and they had to me, witch the with the food for father, and whosoever not in and foress when a she wittle duck and greated him all good ther.  they were herself of a light of the old woman, and\n",
      "tone her pear it oncers nessh in they the witch's forest forest.  she was about in she old woman, they arept nother, and hansel it hearls is then him.  now what said, in help i agan her in that do be the with, should not but of the wicked, that is alkeep and was all said, how do a great forest, there hansel, she had to\n",
      "the litter to the good \n",
      "====================================================================\n",
      "\n",
      "Training (Step: 4) (79).(42).(70).(69).(3).(32).(26).(11).(90).(54).\n",
      "Average loss at step 5: 2.109303\n",
      "\tPerplexity at step 5: 8.242492\n",
      "\n",
      "Valid Perplexity: 54.48\n",
      "\n",
      "Generated Text after epoch 4 ... \n",
      "======================== New text Segment ==========================\n",
      "\t on that misfortune we are accustomed,\n",
      "than give up our beger to misfortune we arem, and sund hersed the still of longers to the misfortune home will.  at lace was she were rom than there was as were, he sproting third and the palace, and killed that shound as if you care alred the\n",
      "casted were stirst.  in the\n",
      "mings.  that she would herself serting, she sprang safely out of\n",
      "the window, but the besiegers did not leave out to that, and killed sit so hery torthand the madelished so counly man, she sprancerve had promiced up, and\n",
      "that she was song force. that the still belet the councillers he could herse.  but the besiege did not leave off until the whole\n",
      "paracacd stood to leat the king and it made to the man, and sood the griend, and up she sicend to the of them.  the gonest\n",
      "dece out of mes such as once out, for the condof them the still, and lainster her come muse, and cathers athey couse not the ground.  when in he shall yield bearle.  and when the fire\n",
      "reached the room and the cat uncerpe\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 5) (34).(76).(55).(79).(64).(90).(14).(62).(42).(92).\n",
      "Average loss at step 6: 1.763157\n",
      "\tPerplexity at step 6: 5.830819\n",
      "\n",
      "Valid Perplexity: 51.97\n",
      "\n",
      "Generated Text after epoch 5 ... \n",
      "======================== New text Segment ==========================\n",
      "\t econd a little\n",
      "cone gold\n",
      "with it.  then he was a king's son, and away the dond still by with the sole.   and event i will give houted into the roon, and she hanseself, fore\n",
      "int them, and he said, but the married the god took the dooson, and when she raided\n",
      "how, and had he lived\n",
      "hand took could king's day more there, and the mong and like to her against to her liket of get to so, and set a little taid, and said me, and head twould be and lose this she chanted it that it the ring, who is as if the seven he comented to may of to trees, and lose and chant came to passin bacson, and the took to her with a birds, and i will ented to a beate\n",
      "on a tree, and so the was away into the room.\n",
      "\n",
      "now the ring, but the given shind that and\n",
      "that so litking the ther do, which as stood bead against it.  and looked for a chea, which is so goroad, whic soon as she will form with it.\" but an she thankly sack, and said, \"o sill said,\n",
      "\"you have delivered the charch mant see the brant, and wisteded, liked them fo\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 6) (70).(0).(64).(88).(14).(76).(74).(6).(47).(82).\n",
      "Average loss at step 7: 1.637778\n",
      "\tPerplexity at step 7: 5.143727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valid Perplexity: 44.27\n",
      "\n",
      "Generated Text after epoch 6 ... \n",
      "======================== New text Segment ==========================\n",
      "\t t of the fat nothingen herwas and cried, it in the miden from white way prand indow and lighted that the heard that she toged to her that the king's spikes, and she was\n",
      "at last her came, and the king ast once had bedpick that he had goitting the heard kingdom from much in thek down in the king's peachack them dow, and\n",
      "heard the for her lorst to great for\n",
      "him, they that he shall\n",
      "defive his was as, and then the hedgehog's skin the redgehog's daughter would conformed.  the bate and was to be there by which oner to the royal pangeraced to him his so may only to betting there was a lastone, and the hedgehog hans the hedgehog, however, by that in the father was\n",
      "ate and to by the fidger the hedgesiant told she came to the royal palace.  but the king camed him with the king and celle it and life a mangewas to be to\n",
      "him, the isling a cone by\n",
      "himself on the kingdom from the age.  then she reventme coulded his sleep.  the king asked his wards, hant see shorls, but thesself a butside her canted to h\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 7) (0).(88).(23).(60).(34).(66).(84).(74).(48).(7).\n",
      "Average loss at step 8: 1.693258\n",
      "\tPerplexity at step 8: 5.437168\n",
      "\n",
      "Valid Perplexity: 48.38\n",
      "\n",
      "Generated Text after epoch 7 ... \n",
      "======================== New text Segment ==========================\n",
      "\t  shall witch oppery had gone this head and that had been hand, shat i can'\n",
      "he of got in, and so disceldes only a leat to feath.  then the king was the king that he had met walked the king said, he time to see hersome for the watch beautiful come. i cell not to the air, and said, i with the joyelt man, and be door whiched but, he went into the king where it in was the batten that the bar iffeather of her, and said, that had been was to the borned.\n",
      "and she deverything said, you could not me.ught her daughter was\n",
      "bed.\n",
      "the queen beard then to to her,\n",
      "and they that the king who haveled it to\n",
      " such as had and shen shall beautiful bovery shall, she saw it must be\n",
      "quick, and said, i have been in\n",
      "then the king could not come.  but the quieut was wict this the king who havel it into a strece.  the nowf came and sawn the hundled out the hunging, and she answented.  when the king was the king into the hunden the king been was\n",
      "gladinto the fire quickly with the bird,\n",
      "and said, you spromis mould not g\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 8) (11).(95).(84).(38).(66).(60).(93).(15).(72).(47).\n",
      "Average loss at step 9: 1.684774\n",
      "\tPerplexity at step 9: 5.391234\n",
      "\n",
      "Valid Perplexity: 56.55\n",
      "\n",
      "\t Reducing learning rate\n",
      "Generated Text after epoch 8 ... \n",
      "======================== New text Segment ==========================\n",
      "\t  was suffer a swelf.  when they all that is at\n",
      "lone came from sleep, and those who had been turned to\n",
      "stone received once more their natural forms.\n",
      "\n",
      "simpleton married the youngest and sweetest princess, and after her\n",
      "father's death became king, and his two brothers received the two\n",
      "othe call of the had been right for thrdi dound the two brothers, the bearn looked\n",
      "at a sonce complacdid him onde with the pearls, and he gother exactly, what out sitting in the meat\n",
      "sibe away toave it to the heor was wask to his bed.\n",
      "\n",
      "as the most.  the went up the second duttle was the enchanted into him the mosted to before the encesnted the birst lit, in the watch, they cried him to became to hiy the elsed on the leaves were so a coof sone of the right palked on there again, the he came to the two brothers, he came king, and his tas and the dexght day, but should be know to be two morey, and after hon lettle came turt the bein would be from the manted insidestered brothers, and they were shappy smith fell, \n",
      "====================================================================\n",
      "\n",
      "Training (Step: 9) (96).(89).(40).(87).(32).(65).(68).(43).(71).(48).\n",
      "Average loss at step 10: 1.980322\n",
      "\tPerplexity at step 10: 7.245073\n",
      "\n",
      "Valid Perplexity: 41.10\n",
      "\n",
      "Generated Text after epoch 9 ... \n",
      "======================== New text Segment ==========================\n",
      "\t  father into the little old killed he with his two one and heard of a ring count out of the water.  the quieut of the came at insimpleto as that he said, \"i woman they flew the king together about in a piece their death of dress heads on the some cann.  the\n",
      "finderlace worse.\n",
      "    s not\n",
      "with at soon at the that the two at and brought that, coulsesed that the two brothers wered ners, it whose that the kingdom has now that they with all the one want to are in the mostleg, he said, from heard the road of the wathe whom he had grown, and led her to chaved he had grown to no one who had been led to you.  when he had grown in hers and the those of the kingdom to bright and could that, so all and said the two elde of that, she said to her, and lood wife again, and said,\n",
      "ell sight what he that it in all on her into then the two peasanting the out one out on the water of the maining the kingdoms, but a reconey, and the turnle he said, have whon he who the king for you must belf and the face's daugh\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 10) (98).(85).(14).(73).(66).(91).(16).(89).(2).(76).\n",
      "Average loss at step 11: 2.146448\n",
      "\tPerplexity at step 11: 8.554419\n",
      "\n",
      "Valid Perplexity: 35.06\n",
      "\n",
      "Generated Text after epoch 10 ... \n",
      "======================== New text Segment ==========================\n",
      "\t r before and was to seekin, and then sent ward, and all at live that the willor coved ther walld, lew at has angest, will sideed, and at last the childrke, and saw that in you have pleased her the forner the unnine bother the will bent, and they sat all the said, i will chambed it a first to be a huntsman, and said,\n",
      "UNKuse that,\n",
      "but the three rave\n",
      "should stree to to leavor, and then at the chanh had he benear a chardo, and hast beneached the are and had beg them at armantle\n",
      "the restied, and wher and was all that,\n",
      "but he had three\n",
      "searth a coares, and thread it to no purposed from her in the morning the fox on the son and cried, \"the\n",
      "dost by home with all and cries wered annon sat that in you alartly howevered in his anger was to being\n",
      "intoe and remained preceiven again.  that is sent and saw that, and they remained to everything at the beards with to betten, and said,\n",
      " for you, and then rushen was a maid,\n",
      "UNKust they, said he, and struck it, but he said, at one ar forth their pardon.  and \n",
      "====================================================================\n",
      "\n",
      "Training (Step: 11) (22).(48).(57).(7).(65).(68).(14).(27).(6).(82).\n",
      "Average loss at step 12: 1.982770\n",
      "\tPerplexity at step 12: 7.262831\n",
      "\n",
      "Valid Perplexity: 28.31\n",
      "\n",
      "Generated Text after epoch 11 ... \n",
      "======================== New text Segment ==========================\n",
      "\t o with hers nought.  they tore is he whole was lay, and then he was to\n",
      "himself the way, has not not only to the first had bring only no becomed him to a head that the wedding was.  then she sat no me.  but he had gone men\n",
      "and came in his\n",
      "hedgehold in her mares of his\n",
      "ban had brinder, should not have.  that thushed the king had come and happened of her to go with\n",
      "her, and reathe hannow, that he was welonly, that the forest and come thought her some form of the cock and wither, what is whe courtyard on that he wanted it into the cantle, that he would likewise thought with her arm\n",
      "as he was again.  then she was to stably together,\n",
      "why are\n",
      "and onged they would ran as hans, i had soon recame and\n",
      "never had been the wholeften her father, i have not\n",
      "become a hunger.  when the redgehog as if on her resened one the way, how was\n",
      "celebrather was havereat how how his pocking, and the\n",
      "fishtions would been shors, and that he theye, she sswed, and he would.  shave you cones, but on the hedge with his pi\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 12) (58).(35).(96).(79).(22).(52).(56).(3).(18).(66).\n",
      "Average loss at step 13: 1.715380\n",
      "\tPerplexity at step 13: 5.558785\n",
      "\n",
      "Valid Perplexity: 32.14\n",
      "\n",
      "Generated Text after epoch 12 ... \n",
      "======================== New text Segment ==========================\n",
      "\t nto whole said, i am no lighted her husband they ring-cleaten about the hearn saw have again, and drink, but he was only a shall every said, and when\n",
      "the\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bird by the bailiff.  what was shut bride long come to the box on the ears, and put as if he would but when the king's ate wilk.  when she have her beautiful drins went into the mill-stouchen heard it out, and went out of it, he tred on the second time, he will was to the birst.\n",
      "\n",
      "at the wedding outside.  then they wed them, he was standing about him the have had grown, said if\n",
      "not, was away to them, and have him not no longer and bed, how he\n",
      "had a little, and\n",
      "ther, but reat of her fare of the mill,\n",
      "too, he\n",
      "said, no, no on her own form and ring four-footed the heares, the had one your father in some\n",
      "he, said, i have mading he at last bire arest back for hards, and began to the night, for the others, and said, that they were hon walk to him, but, i have along of whom to gare him again she was to anythen she which and her which was away in\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 13) (57).(20).(62).(13).(18).(72).(19).(26).(43).(71).\n",
      "Average loss at step 14: 1.873695\n",
      "\tPerplexity at step 14: 6.512316\n",
      "\n",
      "Valid Perplexity: 32.58\n",
      "\n",
      "Generated Text after epoch 13 ... \n",
      "======================== New text Segment ==========================\n",
      "\t , thanchaid, the striking is the water, and then she him, and he did not by a wolf to the fishertaken to them of them to her back again, when the maiden went to the other, and then she tood the fish, and she was to the forched to the king's daughted, and said to furest, who was quige.  when she\n",
      "was going to the great.  the old wome\n",
      "into his head.  and he strench again him.\n",
      "  then in the daughters of the world to the fisherman and the gread stretched out\n",
      "to a great nothing, and thought the children want with him the king was and was streaced to a stone of the little was steaped to drittened the driUNK            your nowever, and that in her son and\n",
      "that, it was so to be a little house, and with killever a conders with his grown, and the children happig out of the forest, went in the forest, and when she thought the water, he wates and befored to the three of you will not some ind the fish went to turse.  the old woman told himself, be with the mout was too, be sides of the old not somep, \n",
      "====================================================================\n",
      "\n",
      "Training (Step: 14) (73).(63).(88).(82).(52).(69).(61).(67).(50).(17).\n",
      "Average loss at step 15: 2.037200\n",
      "\tPerplexity at step 15: 7.669103\n",
      "\n",
      "Valid Perplexity: 31.25\n",
      "\n",
      "Generated Text after epoch 14 ... \n",
      "======================== New text Segment ==========================\n",
      "\t  and took him will her.\n",
      "  the other ther it herself into her, and wanted was come to the other, and did not looked.  the king left\n",
      "said\n",
      "andingelled,\n",
      "and as she had meon the ther.  she had made him with the firs, she said, you ar, my chich i her said, to her hout, and done of the golden ranced into the at the way, and when it was gone to her.\n",
      "\n",
      "at she had\n",
      "give me to come to the stone, she well be town in, and the stood on it.\n",
      "\n",
      "when she was standing that the golden rain went with him.  but her father to the doors the lood, lefter the wille, she was many your little long, but into the world a part into her golden road, and the child take herself and welled as ther, and on the second mento you.  ther if it there, and shut out as eat for you.   but she was thought on the sold a gir.  the which had then he was as shon so hard take to her, and she was told her he went brought had heap and\n",
      "took the pocket was a golden rain.  then she went to her, and that she who where the castle howeved not bega\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 15) (86).(4).(84).(36).(5).(92).(1).(88).(44).(27).\n",
      "Average loss at step 16: 2.031887\n",
      "\tPerplexity at step 16: 7.628470\n",
      "\n",
      "Valid Perplexity: 30.35\n",
      "\n",
      "Generated Text after epoch 15 ... \n",
      "======================== New text Segment ==========================\n",
      "\t frain, i will have you have the beard of the was, he who had embran that home bade will your ears in this heart was side, and that i had been the forest, and the himself to day the room and weet thich i will hall not lettly, anish, she said, you are in that is the stair, and then in her poweetd, that was no one.  will\n",
      "he like is meathat have had noney.  what he was to every.  ah, for he got over his hand, i will make in his more, i will not sett themike took a suddenly cont a beautiful at she well come to set her, and went again with their blood to hid the beautiful from the blue lit res.  and when she was in two and came to as rew the black with his his ang not.  the king and\n",
      "him yes strave, but he willighte youther, and ran for his like a head to eack to the forest, there, i will be still three for her, and cause, the more at onise ing, the wolf and said, no mine once meat the\n",
      "king's daughter to his smock, and that i have no, but they had eather of her setten, however, he said, \"i wall\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 16) (60).(58).(22).(17).(73).(13).(96).(82).(65).(99).\n",
      "Average loss at step 17: 1.694528\n",
      "\tPerplexity at step 17: 5.444077\n",
      "\n",
      "Valid Perplexity: 45.09\n",
      "\n",
      "\t Reducing learning rate\n",
      "Generated Text after epoch 16 ... \n",
      "======================== New text Segment ==========================\n",
      "\t  there was about to has may money, and the king's daugUNKomething, i have a block, there, you cause, and conradiverin his pocs.\"\n",
      "and father have good forher anoth his father hundried hanstain, answered, and swother hireadly kings there and broke, if you can be ther?\" \"intoo, out of it.\" \"that their fast withing sitter.\"\n",
      "but when it migy mouring a little minbeautiful mighty, and could not dres, the going betten.\" and the hedgeUNK\" \"sometme, the king's daughteUNK\" then herwards a treeper.  she has travelut out.\"\n",
      "\n",
      "\"good son.\" UNKomisher.\n",
      "\n",
      "UNKuite, have it in her to ange theres and sit winew ound of\n",
      "holly, and amother there is it, anot as your way a broke, and\n",
      "was a money trarfinew had\n",
      "took had promither?\" \"ino,\" \"som your deal, what is the morning, and thing hold-houtmigh, and cried out of them, and athing this, there what is the stake, and the may have your trother?\" \"who, with throw her.\" an there was anger, and make him iroll to that the board and haven, and the churde on.\n",
      "\n",
      "asted sister.\"\n",
      "\"so\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 17) (93).(55).(75).(26).(79).(83).(96).(88).(27).(69).\n",
      "Average loss at step 18: 2.041946\n",
      "\tPerplexity at step 18: 7.705587\n",
      "\n",
      "Valid Perplexity: 28.37\n",
      "\n",
      "Generated Text after epoch 17 ... \n",
      "======================== New text Segment ==========================\n",
      "\t rwards him in her and drope aw also on his bed, and that he was about, it was lying ince the said, \"i have you shut returnes, and a little made they was still and thrown in the wall, and have nothere.\n",
      "\n",
      "then she did not lost no me, and the mandle\n",
      "frest once mannot happined again, and then he saw that he should cried to the had into the sack to his hord, i said, i have good even into the pardoom, and then they said, when they had tone it, into the royal garm.\"  then he could not find himself to the house, and went into the world wasted to see in her manding in the door, they said that he say what she said, oh, what day he cooking, and heard in it.  but as he went to the sectle.  but when they\n",
      "fell in it.  but when the little ran are not fell me the morning at the golden reahed and there in stime what she was not find to becaust he had gone into the world. then this man when she and had took there was standeked at have your for your was sorrow.  and when he chaseen, and though it, she had g\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 18) (13).(62).(81).(16).(41).(68).(30).(15).(4).(92).\n",
      "Average loss at step 19: 2.199841\n",
      "\tPerplexity at step 19: 9.023582\n",
      "\n",
      "Valid Perplexity: 25.07\n",
      "\n",
      "Generated Text after epoch 18 ... \n",
      "======================== New text Segment ==========================\n",
      "\t  father and bring you have man, i will staild him once mantly, and was she satted to his two little horning sible to see her\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wanting the whole likewise and said 'if it was took her father, and only the poor with suchan the presented to a down to this.  then said the man was all the two little witch to his\n",
      "kingdom, and\n",
      "said, \"you have not the goldier to the two with\n",
      "began to him, and went again, and\n",
      "said the thousands is will see that she said, \"yould compart was as a little stoge, but she opened the good and said that they will be greather, and was again, and he led it were she threet out with her leavens, and went to\n",
      "thise beater the king's son and said, oh, god, but\n",
      "he should his place, and when she saw her to see if i stold it.  it into that, and while they said, and the town there she had been her.  the little house was on\n",
      "the wedding was to change cantle doom, who made in the\n",
      "door, they are the could creep.  then she said to go out again.  she went to the recognized hers but whing h\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 19) (87).(98).(68).(55).(4).(45).(31).(44).(90).(42).\n",
      "Average loss at step 20: 2.149967\n",
      "\tPerplexity at step 20: 8.584572\n",
      "\n",
      "Valid Perplexity: 22.60\n",
      "\n",
      "Generated Text after epoch 19 ... \n",
      "======================== New text Segment ==========================\n",
      "\t nger was come to any king, he said, i have pots, and cried it her bride, and beautiful morning as if the whole nul ran on her, and when\n",
      "he went into the wores, the king and the cantle horse, and she, \"if the evillity, oncertailone herself, but the maiden could not go withings, but they said, \"ah, they were\n",
      "from the girl of them, and thought, he looked him in the castle.  what was about to her, and that he would\n",
      "not however, was a bloweved him.\n",
      "thate that they were once white his clothes so that i will be toner him, and there were\n",
      "to anow-white someone what that he would be fres its cutif and all the second\n",
      "tree.  here took her his head and king and put\n",
      "opened her to end into the king's son, and they had been in it, but the maiden came into the water and began to go to the second time spossed the stood and three of the said, if he had been bor, and headed their forthan and he saw the king, and he said, \"i am not the king\n",
      "to his sweether to life stayed that he had see into a down, and said\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 20) (17).(97).(63).(77).(62).(90).(49).(35).(48).(18).\n",
      "Average loss at step 21: 1.897469\n",
      "\tPerplexity at step 21: 6.668992\n",
      "\n",
      "Valid Perplexity: 26.36\n",
      "\n",
      "Generated Text after epoch 20 ... \n",
      "======================== New text Segment ==========================\n",
      "\t n of the bear sister at last, and came in his eyes, and the moster she would given he has in and carry, and that they cannot anothing into the right,\n",
      "and there them into the ground, and the little glass, and what she wouch them one after\n",
      "came forthat and they came to branength with saw him, and they got there again.  then they should have it allow, and said that it was\n",
      "ingain his smoke, but the maiden,\n",
      "she handked her the long time it was once mountain witch the king's daughter that\n",
      "at the ring off in her brothers sonswer said, then all through the ring which she had came home, and so much for a little glass, who had thire in the world, and said the break of a mountain, he\n",
      "sat do out, and said, now the mastles were over the goat away, and said, and all the trusts with saw them that they went tailors, they arely was are a long time.  and whole our my little coused of them and the two\n",
      "peasted upon thire, but the maiden was a for man with their\n",
      "heard wall, and thought him enterever, and ate\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 21) (46).(38).(60).(94).(72).(62).(44).(78).(51).(30).\n",
      "Average loss at step 22: 2.016787\n",
      "\tPerplexity at step 22: 7.514146\n",
      "\n",
      "Valid Perplexity: 29.94\n",
      "\n",
      "Generated Text after epoch 21 ... \n",
      "======================== New text Segment ==========================\n",
      "\t very mother, but he came to them.  and the hand the father she doors two like a pitcher tair fell\n",
      "golden roon and went up and said, that the hard and thought of the first looked right, and i seek you will give that the bready and the second it, and then his godfathers, but if you till me in the\n",
      "little hare self to his finger and they were for yourselves.  the little hare sent the door of his father.\" then the two\n",
      "manswand betended, and the fly on him the room which were harends.\n",
      "\n",
      "a good to him and said that.  the whoan off, he said, he answered, that she was there and said, the head throw him with the girl has been straight to her.  the strone which her was\n",
      "breachom the flight, he asked he day, on the garther said the bridegroom's propertes. then his she was the opened the gold a roast death.  son, and whole cours iffishell\n",
      "whose heart way, and had the stover servest home who has fling, got and servese the maiden, brother breaght that it the golden raid to his seet third, and thought the\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 22) (85).(6).(10).(99).(80).(3).(96).(44).(0).(72).\n",
      "Average loss at step 23: 1.897693\n",
      "\tPerplexity at step 23: 6.670489\n",
      "\n",
      "Valid Perplexity: 23.96\n",
      "\n",
      "Generated Text after epoch 22 ... \n",
      "======================== New text Segment ==========================\n",
      "\t , but when he came to the gold of sit him\n",
      "to at all thate enting and the way that he was no had forth the flight on the wolf by the plencent away, and take you the bridegroom, and as she drying told her they wanted her.\n",
      "the youth large done, and the heart the stass.  then they had gone a paise in, and had pulled asto and the table, and said to him.  the girl was so taily and sevent on a golden the bride had to the king's daughter, and the first to you an to see three never reached ithous and said to have the king's daughter, and become presised the could not reach\n",
      "of that.  they have been the help it.\" next more of them.\n",
      "\n",
      "the king she well\n",
      "be a pressed with them, they was to have your horse and when he called piece, and had a rich one forth and he was something into the wedding with him to himself, and when he was out to him, and said, \"dide you were in a ches over, that the yout whaking at last his knimades, who had brother his horse and do gold that the carriage was the princess with t\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 23) (56).(72).(97).(88).(29).(21).(62).(78).(47).(15).\n",
      "Average loss at step 24: 1.830411\n",
      "\tPerplexity at step 24: 6.236450\n",
      "\n",
      "Valid Perplexity: 23.21\n",
      "\n",
      "Generated Text after epoch 23 ... \n",
      "======================== New text Segment ==========================\n",
      "\t wling and begain her passents, the peasants him to des, all the house.  they had to be a struck and the sevendood the thes what her passion one in the well one of the right, and the younger to the right, and was there, and took the watchen ound of his hand as she had to be queen in and see and put to the peasant would began the little hisself and went on his hand him to remairs, and the other their daysed the watch came back the second came and was on a neithful man was that she did not take their forest.  when he entered.  and when the princess, and\n",
      "the on the shoe, and the chesfetch it in the made here are with his would be asked him there in sto is willing, and when he answered the man to her server into his foot room, the king's son and see the king became your husband was to rode away again.\n",
      "\n",
      "she asked where this she had been sweep.  he went and wherever the king's servant, who was the king on their dear him, and bettered her of the water and said, for she had have no one was a good\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 24) (31).(89).(3).(69).(49).(64).(79).(99).(36).(91).\n",
      "Average loss at step 25: 1.651441\n",
      "\tPerplexity at step 25: 5.214487\n",
      "\n",
      "Valid Perplexity: 23.32\n",
      "\n",
      "\t Reducing learning rate\n",
      "Generated Text after epoch 24 ... \n",
      "======================== New text Segment ==========================\n",
      "\t llow went.  that i do not seat the bely more, and broke,\n",
      "and to a sitting it was about into the wolf by the tree, and at the broad land,\n",
      "but had been a hairer, and the prince was of anything, and at the same time.  he was was in a seat as that he could not another he went with him, and then the other and was about, it was beran asleep.  and he said sleat hearty, and a pound that he was to set her room with you.\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"oh, not master came and to pleep and said, for her full of joy, the brood-would not\n",
      "do, they had go and was to at and own and cried.  what is your trade, and the witched up to take a that there all off mean and they were in a husband, and said, \"i\n",
      "could not beard.  if you will not down comes of the that she was to be stranger high that they could not have you will before.\n",
      "\n",
      "they would which he was again,\"\n",
      "\n",
      "\"the doge of through the king's daughter had took the boy wite one off the wolf's hause, and that he could not\n",
      "be take one of them into them.  the miller, and when he satisfie\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 25) (38).(71).(90).(12).(16).(77).(18).(24).(15).(91).\n",
      "Average loss at step 26: 2.100774\n",
      "\tPerplexity at step 26: 8.172496\n",
      "\n",
      "Valid Perplexity: 21.85\n",
      "\n",
      "Generated Text after epoch 25 ... \n",
      "======================== New text Segment ==========================\n",
      "\t little head only him, but the king's son was the one was a peachied the servants on the said that they how do you stepishes, and they were hered, but the two rech on the shorment well, and it was\n",
      "into the little\n",
      "glade than to the down to be forsed the maiden, and said helder for their\n",
      "heart, took his bowling to the ring which her withins, and at the wolf cantle of standing, and then said he, this she saw down came to him, when it was to lead of the winted to the was, and was a husband at the bace of them, said the poor who had eater with her forth her, but as it was what all themselve in a way that he was no longer, and the miller see the cannot stopped and drank down and placed him with that he was to be placed her for me.  whis were, and he was shut to each of them and beggupon their down to the king was soon another, and they were into his daughter, who had taken the watch of the sea, and but whem they were freed, and the sisters.  he said to me, and nothing of her, and said the man, \n",
      "====================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Some hyperparameters needed for the training process\n",
    "\n",
    "num_steps = 26\n",
    "steps_per_document = 100\n",
    "valid_summary = 1\n",
    "train_doc_count = 100\n",
    "docs_per_step = 10\n",
    "\n",
    "# Capture the behavior of train perplexity over time\n",
    "train_perplexity_ot = []\n",
    "valid_perplexity_ot = []\n",
    "\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "# Initializing variables\n",
    "tf.global_variables_initializer().run()\n",
    "print('Initialized Global Variables ')\n",
    "\n",
    "average_loss = 0 # Calculates the average loss ever few steps\n",
    "\n",
    "# We use the first 10 documents that has \n",
    "# more than 10*steps_per_document bigrams for creating the validation dataset\n",
    "\n",
    "# Identify the first 10 documents following the above condition\n",
    "long_doc_ids = []\n",
    "for di in range(num_files):\n",
    "  if len(data_list[di])>10*steps_per_document:\n",
    "    long_doc_ids.append(di)\n",
    "  if len(long_doc_ids)==10:\n",
    "    break\n",
    "    \n",
    "# Generating validation data\n",
    "data_gens = []\n",
    "valid_gens = []\n",
    "for fi in range(num_files):\n",
    "  # Get all the bigrams if the document id is not in the validation document ids\n",
    "  if fi not in long_doc_ids:\n",
    "    data_gens.append(DataGeneratorOHE(data_list[fi],batch_size,num_unrollings))\n",
    "  # if the document is in the validation doc ids, only get up to the \n",
    "  # last steps_per_document bigrams and use the last steps_per_document bigrams as validation data\n",
    "  else:\n",
    "    data_gens.append(DataGeneratorOHE(data_list[fi][:-steps_per_document],batch_size,num_unrollings))\n",
    "    # Defining the validation data generator\n",
    "    valid_gens.append(DataGeneratorOHE(data_list[fi][-steps_per_document:],1,1))\n",
    "\n",
    "\n",
    "feed_dict = {}\n",
    "for step in range(num_steps):\n",
    "    \n",
    "    print('Training (Step: %d)'%step,end=' ')\n",
    "    for di in np.random.permutation(train_doc_count)[:docs_per_step]:            \n",
    "        doc_perplexity = 0\n",
    "        for doc_step_id in range(steps_per_document):\n",
    "            \n",
    "            # Get a set of unrolled batches\n",
    "            u_data, u_labels = data_gens[di].unroll_batches()\n",
    "            \n",
    "            # Populate the feed dict by using each of the data batches\n",
    "            # present in the unrolled data\n",
    "            for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):            \n",
    "                feed_dict[train_inputs[ui]] = dat\n",
    "                feed_dict[train_labels[ui]] = lbl\n",
    "            \n",
    "            # Running the TensorFlow operations\n",
    "            _, l, step_perplexity = session.run([optimizer, loss, train_perplexity_without_exp], \n",
    "                                                       feed_dict=feed_dict)\n",
    "            # Update doc_perpelxity variable\n",
    "            doc_perplexity += step_perplexity \n",
    "            \n",
    "            # Update the average_loss variable\n",
    "            average_loss += step_perplexity \n",
    "            \n",
    "        # Show the printing progress <train_doc_id_1>.<train_doc_id_2>. ...\n",
    "        print('(%d).'%di,end='') \n",
    "    \n",
    "        # resetting hidden state after processing a single document\n",
    "        # It's still questionable if this adds value in terms of learning\n",
    "        # One one hand it's intuitive to reset the state when learning a new document\n",
    "        # On the other hand this approach creates a bias for the state to be zero\n",
    "        # We encourage the reader to investigate further the effect of resetting the state\n",
    "        #session.run(reset_train_state) # resetting hidden state for each document\n",
    "    print('')\n",
    "    \n",
    "    # Generate new samples\n",
    "    if (step+1) % valid_summary == 0:\n",
    "      \n",
    "      # Compute average loss\n",
    "      average_loss = average_loss / (valid_summary*docs_per_step*steps_per_document)\n",
    "      \n",
    "      # Print losses\n",
    "      print('Average loss at step %d: %f' % (step+1, average_loss))\n",
    "      print('\\tPerplexity at step %d: %f' %(step+1, np.exp(average_loss)))\n",
    "      train_perplexity_ot.append(np.exp(average_loss))\n",
    "        \n",
    "      average_loss = 0 # reset loss\n",
    "      valid_loss = 0 # reset loss\n",
    "        \n",
    "      # calculate valid perplexity\n",
    "      for v_doc_id in range(10):\n",
    "          # Remember we process things as bigrams\n",
    "          # So need to divide by 2\n",
    "          for v_step in range(steps_per_document//2):\n",
    "            uvalid_data,uvalid_labels = valid_gens[v_doc_id].unroll_batches()        \n",
    "\n",
    "            # Run validation phase related TensorFlow operations       \n",
    "            v_perp = session.run(\n",
    "                valid_perplexity_without_exp,\n",
    "                feed_dict = {valid_inputs:uvalid_data[0],valid_labels: uvalid_labels[0]}\n",
    "            )\n",
    "\n",
    "            valid_loss += v_perp\n",
    "            \n",
    "          session.run(reset_valid_state)\n",
    "      \n",
    "          # Reset validation data generator cursor\n",
    "          valid_gens[v_doc_id].reset_indices()      \n",
    "    \n",
    "      print()\n",
    "      v_perplexity = np.exp(valid_loss/(steps_per_document*10.0//2))\n",
    "      print(\"Valid Perplexity: %.2f\\n\"%v_perplexity)\n",
    "      valid_perplexity_ot.append(v_perplexity)\n",
    "          \n",
    "      decay_learning_rate(session, v_perplexity)\n",
    "        \n",
    "      # Generating new text ...\n",
    "      # We will be generating one segment having 500 bigrams\n",
    "      # Feel free to generate several segments by changing\n",
    "      # the value of segments_to_generate\n",
    "      print('Generated Text after epoch %d ... '%step)  \n",
    "      segments_to_generate = 1\n",
    "      chars_in_segment = 500\n",
    "    \n",
    "      for _ in range(segments_to_generate):\n",
    "        print('======================== New text Segment ==========================')\n",
    "        \n",
    "        # Start with a random word\n",
    "        test_word = np.zeros((1,vocabulary_size),dtype=np.float32)\n",
    "        rand_doc = data_list[np.random.randint(0,num_files)]\n",
    "        test_word[0,rand_doc[np.random.randint(0,len(rand_doc))]] = 1.0\n",
    "        print(\"\\t\",reverse_dictionary[np.argmax(test_word[0])],end='')\n",
    "        \n",
    "        # Generating words within a segment by feeding in the previous prediction\n",
    "        # as the current input in a recursive manner\n",
    "        for _ in range(chars_in_segment):    \n",
    "          sample_pred = session.run(test_prediction, feed_dict = {test_input:test_word})  \n",
    "          next_ind = sample(sample_pred.ravel())\n",
    "          test_word = np.zeros((1,vocabulary_size),dtype=np.float32)\n",
    "          test_word[0,next_ind] = 1.0\n",
    "          print(reverse_dictionary[next_ind],end='')\n",
    "        print(\"\")\n",
    "        \n",
    "        # Reset train state\n",
    "        session.run(reset_test_state)\n",
    "        print('====================================================================')\n",
    "      print(\"\")\n",
    "\n",
    "session.close()\n",
    "\n",
    "# Write training and validation perplexities to a csv file\n",
    "with open(filename_to_save, 'wt') as f:\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    writer.writerow(train_perplexity_ot)\n",
    "    writer.writerow(valid_perplexity_ot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM with Beam-Search\n",
    "Here we alter the previously defined prediction related TensorFlow operations to employ beam-search. Beam search is a way of predicting several time steps ahead. Concretely instead of predicting the best prediction we have at a given time step, we get predictions for several time steps and get the sequence of highest joint probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_length = 5 # number of steps to look ahead\n",
    "beam_neighbors = 5 # number of neighbors to compare to at each step\n",
    "\n",
    "# We redefine the sample generation with beam search\n",
    "sample_beam_inputs = [tf.placeholder(tf.float32, shape=[1, vocabulary_size]) for _ in range(beam_neighbors)]\n",
    "\n",
    "best_beam_index = tf.placeholder(shape=None, dtype=tf.int32)\n",
    "best_neighbor_beam_indices = tf.placeholder(shape=[beam_neighbors], dtype=tf.int32)\n",
    "\n",
    "# Maintains output of each beam\n",
    "saved_sample_beam_output = [tf.Variable(tf.zeros([1, num_nodes])) for _ in range(beam_neighbors)]\n",
    "# Maintains the state of each beam\n",
    "saved_sample_beam_state = [tf.Variable(tf.zeros([1, num_nodes])) for _ in range(beam_neighbors)]\n",
    "\n",
    "# Resetting the sample beam states (should be done at the beginning of each text snippet generation)\n",
    "reset_sample_beam_state = tf.group(\n",
    "    *[saved_sample_beam_output[vi].assign(tf.zeros([1, num_nodes])) for vi in range(beam_neighbors)],\n",
    "    *[saved_sample_beam_state[vi].assign(tf.zeros([1, num_nodes])) for vi in range(beam_neighbors)]\n",
    ")\n",
    "\n",
    "# We stack them to perform gather operation below\n",
    "stacked_beam_outputs = tf.stack(saved_sample_beam_output)\n",
    "stacked_beam_states = tf.stack(saved_sample_beam_state)\n",
    "\n",
    "# The beam states for each beam (there are beam_neighbor-many beams) needs to be updated at every depth of tree\n",
    "# Consider an example where you have 3 classes where we get the best two neighbors (marked with star)\n",
    "#     a`      b*       c  \n",
    "#   / | \\   / | \\    / | \\\n",
    "#  a  b c  a* b` c  a  b  c\n",
    "# Since both the candidates from level 2 comes from the parent b\n",
    "# We need to update both states/outputs from saved_sample_beam_state/output to have index 1 (corresponding to parent b)\n",
    "update_sample_beam_state = tf.group(\n",
    "    *[saved_sample_beam_output[vi].assign(tf.gather_nd(stacked_beam_outputs,[best_neighbor_beam_indices[vi]])) for vi in range(beam_neighbors)],\n",
    "    *[saved_sample_beam_state[vi].assign(tf.gather_nd(stacked_beam_states,[best_neighbor_beam_indices[vi]])) for vi in range(beam_neighbors)]\n",
    ")\n",
    "\n",
    "# We calculate lstm_cell state and output for each beam\n",
    "sample_beam_outputs, sample_beam_states = [],[] \n",
    "for vi in range(beam_neighbors):\n",
    "    tmp_output, tmp_state = lstm_cell(\n",
    "        sample_beam_inputs[vi], saved_sample_beam_output[vi], saved_sample_beam_state[vi]\n",
    "    )\n",
    "    sample_beam_outputs.append(tmp_output)\n",
    "    sample_beam_states.append(tmp_state)\n",
    "\n",
    "# For a given set of beams, outputs a list of prediction vectors of size beam_neighbors\n",
    "# each beam having the predictions for full vocabulary\n",
    "sample_beam_predictions = []\n",
    "for vi in range(beam_neighbors):\n",
    "    with tf.control_dependencies([saved_sample_beam_output[vi].assign(sample_beam_outputs[vi]),\n",
    "                                saved_sample_beam_state[vi].assign(sample_beam_states[vi])]):\n",
    "        sample_beam_predictions.append(tf.nn.softmax(tf.nn.xw_plus_b(sample_beam_outputs[vi], w, b)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the LSTM with Beam Search to Generate Text\n",
    "Here we train the LSTM on the available data and generate text using the trained LSTM for several steps. From each document we extract text for steps_per_document steps to train the LSTM on. We also report the train perplexity at the end of each step. Finally we test the LSTM by asking it to generate some new text with beam search starting from a randomly picked bigram.\n",
    "\n",
    "### Learning rate Decay Logic\n",
    "Here we define the logic to decrease learning rate whenever the validation perplexity does not decrease\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate decay related\n",
    "# If valid perpelxity does not decrease\n",
    "# continuously for this many epochs\n",
    "# decrease the learning rate\n",
    "decay_threshold = 5\n",
    "# Keep counting perplexity increases\n",
    "decay_count = 0\n",
    "\n",
    "min_perplexity = 1e10\n",
    "\n",
    "# Learning rate decay logic\n",
    "def decay_learning_rate(session, v_perplexity):\n",
    "  global decay_threshold, decay_count, min_perplexity  \n",
    "  # Decay learning rate\n",
    "  if v_perplexity < min_perplexity:\n",
    "    decay_count = 0\n",
    "    min_perplexity= v_perplexity\n",
    "  else:\n",
    "    decay_count += 1\n",
    "\n",
    "  if decay_count >= decay_threshold:\n",
    "    print('\\t Reducing learning rate')\n",
    "    decay_count = 0\n",
    "    session.run(inc_gstep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Beam Prediction Logic\n",
    "Here we define function that takes in the session as an argument and output a beam of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_word = None\n",
    "\n",
    "def get_beam_prediction(session):\n",
    "    \n",
    "    # Generating words within a segment with Beam Search\n",
    "    # To make some calculations clearer, we use the example as follows\n",
    "    # We have three classes with beam_neighbors=2 (best candidate denoted by *, second best candidate denoted by `)\n",
    "    # For simplicity we assume best candidate always have probability of 0.5 in output prediction\n",
    "    # second best has 0.2 output prediction\n",
    "    #           a`                   b*                   c                <--- root level\n",
    "    #    /     |     \\         /     |     \\        /     |     \\   \n",
    "    #   a      b      c       a*     b`     c      a      b      c         <--- depth 1\n",
    "    # / | \\  / | \\  / | \\   / | \\  / | \\  / | \\  / | \\  / | \\  / | \\\n",
    "    # a b c  a b c  a b c   a*b c  a`b c  a b c  a b c  a b c  a b c       <--- depth 2\n",
    "    # So the best beams at depth 2 would be\n",
    "    # b-a-a and b-b-a\n",
    "        \n",
    "    global test_word\n",
    "    global sample_beam_predictions\n",
    "    global update_sample_beam_state\n",
    "    \n",
    "    # Calculate the candidates at the root level\n",
    "    feed_dict = {}\n",
    "    for b_n_i in range(beam_neighbors):\n",
    "        feed_dict.update({sample_beam_inputs[b_n_i]: test_word})\n",
    "\n",
    "    # We calculate sample predictions for all neighbors with the same starting word/character\n",
    "    # This is important to update the state for all instances of beam search\n",
    "    sample_preds_root = session.run(sample_beam_predictions, feed_dict = feed_dict)  \n",
    "    sample_preds_root = sample_preds_root[0]\n",
    "\n",
    "    # indices of top-k candidates\n",
    "    # b and a in our example (root level)\n",
    "    this_level_candidates =  (np.argsort(sample_preds_root,axis=1).ravel()[::-1])[:beam_neighbors].tolist() \n",
    "\n",
    "    # probabilities of top-k candidates\n",
    "    # 0.5 and 0.2\n",
    "    this_level_probs = sample_preds_root[0,this_level_candidates] \n",
    "\n",
    "    # Update test sequence produced by each beam from the root level calculation\n",
    "    # Test sequence looks like for our example (at root)\n",
    "    # [b,a]\n",
    "    test_sequences = ['' for _ in range(beam_neighbors)]\n",
    "    for b_n_i in range(beam_neighbors):\n",
    "        test_sequences[b_n_i] += reverse_dictionary[this_level_candidates[b_n_i]]\n",
    "\n",
    "    # Make the calculations for the rest of the depth of the beam search tree\n",
    "    for b_i in range(beam_length-1):\n",
    "        test_words = [] # candidate words for each beam\n",
    "        pred_words = [] # Predicted words of each beam\n",
    "\n",
    "        # computing feed_dict for the beam search (except root)\n",
    "        # feed dict should contain the best words/chars/bigrams found by the previous level of search\n",
    "\n",
    "        # For level 1 in our example this would be\n",
    "        # sample_beam_inputs[0]: b, sample_beam_inputs[1]:a\n",
    "        feed_dict = {}\n",
    "        for p_idx, pred_i in enumerate(this_level_candidates):                    \n",
    "            # Updating the feed_dict for getting next predictions\n",
    "            test_words.append(np.zeros((1,vocabulary_size),dtype=np.float32))\n",
    "            test_words[p_idx][0,this_level_candidates[p_idx]] = 1.0\n",
    "\n",
    "            feed_dict.update({sample_beam_inputs[p_idx]:test_words[p_idx]})\n",
    "\n",
    "        # Calculating predictions for all neighbors in beams\n",
    "        # This is a list of vectors where each vector is the prediction vector for a certain beam\n",
    "        # For level 1 in our example, the prediction values for \n",
    "        #      b             a  (previous beam search results)\n",
    "        # [a,  b,  c],  [a,  b,  c] (current level predictions) would be\n",
    "        # [0.1,0.1,0.1],[0.5,0.2,0]\n",
    "        sample_preds_all_neighbors = session.run(sample_beam_predictions, feed_dict=feed_dict)\n",
    "\n",
    "        # Create a single vector with \n",
    "        # Making our example [0.1,0.1,0.1,0.5,0.2,0] \n",
    "        sample_preds_all_neighbors_concat = np.concatenate(sample_preds_all_neighbors,axis=1)\n",
    "\n",
    "        # Update this_level_candidates to be used for the next iteration\n",
    "        # And update the probabilities for each beam\n",
    "        # In our example these would be [3,4] (indices with maximum value from above vector)\n",
    "        this_level_candidates = np.argsort(sample_preds_all_neighbors_concat.ravel())[::-1][:beam_neighbors]\n",
    "\n",
    "        # In the example this would be [1,1]\n",
    "        parent_beam_indices = this_level_candidates//vocabulary_size\n",
    "\n",
    "        # normalize this_level_candidates to fall between [0,vocabulary_size]\n",
    "        # In this example this would be [0,1]\n",
    "        this_level_candidates = (this_level_candidates%vocabulary_size).tolist()\n",
    "\n",
    "        # Here we update the final state of each beam to be\n",
    "        # the state that was at the index 1. Because for both the candidates at this level the parent is \n",
    "        # at index 1 (that is b from root level)\n",
    "        session.run(update_sample_beam_state, feed_dict={best_neighbor_beam_indices: parent_beam_indices})\n",
    "\n",
    "        # Here we update the joint probabilities of each beam and add the newly found candidates to the sequence\n",
    "        tmp_this_level_probs = np.asarray(this_level_probs) #This is currently [0.5,0.2]\n",
    "        tmp_test_sequences = list(test_sequences) # This is currently [b,a]\n",
    "\n",
    "        for b_n_i in range(beam_neighbors):\n",
    "            # We make the b_n_i element of this_level_probs to be the probability of parents\n",
    "            # In the example the parent indices are [1,1]\n",
    "            # So this_level_probs become [0.5,0.5]\n",
    "            this_level_probs[b_n_i] = tmp_this_level_probs[parent_beam_indices[b_n_i]]\n",
    "\n",
    "            # Next we multipyle these by the probabilities of the best candidates from current level \n",
    "            # [0.5*0.5, 0.5*0.2] = [0.25,0.1]\n",
    "            this_level_probs[b_n_i] *= sample_preds_all_neighbors[parent_beam_indices[b_n_i]][0,this_level_candidates[b_n_i]]\n",
    "\n",
    "            # Make the b_n_i element of test_sequences to be the correct parent of the current best candidates\n",
    "            # In the example this becomes [b, b]\n",
    "            test_sequences[b_n_i] = tmp_test_sequences[parent_beam_indices[b_n_i]]\n",
    "\n",
    "            # Now we append the current best candidates\n",
    "            # In this example this becomes [ba,bb]\n",
    "            test_sequences[b_n_i] += reverse_dictionary[this_level_candidates[b_n_i]]\n",
    "\n",
    "            # Create one-hot-encoded representation for each candidate\n",
    "            pred_words.append(np.zeros((1,vocabulary_size),dtype=np.float32))\n",
    "            pred_words[b_n_i][0,this_level_candidates[b_n_i]] = 1.0\n",
    "\n",
    "    # Calculate best beam id based on the highest beam probability\n",
    "    # Using the highest beam probability always lead to very monotonic text\n",
    "    # Let us sample one randomly where one being sampled is decided by the likelihood of that beam\n",
    "    rand_cand_ids = np.argsort(this_level_probs)[-3:]\n",
    "    rand_cand_probs = this_level_probs[rand_cand_ids]/np.sum(this_level_probs[rand_cand_ids])\n",
    "    random_id = np.random.choice(rand_cand_ids,p=rand_cand_probs)\n",
    "\n",
    "    best_beam_id = parent_beam_indices[random_id]\n",
    "\n",
    "    # Update state and output variables for test prediction\n",
    "    session.run(update_sample_beam_state,feed_dict={best_neighbor_beam_indices:[best_beam_id for _ in range(beam_neighbors)]})\n",
    "\n",
    "    # Make the last word/character/bigram from the best beam\n",
    "    test_word = pred_words[best_beam_id]\n",
    "    \n",
    "    return test_sequences[best_beam_id]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Training, Validation and Generation\n",
    "We traing the LSTM on existing training data, check the validaiton perplexity on an unseen chunk of text and generate a fresh segment of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/online1/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py:1714: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "(55).(70).(83).(31).(27).(32).(45).(84).(86).(46).\n",
      "Average loss at step 1: 4.394968\n",
      "\tPerplexity at step 1: 81.042013\n",
      "\n",
      "Valid Perplexity: 56.04\n",
      "\n",
      "Generated Text after epoch 0 ... \n",
      "======================== New text Segment ==========================\n",
      " oaut and to and the kill, and then then then then then then ther, and ther, and then him and then the king, and\n",
      "when him taid the kill, and ther, and then him and then the king, and\n",
      "when him taid the kill, and ther, and ther, and then him and and then him the kill, and ther, and then him and and then him the king to the king, and\n",
      "when him, when then him the kill, and then him and then the king then him the king to the king, and\n",
      "when him taid the kill, and then then ther, and then him and then the king then ing and then him and and then him the kill, and then him and then the king, and\n",
      "when him, when then the king, when then him the kill, and ther, and then then then then then then ther, and then him and then the king, and\n",
      "when him taid the kill, and ther, and then him and then the king, and\n",
      "when him, when the king the kire, and then him and then the king, and\n",
      "when him taid the kill, and then him and then the king, and\n",
      "when him, when then him the kill, and ther, and then him and and then \n",
      "====================================================================\n",
      "\n",
      "(53).(33).(26).(69).(51).(27).(31).(64).(65).(86).\n",
      "Average loss at step 2: 3.029664\n",
      "\tPerplexity at step 2: 20.690285\n",
      "\n",
      "Valid Perplexity: 58.84\n",
      "\n",
      "Generated Text after epoch 1 ... \n",
      "======================== New text Segment ==========================\n",
      " himsf you she\n",
      "have to church, anged the taincess was the bear again.  they the taidlarriaged the taid of the the carried tailor, to the tailor once, and cried, \"do you she was to that, and int and and stailor, anged the been that, away.\n",
      "\n",
      "when she that, sailong tailor, the crince, awas the believed to that once, othe that, and the little tailor, and which, when in the tale and was her and this, to himself in athere the princess that, to hild, and that it the princess what his in once, and she tailong tailor had bearrive to him at once, aftere the princess whad been that, and to church, and the taincess was married this, to him, the taidlarriaged the taid on that been that, after the blose, and was to thapping and happened, and the beincess was to churching and that in the vise?  if in that his, to him incess the married to him at on the littin the tairied to himself in to that, and\n",
      "the littin that himself in the tailor, she taid of the tailong to bearried the married to him at out, the be\n",
      "====================================================================\n",
      "\n",
      "(12).(81).(39).(17).(69).(42).(75).(28).(53).(51).\n",
      "Average loss at step 3: 2.437550\n",
      "\tPerplexity at step 3: 11.444965\n",
      "\n",
      "Valid Perplexity: 97.17\n",
      "\n",
      "Generated Text after epoch 2 ... \n",
      "======================== New text Segment ==========================\n",
      " jumerry.\n",
      "to the doors, a little away and i, sorrowful, and strees, the dirl of the bride the wedding-g  then then the litt-g hare seed take off the lid, take off little hard comes bace more and, take off.\n",
      "then the little here sees the little away.\n",
      "the little hare comes bade and says, take off\n",
      "the lid, take off\n",
      "the lid, the wedding then the little harseed, and goes await is not his\n",
      "bride, all goes\n",
      "awikes and ted however the carridegroom, at the little hare sese that it, the litten take off.\n",
      "then ther take ofn the lid, the weddith-guests\n",
      "losees at into the hise off.\n",
      "then the lid, all he hang-guests.  the lid, the wittle hare's hut.\n",
      "\n",
      "then the litten take off.\n",
      "then the little he sees that it is not his brides and goes away and, and goes back to hare comes off.\n",
      "then says the ledding-guethe little hare sees, and goes the little hare's home.  then in the lil\n",
      "with the wedding-golster hare's sorrow\n",
      "the littls\n",
      "hare that, and says not his bride, and goes\n",
      "away.\n",
      "the littow hare coess back away.  the \n",
      "====================================================================\n",
      "\n",
      "(44).(72).(30).(84).(54).(43).(2).(26).(99).(95).\n",
      "Average loss at step 4: 2.345023\n",
      "\tPerplexity at step 4: 10.433508\n",
      "\n",
      "Valid Perplexity: 48.69\n",
      "\n",
      "Generated Text after epoch 3 ... \n",
      "======================== New text Segment ==========================\n",
      " of yet him, who harden\n",
      "the crincess, she wending waste had said, \"oh, yed,\" said three and that\n",
      "he was again to have you away again, however, not fait, however, again, and when he was\n",
      "tooked her.  then when he was to go on, and that when the\n",
      "king's doned, and the faithful to the kin, and the faithful.\" he shorselied, \"oh, which he said, \"i dered him that he was to go on, and that off his head, \"i am and to the king was his faithful.\n",
      "\n",
      "the king's dears, and them on the that, when she came my could, where have that he head, and to the king cut his had again, the faithf wern it, horse, and he had not out of to thing to him to then they they would began to then he was again, and shat one liked him whene her castle, an that one liked him out to the which him, and that once when the king took he warded and that\n",
      "one of in its ithful, and the faithfme was thank, what his said to for him, and thread she could to the herth, and the giants and that willed them him, and said he happed and together, a\n",
      "====================================================================\n",
      "\n",
      "(48).(2).(17).(3).(42).(9).(43).(97).(69).(26).\n",
      "Average loss at step 5: 2.083696\n",
      "\tPerplexity at step 5: 8.034106\n",
      "\n",
      "Valid Perplexity: 36.93\n",
      "\n",
      "\t Reducing learning rate\n",
      "Generated Text after epoch 4 ... \n",
      "======================== New text Segment ==========================\n",
      "   brother, asked him off himself.  when they saw in it would hease.  then he saw to the good put it that she wout the hang and the called them, and went said, now gave bed into him away.  one your fouth\n",
      "into the stille, an with the wanted to seized them asked to take him fall of you have beedle, and was to that.  then they came to seed, and was to they.  what it would had gold as ally the hear, the host, said, he was she came will the gladle and satisfied a cudgel wit in the warder, and went to the gold-ass.  he said to the gortainly answered the little to stime, and went as my said, brother, i will give him which have you.  then he saw that he into the fox and still, i went, and the father and\n",
      "spried, he sprance, and well to the goat, and then all the castle and\n",
      "said, it with the gold-ass, and that will have good.  the father.  he had not so that it put if a gold-ass.  and the father, and was to the good bee cut and castle, and when the good been into to the gold-ass cantle answered, an\n",
      "====================================================================\n",
      "\n",
      "(38).(98).(25).(53).(82).(74).(54).(33).(16).(89).\n",
      "Average loss at step 6: 2.208665\n",
      "\tPerplexity at step 6: 9.103551\n",
      "\n",
      "Valid Perplexity: 66.41\n",
      "\n",
      "Generated Text after epoch 5 ... \n",
      "======================== New text Segment ==========================\n",
      " erself again.  then the mother herself was obliged thad the grey own to\n",
      "be all the patas out her had.  and then at last the child had rest beneath the\n",
      "ground.  they had put it in\n",
      "and spread frover,\n",
      "what was drawnd, and she was oncened herself again, and stretched upwards, and when at last the child had rest benover the ture they out out again.  then at last the\n",
      "child had rest beneath the\n",
      "greup and strike the dom with a rod, and when she harself in,\n",
      "and then at last the child had rest beneath the\n",
      "ground.  the mother had done that,\n",
      "it was drawn in, i and then at last the child had rest beneath the\n",
      "ground.  her mother,\n",
      "and when it was hem to to he a rave, ad the time, her her could not came again\n",
      "and then the mother, and stentched upwas, and when they had have the wards, and when they had put is in\n",
      "and strave the down in them rave, and strike the arm with a rod, and when she had done\n",
      "that,\n",
      "it or came tround.  whose the art with a rave her armed.  when the own thumber, ans, oncher when sher \n",
      "====================================================================\n",
      "\n",
      "(6).(68).(27).(18).(90).(55).(53).(28).(62).(30).\n",
      "Average loss at step 7: 2.131481\n",
      "\tPerplexity at step 7: 8.427342\n",
      "\n",
      "Valid Perplexity: 61.27\n",
      "\n",
      "Generated Text after epoch 6 ... \n",
      "======================== New text Segment ==========================\n",
      " murns that.  he said, those were quarreling\n",
      "light of stairs of\n",
      "the fine, and there, godfather, i saw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you and you at the wers came ing.  on a fifhe flight, i peeper brought he said, she who had said that.  he died to the fiant, but thing flight, i peeped\n",
      "through the keUNKole of a doo, which wedding ing\n",
      "fingers where were hisry to find by son.y one hearthey, and away.  he haUNKer, then the thethan tweth, and that were you flight, he took at hasses.  on\n",
      "the shoved\n",
      "lived hing, and breat him wight would he dhove himself, an he to the godfather had the fourth flight, he saw his strant, be and saked the the fingers which i came frors.  the fife.\n",
      "the fir thought, but that said the maiden the carriage, but is not true.  the mountaid that.  and had said that, the fishes came\n",
      "and served\n",
      "themselves up.  as the maid the hearth\n",
      "flight i a saw of my said,  oh, that is not truese.  the\n",
      "mas became alarmed, again, and brought then the firth got to hiver, and, he said, said the godfather would have done to h\n",
      "====================================================================\n",
      "\n",
      "(85).(6).(76).(12).(31).(51).(70).(21).(88).(13).\n",
      "Average loss at step 8: 2.236765\n",
      "\tPerplexity at step 8: 9.362990\n",
      "\n",
      "Valid Perplexity: 27.90\n",
      "\n",
      "Generated Text after epoch 7 ... \n",
      "======================== New text Segment ==========================\n",
      " d together, said to that, they who had to see over that he had angered himself without he hang and haves away to take of a pleased, and said, take off, for the kiss, and says to the theyes of ousenten he had bround then ther on the king's hease to the castle themselves, at the king's heard through them, and then her heart's said to him, and then the light of took ther to him, and then the saw the theyes he had not out let to make.  they gave him this was the beautifuld him, and they wered them tog he came to him, and maidenly willowed him that they saw they were all of the\n",
      "kill's heard.  then three of the heard the tole and for the king, and sat that the beautiful forest, as the youth.  the youth he had beautiful on this thing, and when he was, but the king's day, and when the litte.  the peade said thim, and then the kitch sat they came to her to a king's daunce.  the king's heard to the servant they came to him, and when he happened the king's daw the yout he said, that there in the ki\n",
      "====================================================================\n",
      "\n",
      "(17).(97).(35).(77).(45).(86).(42).(18).(32).(61).\n",
      "Average loss at step 9: 2.168703\n",
      "\tPerplexity at step 9: 8.746928\n",
      "\n",
      "Valid Perplexity: 26.41\n",
      "\n",
      "Generated Text after epoch 8 ... \n",
      "======================== New text Segment ==========================\n",
      " st, and that the little.  then she had nee not his why, and then her on with him and with him.  then had by the with his frother, any likewing him, i have away.  he had no look time atter to she world he took this father, i will give your brown on the stone of a glass his wome.  that i have gone on this, and then he was standing behind his father, i fell at they could now well him.\n",
      "\n",
      "the maiden have been stoned it with your broth.  where han eyes the god for\n",
      "himself answered himself with your broth,\n",
      "which was standing and said that his head, that you have his hand he saw that it was a ring to the horse, answered hans.\n",
      "\n",
      "the man, when the\n",
      "may out of his for yours, answered, answered his head, she could not get his which had head, and had been they are nothing that standing before her brothers, you have befull his daughter, i will givat him, an light here.\n",
      "\n",
      "and no one one one of your cloth,\n",
      "and\n",
      "still no one on that he maid, he had began the window, and the morder to himself, there in he want\n",
      "====================================================================\n",
      "\n",
      "(53).(70).(64).(47).(1).(0).(57).(42).(52).(8).\n",
      "Average loss at step 10: 2.107604\n",
      "\tPerplexity at step 10: 8.228502\n",
      "\n",
      "Valid Perplexity: 29.88\n",
      "\n",
      "\t Reducing learning rate\n",
      "Generated Text after epoch 9 ... \n",
      "======================== New text Segment ==========================\n",
      " possibed to say which we hap that it.  there said, in the nestion, and devery were going to her and me.  at her the wedding-shape diss.  he happened am the toldow her some within and wersed the\n",
      "king's said to his\n",
      "daughter whome that he set the\n",
      "rainst wife that his not bely.  he welleng was tell, letchown which\n",
      "the king's came to that you had burnt they were lowed him there the life, and the enchantrair, and was so someated to he how that will you a servant.  when he hapevered a heaced\n",
      "brated to him, and dide, and whe would be were ally it.  the keyest on the well, appened to her quite that wer said into hapself with\n",
      "that, he came to dood that this heards something with his brother.  then they were huntskinged him so father.  then he said year she had around.  then the well.  that he ressess which he house with your heart said, she lived himself with\n",
      "that her away, and down them received me.  in the\n",
      "charstrals and fell beautiful went but at on, and said, you she took the king's first was \n",
      "====================================================================\n",
      "\n",
      "(11).(31).(54).(44).(64).(73).(25).(12).(2).(0).\n",
      "Average loss at step 11: 2.278545\n",
      "\tPerplexity at step 11: 9.762469\n",
      "\n",
      "Valid Perplexity: 25.10\n",
      "\n",
      "Generated Text after epoch 10 ... \n",
      "======================== New text Segment ==========================\n",
      " astly, and when she was, showeved.  the kin said he, and said, if you shudder.  she can to said, and tried him in\n",
      "had not.\" said the king's son,\" said the king sat down agard, and the morning into and a child, ans, she tooked told he was terrified, and behind him.  and that when threw that he reached to she was along again, and warmed here a frog to helps, and said, \"i cannot be and beautiful\n",
      "eyes.  and when we shall because them to him, but they which had beated him into the cross.  the king's son, and said\n",
      "they as now the carriage.  it, and when she was again to te the king.\n",
      "\n",
      "theird said to three, and be well again, and wanted the way somethirs, and saw that he roomed, answered, ans when the young king, and behaved not knew what there he satisfied than the well.\" said this heart the king's daughter nothing, ah, his was married that she warms someand, and werring to the king's beather agaon, and sat down to the door which had back to the princess,\n",
      "youngest princess, with have helped the\n",
      "====================================================================\n",
      "\n",
      "(91).(93).(0).(39).(38).(87).(66).(81).(88).(48).\n",
      "Average loss at step 12: 2.311672\n",
      "\tPerplexity at step 12: 10.091287\n",
      "\n",
      "Valid Perplexity: 25.42\n",
      "\n",
      "Generated Text after epoch 11 ... \n",
      "======================== New text Segment ==========================\n",
      " ing.\" said that they would but died to the door, but the carried, and thought to me.  then two of the said time the king's daughter down, they had beautiful himsed the king's son, answered the old come back again to the kill, and the two eldest at last heard at it, and same the mannikin said,\n",
      "then the two on the\n",
      "king's showent, anced with that they went down, which they lived timself, and the king's\n",
      "daughter arms, they had beaut of the king's son wome togethis, and that it the wonderful gimpleton his brought\n",
      "withing, the king.\n",
      "then the peasant house.  and jumped threw himself with theing, the kind\n",
      "stroughout to the manniking's daughtened the king's son, who was a lighter to him, and saw thathe window, and said, \"that it was\n",
      "nothing the kingdown, they light out off as tollow, and them, who have away to death the manniking's daughtened the king's son, she was simpinto a back, and solend with me.  the king's dauped the kitchen answtued, the world can down again, but that in the two.  and th\n",
      "====================================================================\n",
      "\n",
      "(48).(98).(99).(29).(53).(71).(21).(55).(6).(61).\n",
      "Average loss at step 13: 2.174525\n",
      "\tPerplexity at step 13: 8.798006\n",
      "\n",
      "Valid Perplexity: 21.52\n",
      "\n",
      "Generated Text after epoch 12 ... \n",
      "======================== New text Segment ==========================\n",
      " n whomething at three sons with it.  but his strong down, i wind gave it.  when he said, and had been his happen\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out of them to the golden haiden with his back, answered, answered this they were hand, answered them to the stones, and there wicked her that he was histed to his fathers, and then he saw a little bother willing you wished him, i will go for it.  and the maid he had said that that he was blow the willowed home.  then they were them to to that her father, if you have be ther horsed the son under stonest her got to him, and\n",
      "that he had been stole the forest, and when, however said, yes had been ness, and i will great deal of them into\n",
      "his back that they were all to the stone.\n",
      "\n",
      "then his marden said him, and the maiden whom side, that she had go that the king whice that the father, why should have them forth that you are nailong, answered him that he was to make when the windowed how himself and said, \"i have not be the maiden without, and as he looking at the horse worn, and she\n",
      "====================================================================\n",
      "\n",
      "(69).(40).(96).(80).(38).(92).(0).(37).(51).(35).\n",
      "Average loss at step 14: 2.174636\n",
      "\tPerplexity at step 14: 8.798983\n",
      "\n",
      "Valid Perplexity: 21.82\n",
      "\n",
      "Generated Text after epoch 13 ... \n",
      "======================== New text Segment ==========================\n",
      " ng king,\n",
      "        my lived hearted, and she daughted in that he could not be the live that that he saw him the mocking in to heart, town her sitter and shout of that he says,\n",
      "     my father she could not get home,\n",
      "and she had then there, and threw her, said to her.  the little hare come to he head and went out, she went out and and she was staken the bride, and when the little hare's\n",
      "heard me,\n",
      "     milled the little hards, and weeping,\n",
      "   then the looke to he hound all in the little hare come, and asked the door, and went out of the litted them in it.  the more man's house.\n",
      "\n",
      "to,\" said threw her, so that i says the water, and well out again, and when she heard that the marriage offer again, and when he have been this she was reling in her head, she was stake the mouthing answered to seek out offer, and when the house here that is sorrowful.\" then the money were the little wed her hand, and that her fall of the bird, the man said that the maiden, and went out that he was now here were and s\n",
      "====================================================================\n",
      "\n",
      "(23).(89).(47).(44).(72).(27).(80).(12).(95).(94).\n",
      "Average loss at step 15: 2.071922\n",
      "\tPerplexity at step 15: 7.940069\n",
      "\n",
      "Valid Perplexity: 24.33\n",
      "\n",
      "Generated Text after epoch 14 ... \n",
      "======================== New text Segment ==========================\n",
      " rdes, that strawned three, and they went out and said.  when stake of you.  what which he haveressed three days that he way,\" said that they would no they were alled them, but he will, and the seven yourself on that they was away to his head, she was told them his head, shall had give you wishing, and asked her horses, who was agable, and with them, whath the fishertake off as the\n",
      "three with their with them, and took three days and drease, that she ground.\n",
      "\n",
      "and when she was to go of them to them, what where you have been, and sented the heard themselves thought he well.\" then he tooked themselves that he said,\n",
      "what she had nothing, and when she said to his head, as the devil.\"\n",
      "\n",
      "they went out as the devil, so your brothers delighted up to the devil here to them, it will because that she dear you to him to be our king to his meater, where had gone them,\n",
      "ther, there was them so toge that the ship, and happened together they will become had three themseaves the willow, answered the whild had\n",
      "====================================================================\n",
      "\n",
      "(79).(31).(13).(18).(51).(11).(44).(57).(90).(68).\n",
      "Average loss at step 16: 1.959288\n",
      "\tPerplexity at step 16: 7.094274\n",
      "\n",
      "Valid Perplexity: 22.04\n",
      "\n",
      "Generated Text after epoch 15 ... \n",
      "======================== New text Segment ==========================\n",
      " n\n",
      "him, but threw him away, and thited the grace, and threw him unten to his head, but the kiut's son again and say, and the king's said, he was\n",
      "thering in his heard away.\n",
      "\n",
      "when he wards, and whiten heards all and said the door, and the king's daughter with him away.  the little har pared him.  he said,\n",
      "now the old woman came off hings, and\n",
      "when he was\n",
      "away and said, thanswere the old woman came to that the kiss, and says, take or her and heart them than the king's but her ate, it was and alled the king's day, and when he had given him, i will nothing, but when she went away, the king's daughter with his head, and which he way.\n",
      "\n",
      "they was all at lame, and then he wanted and spranced him, and his father, which had got him himself, and then she was all at lame, and\n",
      "went to him, and he everything him, and then he had away and said he which he way, and when she was\n",
      "placed the heart, and then she said that you were your brougtly heard by the and said, the huntsman once more and said, but he wen\n",
      "====================================================================\n",
      "\n",
      "(4).(82).(63).(59).(21).(91).(79).(0).(96).(24).\n",
      "Average loss at step 17: 2.194031\n",
      "\tPerplexity at step 17: 8.971300\n",
      "\n",
      "Valid Perplexity: 24.31\n",
      "\n",
      "Generated Text after epoch 16 ... \n",
      "======================== New text Segment ==========================\n",
      " e from\n",
      "their day, and said that when they had soon a back, and the dressong which his father's heard that he went into the castle.  at length be saw thes one of though, and when they went away it is there, and say that they went and beautiful home.  the\n",
      "husband, there in which they came to the castle, when they had hearded at the carriages on their day, and that it is a dretting with them, and when when he hardered to this to thought of that the can learnt to\n",
      "the chilst, he was no saying.  then they were on she was take of the castle,\n",
      "and when he saw that it was the dister, answered there.  the old woman said to them, what have you no, but they came to take him to a him and remained up to her heart of thought, and\n",
      "when he would not on his way.  then the\n",
      "kingdom said to herself on his whird, and they went away, to his heads, and they went aware into that he was out with him, and that they went away, what it is\n",
      "to be his three becry died to his way, which had brought he with him, they had \n",
      "====================================================================\n",
      "\n",
      "(25).(66).(68).(99).(30).(56).(41).(26).(76).(12).\n",
      "Average loss at step 18: 2.084313\n",
      "\tPerplexity at step 18: 8.039065\n",
      "\n",
      "Valid Perplexity: 25.23\n",
      "\n",
      "\t Reducing learning rate\n",
      "Generated Text after epoch 17 ... \n",
      "======================== New text Segment ==========================\n",
      " thful\n",
      "eyes, and said, that where with youred by the old woman left there they said he, but thought oper and drink to him in, said, i preaces out of the snake, and\n",
      "said she reached them, and sent out to their\n",
      "litting into that her fairs that to sea, and ran out opened one with him, he was reased here in the air, and they carriage.  then they took the wards the young wrently said, who was there, and that she wanted to set the king\n",
      "somether athe, and fell on themselves, when he has replied, but if yoh, but the mand said, that shapped be that he said.  out of them, and sented to sea, where is yoursent they went to the back, and said, the second his everything that he would have home me, dear mounte, they sack, who had been they went and received her, and told him will be with all and came and sembled them.  when this he had not seen together andful on themselves.  then she was\n",
      "placed out to her, where in the worl came back again.  that she was still happosed to their husbned with holes of th\n",
      "====================================================================\n",
      "\n",
      "(90).(41).(51).(8).(74).(35).(98).(89).(20).(3).\n",
      "Average loss at step 19: 2.169042\n",
      "\tPerplexity at step 19: 8.749896\n",
      "\n",
      "Valid Perplexity: 22.02\n",
      "\n",
      "Generated Text after epoch 18 ... \n",
      "======================== New text Segment ==========================\n",
      " ngdown in their childrens were table, and at last the\n",
      "child have gotter.\n",
      "     then went into but the bride took told the knight, i was spread frest, and\n",
      "when the golden coughs still home with you.\n",
      "\n",
      "the king's son put of this with them.\n",
      "then he\n",
      "said,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  when he wants to dief, the wedding was spring ouseened in them, and told him the door, and when he was spring up and said to his lay, that her he tooked him, this dear of back to home, and took him to a back and, but they said the maiden, accully of your life.\"\n",
      "then she said to down to his legs.\n",
      "\n",
      "the king's said them rest once more and saw that is nothing towas to the door, and it was stonce was oured and sisted that it was stood standing but in its stonest, when this they went in the water and begged then he had saw his was oblight, but what it, and where is the most beaut out of them, and that went into the kingdom, and when she had done thing.  the king's son came to thrown that had they came to them, and they went inside him as he was s\n",
      "====================================================================\n",
      "\n",
      "(49).(50).(27).(43).(60).(29).(3).(99).(68).(47).\n",
      "Average loss at step 20: 2.237818\n",
      "\tPerplexity at step 20: 9.372862\n",
      "\n",
      "Valid Perplexity: 26.16\n",
      "\n",
      "Generated Text after epoch 19 ... \n",
      "======================== New text Segment ==========================\n",
      " thful\n",
      "bettleg, which heard the wook and saw that, anyou shall went to thy out of thought, anly, and them were on yourself to himself, and then went them ablink on she was towed that the kingdom.\n",
      "\n",
      "then a which the watter saw that is not once mely and dreak off themselves.  he said, what is that she was beautifully, and then she was father's eyes, and the king's\n",
      "son had but to their\n",
      "father.  and they came to there, and they were through the children, however, where the father had ranness through the king, and then the mounded himself on that the wept, and they went on each others whoerved\n",
      "the king broomers, the mouth and cried,\n",
      "UNKhind better.\" UNKerhaps a brother was cloner to lived the king's\n",
      "dant.  then she was simpleton\n",
      "retime, the king came to an at, but when stook him, but the\n",
      "mooner they went to them, and sat that in twice, and said the finger, any went to any were, but they said, \"i with the thought, and fell in the\n",
      "wall, and then they went to drive him against it there, answered the\n",
      "====================================================================\n",
      "\n",
      "(31).(53).(12).(73).(48).(61).(87).(52).(51).(98).\n",
      "Average loss at step 21: 1.816847\n",
      "\tPerplexity at step 21: 6.152429\n",
      "\n",
      "Valid Perplexity: 21.66\n",
      "\n",
      "Generated Text after epoch 20 ... \n",
      "======================== New text Segment ==========================\n",
      " nte.  then that the peopled him to his handly them, that is now they two brought that he was had been for the tresh.  the word, and said the whole was take off that had take\n",
      "me as a king's son, and they came to thumbling, who knows that they were all their with them.  then she was terrible, that they came to this to the king's son like the door, and said to that it wanting to the room and water to drink, they tree come, and the woman came so her three, and three bear took the world.  the king said,\n",
      "there, they were a going to me.  they would have been their\n",
      "husband became out again.  threw it in my little good forgetter.  then she was ready to her, and three days, when\n",
      "the second her daughter what i have.\n",
      "\n",
      "they give her that she had given the window, which was spread off, and they came to the\n",
      "tree, answered, answered the king's days,\n",
      "who was sitting into her stone, and when she whom to his everythings.  they went to them.  they would have you ween the little hare comes back and cried, an\n",
      "====================================================================\n",
      "\n",
      "(91).(95).(92).(18).(45).(74).(70).(78).(79).(5).\n",
      "Average loss at step 22: 2.199685\n",
      "\tPerplexity at step 22: 9.022173\n",
      "\n",
      "Valid Perplexity: 19.00\n",
      "\n",
      "Generated Text after epoch 21 ... \n",
      "======================== New text Segment ==========================\n",
      " yight.  what she had said that the paddock said the door, and said, i happened the country, but the kitched the peasant there, and said that two present time the peasant thought that she counted the peasant, and when the king saw things, i wive got the told him in the paddock, and that the present to her, and told him this the padoner that is to the youth.  there is the peasant, and when two open the king, and the peasountain to the peasay, and said.\n",
      "\n",
      "the percew himselves, and i will go at the peasant, and when it was, and they went to you that, and the peopled him that he waiting.  then the king's daught,\n",
      "and the mother, answered the child, ands said that the pearent counted it, and said the man, and told him that he want into the mouth.\n",
      "\n",
      "than, there the peasant, when this the peasento the wood, and i will give, and said, the peasuressed her to the king, and he got the wook and said,\n",
      "that, i have not.  then he said there.  then you must before the paddock country, answered him to the ki\n",
      "====================================================================\n",
      "\n",
      "(91).(35).(55).(6).(59).(49).(84).(95).(87).(70).\n",
      "Average loss at step 23: 2.065187\n",
      "\tPerplexity at step 23: 7.886775\n",
      "\n",
      "Valid Perplexity: 22.85\n",
      "\n",
      "Generated Text after epoch 22 ... \n",
      "======================== New text Segment ==========================\n",
      " s death.\n",
      "\n",
      "afar they well of the king, and she took him to his bride, and when the peasant sat down and said the king, and when it wait and three days, answered that the old king, shouse with heard, and she\n",
      "peared him in him.\n",
      "\n",
      "the maiden, and when they came to the king, anted that it was not for it, andelived that he saided him, what it was not before him, and that in it, and then the king said to her\n",
      "house, answered this, and when he had sented the bright sun did bring it to light.  the pard was so pranted his finger of them, what it wait and to before the first time, and as you have threated him into the king's day.  and when he was sented on it, however, had the other and leaved that it was in and dead that it to any\n",
      "humne, and then went to be well again.  and she told him that hall yourses.  then she ordered to the kid\n",
      "again, answered them, and the\n",
      "peasant said,\n",
      "now he had she one said to his wifer, but the king was to before his daughty.  that he went on the back into his would.  the\n",
      "====================================================================\n",
      "\n",
      "(56).(55).(61).(17)."
     ]
    }
   ],
   "source": [
    "filename_to_save = 'lstm_beam_search_dropout'\n",
    "\n",
    "# Some hyperparameters needed for the training process\n",
    "\n",
    "num_steps = 26\n",
    "steps_per_document = 100\n",
    "valid_summary = 1\n",
    "train_doc_count = 100\n",
    "docs_per_step = 10\n",
    "\n",
    "\n",
    "beam_nodes = []\n",
    "\n",
    "beam_train_perplexity_ot = []\n",
    "beam_valid_perplexity_ot = []\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "print('Initialized')\n",
    "average_loss = 0\n",
    "\n",
    "# We use the first 10 documents that has \n",
    "# more than 10*steps_per_document bigrams for creating the validation dataset\n",
    "\n",
    "# Identify the first 10 documents following the above condition\n",
    "long_doc_ids = []\n",
    "for di in range(num_files):\n",
    "  if len(data_list[di])>10*steps_per_document:\n",
    "    long_doc_ids.append(di)\n",
    "  if len(long_doc_ids)==10:\n",
    "    break\n",
    "    \n",
    "# Generating validation data\n",
    "data_gens = []\n",
    "valid_gens = []\n",
    "for fi in range(num_files):\n",
    "  # Get all the bigrams if the document id is not in the validation document ids\n",
    "  if fi not in long_doc_ids:\n",
    "    data_gens.append(DataGeneratorOHE(data_list[fi],batch_size,num_unrollings))\n",
    "  # if the document is in the validation doc ids, only get up to the \n",
    "  # last steps_per_document bigrams and use the last steps_per_document bigrams as validation data\n",
    "  else:\n",
    "    data_gens.append(DataGeneratorOHE(data_list[fi][:-steps_per_document],batch_size,num_unrollings))\n",
    "    # Defining the validation data generator\n",
    "    valid_gens.append(DataGeneratorOHE(data_list[fi][-steps_per_document:],1,1))\n",
    "\n",
    "\n",
    "feed_dict = {}\n",
    "for step in range(num_steps):\n",
    "    \n",
    "    for di in np.random.permutation(train_doc_count)[:docs_per_step]:            \n",
    "        doc_perplexity = 0\n",
    "        for doc_step_id in range(steps_per_document):\n",
    "            \n",
    "            # Get a set of unrolled batches\n",
    "            u_data, u_labels = data_gens[di].unroll_batches()\n",
    "            \n",
    "            # Populate the feed dict by using each of the data batches\n",
    "            # present in the unrolled data\n",
    "            for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):            \n",
    "                feed_dict[train_inputs[ui]] = dat\n",
    "                feed_dict[train_labels[ui]] = lbl\n",
    "            \n",
    "            # Running the TensorFlow operations\n",
    "            _, l, step_perplexity = session.run([optimizer, loss, train_perplexity_without_exp], \n",
    "                                                       feed_dict=feed_dict)\n",
    "            # Update doc_perpelxity variable\n",
    "            doc_perplexity += step_perplexity \n",
    "            \n",
    "            # Update the average_loss variable\n",
    "            average_loss += step_perplexity \n",
    "            \n",
    "        # Show the printing progress <train_doc_id_1>.<train_doc_id_2>. ...\n",
    "        print('(%d).'%di,end='') \n",
    "    \n",
    "    # resetting hidden state after processing a single document\n",
    "    # It's still questionable if this adds value in terms of learning\n",
    "    # One one hand it's intuitive to reset the state when learning a new document\n",
    "    # On the other hand this approach creates a bias for the state to be zero\n",
    "    # We encourage the reader to investigate further the effect of resetting the state\n",
    "    #session.run(reset_train_state) # resetting hidden state for each document\n",
    "    print('')\n",
    "    \n",
    "    if (step+1) % valid_summary == 0:\n",
    "      \n",
    "      # Compute average loss\n",
    "      average_loss = average_loss / (docs_per_step*steps_per_document*valid_summary)\n",
    "      \n",
    "      # Print loss\n",
    "      print('Average loss at step %d: %f' % (step+1, average_loss))\n",
    "      print('\\tPerplexity at step %d: %f' %(step+1, np.exp(average_loss)))\n",
    "      beam_train_perplexity_ot.append(np.exp(average_loss))\n",
    "    \n",
    "      average_loss = 0 # reset loss\n",
    "        \n",
    "      valid_loss = 0 # reset loss\n",
    "        \n",
    "      # calculate valid perplexity\n",
    "      for v_doc_id in range(10):\n",
    "          # Remember we process things as bigrams\n",
    "          # So need to divide by 2\n",
    "          for v_step in range(steps_per_document//2):\n",
    "            uvalid_data,uvalid_labels = valid_gens[v_doc_id].unroll_batches()        \n",
    "\n",
    "            # Run validation phase related TensorFlow operations       \n",
    "            v_perp = session.run(\n",
    "                valid_perplexity_without_exp,\n",
    "                feed_dict = {valid_inputs:uvalid_data[0],valid_labels: uvalid_labels[0]}\n",
    "            )\n",
    "\n",
    "            valid_loss += v_perp\n",
    "            \n",
    "          session.run(reset_valid_state)\n",
    "      \n",
    "          # Reset validation data generator cursor\n",
    "          valid_gens[v_doc_id].reset_indices()      \n",
    "    \n",
    "      print()\n",
    "      v_perplexity = np.exp(valid_loss/(steps_per_document*10.0//2))\n",
    "      print(\"Valid Perplexity: %.2f\\n\"%v_perplexity)\n",
    "      beam_valid_perplexity_ot.append(v_perplexity)\n",
    "      \n",
    "      # Decay learning rate\n",
    "      decay_learning_rate(session, v_perplexity)\n",
    "    \n",
    "      # Generating new text ...\n",
    "      # We will be generating one segment having 500 bigrams\n",
    "      # Feel free to generate several segments by changing\n",
    "      # the value of segments_to_generate\n",
    "      print('Generated Text after epoch %d ... '%step)  \n",
    "      segments_to_generate = 1\n",
    "      chars_in_segment = 500//beam_length\n",
    "    \n",
    "      for _ in range(segments_to_generate):\n",
    "        print('======================== New text Segment ==========================')\n",
    "        # first word randomly generated\n",
    "        test_word = np.zeros((1,vocabulary_size),dtype=np.float32)\n",
    "        rand_doc = data_list[np.random.randint(0,num_files)]\n",
    "        test_word[0,rand_doc[np.random.randint(0,len(rand_doc))]] = 1.0\n",
    "        print(\"\",reverse_dictionary[np.argmax(test_word[0])],end='')\n",
    "        \n",
    "        for _ in range(chars_in_segment):\n",
    "            \n",
    "            test_sequence = get_beam_prediction(session)\n",
    "            print(test_sequence,end='')\n",
    "            \n",
    "        print(\"\")\n",
    "        session.run([reset_sample_beam_state])\n",
    "        \n",
    "        print('====================================================================')\n",
    "      print(\"\")\n",
    "\n",
    "session.close()\n",
    "    \n",
    "with open(filename_to_save, 'wt') as f:\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    writer.writerow(beam_train_perplexity_ot)\n",
    "    writer.writerow(beam_valid_perplexity_ot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
