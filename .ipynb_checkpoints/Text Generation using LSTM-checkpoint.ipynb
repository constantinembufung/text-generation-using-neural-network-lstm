{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import tensorflow as tf\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Stories\n",
    "Stories are automatically downloaded from https://www.cs.cmu.edu/~spok/grimmtmp/, if not detected in the disk. The total size of stories is around ~500KB. The dataset consists of 100 stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file:  stories/001.txt\n",
      "file  001.txt  already exits\n",
      "Downloading file:  stories/002.txt\n",
      "file  002.txt  already exits\n",
      "Downloading file:  stories/003.txt\n",
      "file  003.txt  already exits\n",
      "Downloading file:  stories/004.txt\n",
      "file  004.txt  already exits\n",
      "Downloading file:  stories/005.txt\n",
      "file  005.txt  already exits\n",
      "Downloading file:  stories/006.txt\n",
      "file  006.txt  already exits\n",
      "Downloading file:  stories/007.txt\n",
      "file  007.txt  already exits\n",
      "Downloading file:  stories/008.txt\n",
      "file  008.txt  already exits\n",
      "Downloading file:  stories/009.txt\n",
      "file  009.txt  already exits\n",
      "Downloading file:  stories/010.txt\n",
      "file  010.txt  already exits\n",
      "Downloading file:  stories/011.txt\n",
      "file  011.txt  already exits\n",
      "Downloading file:  stories/012.txt\n",
      "file  012.txt  already exits\n",
      "Downloading file:  stories/013.txt\n",
      "file  013.txt  already exits\n",
      "Downloading file:  stories/014.txt\n",
      "file  014.txt  already exits\n",
      "Downloading file:  stories/015.txt\n",
      "file  015.txt  already exits\n",
      "Downloading file:  stories/016.txt\n",
      "file  016.txt  already exits\n",
      "Downloading file:  stories/017.txt\n",
      "file  017.txt  already exits\n",
      "Downloading file:  stories/018.txt\n",
      "file  018.txt  already exits\n",
      "Downloading file:  stories/019.txt\n",
      "file  019.txt  already exits\n",
      "Downloading file:  stories/020.txt\n",
      "file  020.txt  already exits\n",
      "Downloading file:  stories/021.txt\n",
      "file  021.txt  already exits\n",
      "Downloading file:  stories/022.txt\n",
      "file  022.txt  already exits\n",
      "Downloading file:  stories/023.txt\n",
      "file  023.txt  already exits\n",
      "Downloading file:  stories/024.txt\n",
      "file  024.txt  already exits\n",
      "Downloading file:  stories/025.txt\n",
      "file  025.txt  already exits\n",
      "Downloading file:  stories/026.txt\n",
      "file  026.txt  already exits\n",
      "Downloading file:  stories/027.txt\n",
      "file  027.txt  already exits\n",
      "Downloading file:  stories/028.txt\n",
      "file  028.txt  already exits\n",
      "Downloading file:  stories/029.txt\n",
      "file  029.txt  already exits\n",
      "Downloading file:  stories/030.txt\n",
      "file  030.txt  already exits\n",
      "Downloading file:  stories/031.txt\n",
      "file  031.txt  already exits\n",
      "Downloading file:  stories/032.txt\n",
      "file  032.txt  already exits\n",
      "Downloading file:  stories/033.txt\n",
      "file  033.txt  already exits\n",
      "Downloading file:  stories/034.txt\n",
      "file  034.txt  already exits\n",
      "Downloading file:  stories/035.txt\n",
      "file  035.txt  already exits\n",
      "Downloading file:  stories/036.txt\n",
      "file  036.txt  already exits\n",
      "Downloading file:  stories/037.txt\n",
      "file  037.txt  already exits\n",
      "Downloading file:  stories/038.txt\n",
      "file  038.txt  already exits\n",
      "Downloading file:  stories/039.txt\n",
      "file  039.txt  already exits\n",
      "Downloading file:  stories/040.txt\n",
      "file  040.txt  already exits\n",
      "Downloading file:  stories/041.txt\n",
      "file  041.txt  already exits\n",
      "Downloading file:  stories/042.txt\n",
      "file  042.txt  already exits\n",
      "Downloading file:  stories/043.txt\n",
      "file  043.txt  already exits\n",
      "Downloading file:  stories/044.txt\n",
      "file  044.txt  already exits\n",
      "Downloading file:  stories/045.txt\n",
      "file  045.txt  already exits\n",
      "Downloading file:  stories/046.txt\n",
      "file  046.txt  already exits\n",
      "Downloading file:  stories/047.txt\n",
      "file  047.txt  already exits\n",
      "Downloading file:  stories/048.txt\n",
      "file  048.txt  already exits\n",
      "Downloading file:  stories/049.txt\n",
      "file  049.txt  already exits\n",
      "Downloading file:  stories/050.txt\n",
      "file  050.txt  already exits\n",
      "Downloading file:  stories/051.txt\n",
      "file  051.txt  already exits\n",
      "Downloading file:  stories/052.txt\n",
      "file  052.txt  already exits\n",
      "Downloading file:  stories/053.txt\n",
      "file  053.txt  already exits\n",
      "Downloading file:  stories/054.txt\n",
      "file  054.txt  already exits\n",
      "Downloading file:  stories/055.txt\n",
      "file  055.txt  already exits\n",
      "Downloading file:  stories/056.txt\n",
      "file  056.txt  already exits\n",
      "Downloading file:  stories/057.txt\n",
      "file  057.txt  already exits\n",
      "Downloading file:  stories/058.txt\n",
      "file  058.txt  already exits\n",
      "Downloading file:  stories/059.txt\n",
      "file  059.txt  already exits\n",
      "Downloading file:  stories/060.txt\n",
      "file  060.txt  already exits\n",
      "Downloading file:  stories/061.txt\n",
      "file  061.txt  already exits\n",
      "Downloading file:  stories/062.txt\n",
      "file  062.txt  already exits\n",
      "Downloading file:  stories/063.txt\n",
      "file  063.txt  already exits\n",
      "Downloading file:  stories/064.txt\n",
      "file  064.txt  already exits\n",
      "Downloading file:  stories/065.txt\n",
      "file  065.txt  already exits\n",
      "Downloading file:  stories/066.txt\n",
      "file  066.txt  already exits\n",
      "Downloading file:  stories/067.txt\n",
      "file  067.txt  already exits\n",
      "Downloading file:  stories/068.txt\n",
      "file  068.txt  already exits\n",
      "Downloading file:  stories/069.txt\n",
      "file  069.txt  already exits\n",
      "Downloading file:  stories/070.txt\n",
      "file  070.txt  already exits\n",
      "Downloading file:  stories/071.txt\n",
      "file  071.txt  already exits\n",
      "Downloading file:  stories/072.txt\n",
      "file  072.txt  already exits\n",
      "Downloading file:  stories/073.txt\n",
      "file  073.txt  already exits\n",
      "Downloading file:  stories/074.txt\n",
      "file  074.txt  already exits\n",
      "Downloading file:  stories/075.txt\n",
      "file  075.txt  already exits\n",
      "Downloading file:  stories/076.txt\n",
      "file  076.txt  already exits\n",
      "Downloading file:  stories/077.txt\n",
      "file  077.txt  already exits\n",
      "Downloading file:  stories/078.txt\n",
      "file  078.txt  already exits\n",
      "Downloading file:  stories/079.txt\n",
      "file  079.txt  already exits\n",
      "Downloading file:  stories/080.txt\n",
      "file  080.txt  already exits\n",
      "Downloading file:  stories/081.txt\n",
      "file  081.txt  already exits\n",
      "Downloading file:  stories/082.txt\n",
      "file  082.txt  already exits\n",
      "Downloading file:  stories/083.txt\n",
      "file  083.txt  already exits\n",
      "Downloading file:  stories/084.txt\n",
      "file  084.txt  already exits\n",
      "Downloading file:  stories/085.txt\n",
      "file  085.txt  already exits\n",
      "Downloading file:  stories/086.txt\n",
      "file  086.txt  already exits\n",
      "Downloading file:  stories/087.txt\n",
      "file  087.txt  already exits\n",
      "Downloading file:  stories/088.txt\n",
      "file  088.txt  already exits\n",
      "Downloading file:  stories/089.txt\n",
      "file  089.txt  already exits\n",
      "Downloading file:  stories/090.txt\n",
      "file  090.txt  already exits\n",
      "Downloading file:  stories/091.txt\n",
      "file  091.txt  already exits\n",
      "Downloading file:  stories/092.txt\n",
      "file  092.txt  already exits\n",
      "Downloading file:  stories/093.txt\n",
      "file  093.txt  already exits\n",
      "Downloading file:  stories/094.txt\n",
      "file  094.txt  already exits\n",
      "Downloading file:  stories/095.txt\n",
      "file  095.txt  already exits\n",
      "Downloading file:  stories/096.txt\n",
      "file  096.txt  already exits\n",
      "Downloading file:  stories/097.txt\n",
      "file  097.txt  already exits\n",
      "Downloading file:  stories/098.txt\n",
      "file  098.txt  already exits\n",
      "Downloading file:  stories/099.txt\n",
      "file  099.txt  already exits\n",
      "Downloading file:  stories/100.txt\n",
      "file  100.txt  already exits\n",
      "Downloading file:  stories/101.txt\n",
      "file  101.txt  already exits\n",
      "Downloading file:  stories/102.txt\n",
      "file  102.txt  already exits\n",
      "Downloading file:  stories/103.txt\n",
      "file  103.txt  already exits\n",
      "Downloading file:  stories/104.txt\n",
      "file  104.txt  already exits\n",
      "Downloading file:  stories/105.txt\n",
      "file  105.txt  already exits\n",
      "Downloading file:  stories/106.txt\n",
      "file  106.txt  already exits\n",
      "Downloading file:  stories/107.txt\n",
      "file  107.txt  already exits\n",
      "Downloading file:  stories/108.txt\n",
      "file  108.txt  already exits\n",
      "Downloading file:  stories/109.txt\n",
      "file  109.txt  already exits\n",
      "Downloading file:  stories/110.txt\n",
      "file  110.txt  already exits\n",
      "Downloading file:  stories/111.txt\n",
      "file  111.txt  already exits\n",
      "Downloading file:  stories/112.txt\n",
      "file  112.txt  already exits\n",
      "Downloading file:  stories/113.txt\n",
      "file  113.txt  already exits\n",
      "Downloading file:  stories/114.txt\n",
      "file  114.txt  already exits\n",
      "Downloading file:  stories/115.txt\n",
      "file  115.txt  already exits\n",
      "Downloading file:  stories/116.txt\n",
      "file  116.txt  already exits\n",
      "Downloading file:  stories/117.txt\n",
      "file  117.txt  already exits\n",
      "Downloading file:  stories/118.txt\n",
      "file  118.txt  already exits\n",
      "Downloading file:  stories/119.txt\n",
      "file  119.txt  already exits\n",
      "Downloading file:  stories/120.txt\n",
      "file  120.txt  already exits\n",
      "Downloading file:  stories/121.txt\n",
      "file  121.txt  already exits\n",
      "Downloading file:  stories/122.txt\n",
      "file  122.txt  already exits\n",
      "Downloading file:  stories/123.txt\n",
      "file  123.txt  already exits\n",
      "Downloading file:  stories/124.txt\n",
      "file  124.txt  already exits\n",
      "Downloading file:  stories/125.txt\n",
      "file  125.txt  already exits\n",
      "Downloading file:  stories/126.txt\n",
      "file  126.txt  already exits\n",
      "Downloading file:  stories/127.txt\n",
      "file  127.txt  already exits\n",
      "Downloading file:  stories/128.txt\n",
      "file  128.txt  already exits\n",
      "Downloading file:  stories/129.txt\n",
      "file  129.txt  already exits\n",
      "Downloading file:  stories/130.txt\n",
      "file  130.txt  already exits\n",
      "Downloading file:  stories/131.txt\n",
      "file  131.txt  already exits\n",
      "Downloading file:  stories/132.txt\n",
      "file  132.txt  already exits\n",
      "Downloading file:  stories/133.txt\n",
      "file  133.txt  already exits\n",
      "Downloading file:  stories/134.txt\n",
      "file  134.txt  already exits\n",
      "Downloading file:  stories/135.txt\n",
      "file  135.txt  already exits\n",
      "Downloading file:  stories/136.txt\n",
      "file  136.txt  already exits\n",
      "Downloading file:  stories/137.txt\n",
      "file  137.txt  already exits\n",
      "Downloading file:  stories/138.txt\n",
      "file  138.txt  already exits\n",
      "Downloading file:  stories/139.txt\n",
      "file  139.txt  already exits\n",
      "Downloading file:  stories/140.txt\n",
      "file  140.txt  already exits\n",
      "Downloading file:  stories/141.txt\n",
      "file  141.txt  already exits\n",
      "Downloading file:  stories/142.txt\n",
      "file  142.txt  already exits\n",
      "Downloading file:  stories/143.txt\n",
      "file  143.txt  already exits\n",
      "Downloading file:  stories/144.txt\n",
      "file  144.txt  already exits\n",
      "Downloading file:  stories/145.txt\n",
      "file  145.txt  already exits\n",
      "Downloading file:  stories/146.txt\n",
      "file  146.txt  already exits\n",
      "Downloading file:  stories/147.txt\n",
      "file  147.txt  already exits\n",
      "Downloading file:  stories/148.txt\n",
      "file  148.txt  already exits\n",
      "Downloading file:  stories/149.txt\n",
      "file  149.txt  already exits\n",
      "Downloading file:  stories/150.txt\n",
      "file  150.txt  already exits\n",
      "Downloading file:  stories/151.txt\n",
      "file  151.txt  already exits\n",
      "Downloading file:  stories/152.txt\n",
      "file  152.txt  already exits\n",
      "Downloading file:  stories/153.txt\n",
      "file  153.txt  already exits\n",
      "Downloading file:  stories/154.txt\n",
      "file  154.txt  already exits\n",
      "Downloading file:  stories/155.txt\n",
      "file  155.txt  already exits\n",
      "Downloading file:  stories/156.txt\n",
      "file  156.txt  already exits\n",
      "Downloading file:  stories/157.txt\n",
      "file  157.txt  already exits\n",
      "Downloading file:  stories/158.txt\n",
      "file  158.txt  already exits\n",
      "Downloading file:  stories/159.txt\n",
      "file  159.txt  already exits\n",
      "Downloading file:  stories/160.txt\n",
      "file  160.txt  already exits\n",
      "Downloading file:  stories/161.txt\n",
      "file  161.txt  already exits\n",
      "Downloading file:  stories/162.txt\n",
      "file  162.txt  already exits\n",
      "Downloading file:  stories/163.txt\n",
      "file  163.txt  already exits\n",
      "Downloading file:  stories/164.txt\n",
      "file  164.txt  already exits\n",
      "Downloading file:  stories/165.txt\n",
      "file  165.txt  already exits\n",
      "Downloading file:  stories/166.txt\n",
      "file  166.txt  already exits\n",
      "Downloading file:  stories/167.txt\n",
      "file  167.txt  already exits\n",
      "Downloading file:  stories/168.txt\n",
      "file  168.txt  already exits\n",
      "Downloading file:  stories/169.txt\n",
      "file  169.txt  already exits\n",
      "Downloading file:  stories/170.txt\n",
      "file  170.txt  already exits\n",
      "Downloading file:  stories/171.txt\n",
      "file  171.txt  already exits\n",
      "Downloading file:  stories/172.txt\n",
      "file  172.txt  already exits\n",
      "Downloading file:  stories/173.txt\n",
      "file  173.txt  already exits\n",
      "Downloading file:  stories/174.txt\n",
      "file  174.txt  already exits\n",
      "Downloading file:  stories/175.txt\n",
      "file  175.txt  already exits\n",
      "Downloading file:  stories/176.txt\n",
      "file  176.txt  already exits\n",
      "Downloading file:  stories/177.txt\n",
      "file  177.txt  already exits\n",
      "Downloading file:  stories/178.txt\n",
      "file  178.txt  already exits\n",
      "Downloading file:  stories/179.txt\n",
      "file  179.txt  already exits\n",
      "Downloading file:  stories/180.txt\n",
      "file  180.txt  already exits\n",
      "Downloading file:  stories/181.txt\n",
      "file  181.txt  already exits\n",
      "Downloading file:  stories/182.txt\n",
      "file  182.txt  already exits\n",
      "Downloading file:  stories/183.txt\n",
      "file  183.txt  already exits\n",
      "Downloading file:  stories/184.txt\n",
      "file  184.txt  already exits\n",
      "Downloading file:  stories/185.txt\n",
      "file  185.txt  already exits\n",
      "Downloading file:  stories/186.txt\n",
      "file  186.txt  already exits\n",
      "Downloading file:  stories/187.txt\n",
      "file  187.txt  already exits\n",
      "Downloading file:  stories/188.txt\n",
      "file  188.txt  already exits\n",
      "Downloading file:  stories/189.txt\n",
      "file  189.txt  already exits\n",
      "Downloading file:  stories/190.txt\n",
      "file  190.txt  already exits\n",
      "Downloading file:  stories/191.txt\n",
      "file  191.txt  already exits\n",
      "Downloading file:  stories/192.txt\n",
      "file  192.txt  already exits\n",
      "Downloading file:  stories/193.txt\n",
      "file  193.txt  already exits\n",
      "Downloading file:  stories/194.txt\n",
      "file  194.txt  already exits\n",
      "Downloading file:  stories/195.txt\n",
      "file  195.txt  already exits\n",
      "Downloading file:  stories/196.txt\n",
      "file  196.txt  already exits\n",
      "Downloading file:  stories/197.txt\n",
      "file  197.txt  already exits\n",
      "Downloading file:  stories/198.txt\n",
      "file  198.txt  already exits\n",
      "Downloading file:  stories/199.txt\n",
      "file  199.txt  already exits\n",
      "Downloading file:  stories/200.txt\n",
      "file  200.txt  already exits\n",
      "Downloading file:  stories/201.txt\n",
      "file  201.txt  already exits\n",
      "Downloading file:  stories/202.txt\n",
      "file  202.txt  already exits\n",
      "Downloading file:  stories/203.txt\n",
      "file  203.txt  already exits\n",
      "Downloading file:  stories/204.txt\n",
      "file  204.txt  already exits\n",
      "Downloading file:  stories/205.txt\n",
      "file  205.txt  already exits\n",
      "Downloading file:  stories/206.txt\n",
      "file  206.txt  already exits\n",
      "Downloading file:  stories/207.txt\n",
      "file  207.txt  already exits\n",
      "Downloading file:  stories/208.txt\n",
      "file  208.txt  already exits\n",
      "Downloading file:  stories/209.txt\n",
      "file  209.txt  already exits\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.cs.cmu.edu/~spok/grimmtmp/'\n",
    "\n",
    "#create a directory if needed\n",
    "dir_name = 'stories'\n",
    "if not os.path.exists(dir_name):\n",
    "    os.mkdir(dir_name)\n",
    "    \n",
    "def download_data(filename):\n",
    "    \"\"\"Download a file if not present\"\"\"\n",
    "    print('Downloading file: ', dir_name+os.sep+filename)\n",
    "    \n",
    "    if not os.path.exists(dir_name + os.sep+filename):\n",
    "        filename, _ = urlretrieve(url + filename, dir_name+os.sep+filename)\n",
    "    else: \n",
    "        print('file ', filename, ' already exits')\n",
    "        \n",
    "    return filename\n",
    "\n",
    "num_files = 209 \n",
    "\n",
    "filenames = [format(i, '03d')+'.txt' for i in range(1, num_files+1)]\n",
    "\n",
    "for fn in filenames:\n",
    "    download_data(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209 files found.\n"
     ]
    }
   ],
   "source": [
    "## check if the files are downloaded \n",
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
    "    assert file_exists\n",
    "print('%d files found.' %len(filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data \n",
    "Data will be stored in a list of lists where each list represents document and a document is a list of words. we will then break the text into bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file stories/001.txt\n",
      "Data size (characters) (Document 0) 3667\n",
      "sample string (Documents 0) ['in', ' o', 'ld', 'en', ' t', 'im', 'es', ' w', 'he', 'n ', 'wi', 'sh', 'in', 'g ', 'st', 'il', 'l ', 'he', 'lp', 'ed', ' o', 'ne', ', ', 'th', 'er', 'e ', 'li', 've', 'd ', 'a ', 'ki', 'ng', '\\nw', 'ho', 'se', ' d', 'au', 'gh', 'te', 'rs', ' w', 'er', 'e ', 'al', 'l ', 'be', 'au', 'ti', 'fu', 'l,']\n",
      "\n",
      "Processing file stories/002.txt\n",
      "Data size (characters) (Document 1) 4928\n",
      "sample string (Documents 1) ['ha', 'rd', ' b', 'y ', 'a ', 'gr', 'ea', 't ', 'fo', 're', 'st', ' d', 'we', 'lt', ' a', ' w', 'oo', 'd-', 'cu', 'tt', 'er', ' w', 'it', 'h ', 'hi', 's ', 'wi', 'fe', ', ', 'wh', 'o ', 'ha', 'd ', 'an', '\\no', 'nl', 'y ', 'ch', 'il', 'd,', ' a', ' l', 'it', 'tl', 'e ', 'gi', 'rl', ' t', 'hr', 'ee']\n",
      "\n",
      "Processing file stories/003.txt\n",
      "Data size (characters) (Document 2) 9745\n",
      "sample string (Documents 2) ['a ', 'ce', 'rt', 'ai', 'n ', 'fa', 'th', 'er', ' h', 'ad', ' t', 'wo', ' s', 'on', 's,', ' t', 'he', ' e', 'ld', 'er', ' o', 'f ', 'wh', 'om', ' w', 'as', ' s', 'ma', 'rt', ' a', 'nd', '\\ns', 'en', 'si', 'bl', 'e,', ' a', 'nd', ' c', 'ou', 'ld', ' d', 'o ', 'ev', 'er', 'yt', 'hi', 'ng', ', ', 'bu']\n",
      "\n",
      "Processing file stories/004.txt\n",
      "Data size (characters) (Document 3) 2852\n",
      "sample string (Documents 3) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', 'n ', 'ol', 'd ', 'go', 'at', ' w', 'ho', ' h', 'ad', ' s', 'ev', 'en', ' l', 'it', 'tl', 'e ', 'ki', 'ds', ', ', 'an', 'd\\n', 'lo', 've', 'd ', 'th', 'em', ' w', 'it', 'h ', 'al', 'l ', 'th', 'e ', 'lo', 've', ' o']\n",
      "\n",
      "Processing file stories/005.txt\n",
      "Data size (characters) (Document 4) 8189\n",
      "sample string (Documents 4) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', 'n ', 'ol', 'd ', 'ki', 'ng', ' w', 'ho', ' w', 'as', ' i', 'll', ' a', 'nd', ' t', 'ho', 'ug', 'ht', ' t', 'o\\n', 'hi', 'ms', 'el', 'f ', \"'i\", ' a', 'm ', 'ly', 'in', 'g ', 'on', ' w', 'ha', 't ', 'mu', 'st', ' b']\n",
      "\n",
      "Processing file stories/006.txt\n",
      "Data size (characters) (Document 5) 4369\n",
      "sample string (Documents 5) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'ea', 'sa', 'nt', ' w', 'ho', ' h', 'ad', ' d', 'ri', 've', 'n ', 'hi', 's ', 'co', 'w ', 'to', ' t', 'he', ' f', 'ai', 'r,', ' a', 'nd', ' s', 'ol', 'd\\n', 'he', 'r ', 'fo', 'r ', 'se', 've', 'n ', 'ta', 'le', 'rs', '. ', ' o', 'n ', 'th', 'e ']\n",
      "\n",
      "Processing file stories/007.txt\n",
      "Data size (characters) (Document 6) 5216\n",
      "sample string (Documents 6) ['th', 'er', 'e ', 'we', 're', ' o', 'nc', 'e ', 'up', 'on', ' a', ' t', 'im', 'e ', 'a ', 'ki', 'ng', ' a', 'nd', ' a', ' q', 'ue', 'en', ' w', 'ho', ' l', 'iv', 'ed', '\\nh', 'ap', 'pi', 'ly', ' t', 'og', 'et', 'he', 'r ', 'an', 'd ', 'ha', 'd ', 'tw', 'el', 've', ' c', 'hi', 'ld', 're', 'n,', ' b']\n",
      "\n",
      "Processing file stories/008.txt\n",
      "Data size (characters) (Document 7) 6097\n",
      "sample string (Documents 7) ['li', 'tt', 'le', ' b', 'ro', 'th', 'er', ' t', 'oo', 'k ', 'hi', 's ', 'li', 'tt', 'le', ' s', 'is', 'te', 'r ', 'by', ' t', 'he', ' h', 'an', 'd ', 'an', 'd ', 'sa', 'id', ', ', 'si', 'nc', 'e\\n', 'ou', 'r ', 'mo', 'th', 'er', ' d', 'ie', 'd ', 'we', ' h', 'av', 'e ', 'ha', 'd ', 'no', ' h', 'ap']\n",
      "\n",
      "Processing file stories/009.txt\n",
      "Data size (characters) (Document 8) 3699\n",
      "sample string (Documents 8) ['th', 'er', 'e ', 'we', 're', ' o', 'nc', 'e ', 'a ', 'ma', 'n ', 'an', 'd ', 'a ', 'wo', 'ma', 'n ', 'wh', 'o ', 'ha', 'd ', 'lo', 'ng', ' i', 'n ', 'va', 'in', '\\nw', 'is', 'he', 'd ', 'fo', 'r ', 'a ', 'ch', 'il', 'd.', '  ', 'at', ' l', 'en', 'gt', 'h ', 'th', 'e ', 'wo', 'ma', 'n ', 'ho', 'pe']\n",
      "\n",
      "Processing file stories/010.txt\n",
      "Data size (characters) (Document 9) 5268\n",
      "sample string (Documents 9) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' m', 'an', ' w', 'ho', 'se', ' w', 'if', 'e ', 'di', 'ed', ', ', 'an', 'd ', 'a ', 'wo', 'ma', 'n ', 'wh', 'os', 'e ', 'hu', 'sb', 'an', 'd\\n', 'di', 'ed', ', ', 'an', 'd ', 'th', 'e ', 'ma', 'n ', 'ha', 'd ', 'a ', 'da', 'ug', 'ht', 'er', ', ', 'an']\n",
      "\n",
      "Processing file stories/011.txt\n",
      "Data size (characters) (Document 10) 2377\n",
      "sample string (Documents 10) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' g', 'ir', 'l ', 'wh', 'o ', 'wa', 's ', 'id', 'le', ' a', 'nd', ' w', 'ou', 'ld', ' n', 'ot', ' s', 'pi', 'n,', ' a', 'nd', '\\nl', 'et', ' h', 'er', ' m', 'ot', 'he', 'r ', 'sa', 'y ', 'wh', 'at', ' s', 'he', ' w', 'ou', 'ld', ', ', 'sh', 'e ', 'co']\n",
      "\n",
      "Processing file stories/012.txt\n",
      "Data size (characters) (Document 11) 7695\n",
      "sample string (Documents 11) ['ha', 'rd', ' b', 'y ', 'a ', 'gr', 'ea', 't ', 'fo', 're', 'st', ' d', 'we', 'lt', ' a', ' p', 'oo', 'r ', 'wo', 'od', '-c', 'ut', 'te', 'r ', 'wi', 'th', ' h', 'is', ' w', 'if', 'e\\n', 'an', 'd ', 'hi', 's ', 'tw', 'o ', 'ch', 'il', 'dr', 'en', '. ', ' t', 'he', ' b', 'oy', ' w', 'as', ' c', 'al']\n",
      "\n",
      "Processing file stories/013.txt\n",
      "Data size (characters) (Document 12) 3665\n",
      "sample string (Documents 12) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' o', 'n ', 'a ', 'ti', 'me', ' a', ' p', 'oo', 'r ', 'ma', 'n,', ' w', 'ho', ' c', 'ou', 'ld', ' n', 'o ', 'lo', 'ng', 'er', '\\ns', 'up', 'po', 'rt', ' h', 'is', ' o', 'nl', 'y ', 'so', 'n.', '  ', 'th', 'en', ' s', 'ai', 'd ', 'th', 'e ', 'so', 'n,', ' d']\n",
      "\n",
      "Processing file stories/014.txt\n",
      "Data size (characters) (Document 13) 4178\n",
      "sample string (Documents 13) ['a ', 'lo', 'ng', ' t', 'im', 'e ', 'ag', 'o ', 'th', 'er', 'e ', 'li', 've', 'd ', 'a ', 'ki', 'ng', ' w', 'ho', ' w', 'as', ' f', 'am', 'ed', ' f', 'or', ' h', 'is', ' w', 'is', 'do', 'm\\n', 'th', 'ro', 'ug', 'h ', 'al', 'l ', 'th', 'e ', 'la', 'nd', '. ', ' n', 'ot', 'hi', 'ng', ' w', 'as', ' h']\n",
      "\n",
      "Processing file stories/015.txt\n",
      "Data size (characters) (Document 14) 8674\n",
      "sample string (Documents 14) ['on', 'e ', 'su', 'mm', 'er', \"'s\", ' m', 'or', 'ni', 'ng', ' a', ' l', 'it', 'tl', 'e ', 'ta', 'il', 'or', ' w', 'as', ' s', 'it', 'ti', 'ng', ' o', 'n ', 'hi', 's ', 'ta', 'bl', 'e\\n', 'by', ' t', 'he', ' w', 'in', 'do', 'w,', ' h', 'e ', 'wa', 's ', 'in', ' g', 'oo', 'd ', 'sp', 'ir', 'it', 's,']\n",
      "\n",
      "Processing file stories/016.txt\n",
      "Data size (characters) (Document 15) 7018\n",
      "sample string (Documents 15) ['\\tc', 'in', 'de', 're', 'll', 'a\\n', 'th', 'e ', 'wi', 'fe', ' o', 'f ', 'a ', 'ri', 'ch', ' m', 'an', ' f', 'el', 'l ', 'si', 'ck', ', ', 'an', 'd ', 'as', ' s', 'he', ' f', 'el', 't ', 'th', 'at', ' h', 'er', ' e', 'nd', '\\nw', 'as', ' d', 'ra', 'wi', 'ng', ' n', 'ea', 'r,', ' s', 'he', ' c', 'al']\n",
      "\n",
      "Processing file stories/017.txt\n",
      "Data size (characters) (Document 16) 3039\n",
      "sample string (Documents 16) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' k', 'in', \"g'\", 's ', 'so', 'n ', 'wh', 'o ', 'wa', 's ', 'se', 'iz', 'ed', ' w', 'it', 'h ', 'a ', 'de', 'si', 're', ' t', 'o ', 'tr', 'av', 'el', '\\na', 'bo', 'ut', ' t', 'he', ' w', 'or', 'ld', ', ', 'an', 'd ', 'to', 'ok', ' n', 'o ', 'on', 'e ']\n",
      "\n",
      "Processing file stories/018.txt\n",
      "Data size (characters) (Document 17) 3020\n",
      "sample string (Documents 17) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' w', 'id', 'ow', ' w', 'ho', ' h', 'ad', ' t', 'wo', ' d', 'au', 'gh', 'te', 'rs', ' -', ' o', 'ne', ' o', 'f\\n', 'wh', 'om', ' w', 'as', ' p', 're', 'tt', 'y ', 'an', 'd ', 'in', 'du', 'st', 'ri', 'ou', 's,', ' w', 'hi', 'ls', 't ', 'th', 'e ', 'ot']\n",
      "\n",
      "Processing file stories/019.txt\n",
      "Data size (characters) (Document 18) 2465\n",
      "sample string (Documents 18) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' m', 'an', ' w', 'ho', ' h', 'ad', ' s', 'ev', 'en', ' s', 'on', 's,', ' a', 'nd', ' s', 'ti', 'll', ' h', 'e ', 'ha', 'd\\n', 'no', ' d', 'au', 'gh', 'te', 'r,', ' h', 'ow', 'ev', 'er', ' m', 'uc', 'h ', 'he', ' w', 'is', 'he', 'd ', 'fo', 'r ', 'on']\n",
      "\n",
      "Processing file stories/020.txt\n",
      "Data size (characters) (Document 19) 3703\n",
      "sample string (Documents 19) ['\\tl', 'it', 'tl', 'e ', 're', 'd-', 'ca', 'p\\n', '\\no', 'nc', 'e ', 'up', 'on', ' a', ' t', 'im', 'e ', 'th', 'er', 'e ', 'wa', 's ', 'a ', 'de', 'ar', ' l', 'it', 'tl', 'e ', 'gi', 'rl', ' w', 'ho', ' w', 'as', ' l', 'ov', 'ed', '\\nb', 'y ', 'ev', 'er', 'y ', 'on', 'e ', 'wh', 'o ', 'lo', 'ok', 'ed']\n",
      "\n",
      "Processing file stories/021.txt\n",
      "Data size (characters) (Document 20) 1924\n",
      "sample string (Documents 20) ['in', ' a', ' c', 'er', 'ta', 'in', ' c', 'ou', 'nt', 'ry', ' t', 'he', 're', ' w', 'as', ' o', 'nc', 'e ', 'gr', 'ea', 't ', 'la', 'me', 'nt', 'at', 'io', 'n ', 'ov', 'er', ' a', '\\nw', 'il', 'd ', 'bo', 'ar', ' t', 'ha', 't ', 'la', 'id', ' w', 'as', 'te', ' t', 'he', ' f', 'ar', 'me', \"r'\", 's ']\n",
      "\n",
      "Processing file stories/022.txt\n",
      "Data size (characters) (Document 21) 6561\n",
      "sample string (Documents 21) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'wo', 'ma', 'n ', 'wh', 'o ', 'ga', 've', ' b', 'ir', 'th', ' t', 'o ', 'a ', 'li', 'tt', 'le', ' s', 'on', ',\\n', 'an', 'd ', 'as', ' h', 'e ', 'ca', 'me', ' i', 'nt', 'o ', 'th', 'e ', 'wo', 'rl', 'd ', 'wi', 'th', ' a', ' c', 'au']\n",
      "\n",
      "Processing file stories/023.txt\n",
      "Data size (characters) (Document 22) 5956\n",
      "sample string (Documents 22) ['a ', 'ce', 'rt', 'ai', 'n ', 'mi', 'll', 'er', ' h', 'ad', ' l', 'it', 'tl', 'e ', 'by', ' l', 'it', 'tl', 'e ', 'fa', 'll', 'en', ' i', 'nt', 'o ', 'po', 've', 'rt', 'y,', ' a', 'nd', '\\nh', 'ad', ' n', 'ot', 'hi', 'ng', ' l', 'ef', 't ', 'bu', 't ', 'hi', 's ', 'mi', 'll', ' a', 'nd', ' a', ' l']\n",
      "\n",
      "Processing file stories/024.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size (characters) (Document 23) 2529\n",
      "sample string (Documents 23) ['th', 'e ', 'mo', 'th', 'er', ' o', 'f ', 'ha', 'ns', ' s', 'ai', 'd,', ' w', 'hi', 'th', 'er', ' a', 'wa', 'y,', ' h', 'an', 's.', '  ', 'ha', 'ns', ' a', 'ns', 'we', 're', 'd,', ' t', 'o\\n', 'gr', 'et', 'el', '. ', ' b', 'eh', 'av', 'e ', 'we', 'll', ', ', 'ha', 'ns', '. ', ' o', 'h,', ' i', \"'l\"]\n",
      "\n",
      "Processing file stories/025.txt\n",
      "Data size (characters) (Document 24) 2416\n",
      "sample string (Documents 24) ['an', ' a', 'ge', 'd ', 'co', 'un', 't ', 'on', 'ce', ' l', 'iv', 'ed', ' i', 'n ', 'sw', 'it', 'ze', 'rl', 'an', 'd,', ' w', 'ho', ' h', 'ad', ' a', 'n ', 'on', 'ly', ' s', 'on', ',\\n', 'bu', 't ', 'he', ' w', 'as', ' s', 'tu', 'pi', 'd,', ' a', 'nd', ' c', 'ou', 'ld', ' l', 'ea', 'rn', ' n', 'ot']\n",
      "\n",
      "Processing file stories/026.txt\n",
      "Data size (characters) (Document 25) 3369\n",
      "sample string (Documents 25) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' m', 'an', ' w', 'ho', ' h', 'ad', ' a', ' d', 'au', 'gh', 'te', 'r ', 'wh', 'o ', 'wa', 's ', 'ca', 'll', 'ed', ' c', 'le', 've', 'r\\n', 'el', 'si', 'e.', '  ', 'an', 'd ', 'wh', 'en', ' s', 'he', ' h', 'ad', ' g', 'ro', 'wn', ' u', 'p ', 'he', 'r ']\n",
      "\n",
      "Processing file stories/027.txt\n",
      "Data size (characters) (Document 26) 10013\n",
      "sample string (Documents 26) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' t', 'ai', 'lo', 'r ', 'wh', 'o ', 'ha', 'd ', 'th', 're', 'e ', 'so', 'ns', ', ', 'an', 'd\\n', 'on', 'ly', ' o', 'ne', ' g', 'oa', 't.', '  ', 'bu', 't ', 'as', ' t', 'he', ' g', 'oa', 't ', 'su', 'pp', 'or', 'te']\n",
      "\n",
      "Processing file stories/028.txt\n",
      "Data size (characters) (Document 27) 5788\n",
      "sample string (Documents 27) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'pe', 'as', 'an', 't ', 'wh', 'o ', 'sa', 't ', 'in', ' t', 'he', ' e', 've', 'ni', 'ng', ' b', 'y ', 'th', 'e\\n', 'he', 'ar', 'th', ' a', 'nd', ' p', 'ok', 'ed', ' t', 'he', ' f', 'ir', 'e,', ' a', 'nd', ' h', 'is', ' w', 'if', 'e ']\n",
      "\n",
      "Processing file stories/029.txt\n",
      "Data size (characters) (Document 28) 1335\n",
      "sample string (Documents 28) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'se', 'rv', 'an', 't-', 'gi', 'rl', ' w', 'ho', ' w', 'as', ' i', 'nd', 'us', 'tr', 'io', 'us', ' a', 'nd', ' c', 'le', 'an', 'ly', '\\na', 'nd', ' s', 'we', 'pt', ' t', 'he', ' h', 'ou', 'se', ' e', 've', 'ry', ' d', 'ay', ', ', 'an']\n",
      "\n",
      "Processing file stories/030.txt\n",
      "Data size (characters) (Document 29) 3591\n",
      "sample string (Documents 29) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' m', 'il', 'le', 'r,', ' w', 'ho', ' h', 'ad', ' a', ' b', 'ea', 'ut', 'if', 'ul', '\\nd', 'au', 'gh', 'te', 'r,', ' a', 'nd', ' a', 's ', 'sh', 'e ', 'wa', 's ', 'gr', 'ow', 'n ', 'up', ', ', 'he', ' w', 'is', 'he']\n",
      "\n",
      "Processing file stories/031.txt\n",
      "Data size (characters) (Document 30) 1624\n",
      "sample string (Documents 30) ['a ', 'po', 'or', ' m', 'an', ' h', 'ad', ' s', 'o ', 'ma', 'ny', ' c', 'hi', 'ld', 're', 'n ', 'th', 'at', ' h', 'e ', 'ha', 'd ', 'al', 're', 'ad', 'y ', 'as', 'ke', 'd\\n', 'ev', 'er', 'yo', 'ne', ' i', 'n ', 'th', 'e ', 'wo', 'rl', 'd ', 'to', ' b', 'e ', 'go', 'df', 'at', 'he', 'r,', ' a', 'nd']\n",
      "\n",
      "Processing file stories/032.txt\n",
      "Data size (characters) (Document 31) 758\n",
      "sample string (Documents 31) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' l', 'it', 'tl', 'e ', 'gi', 'rl', ' w', 'ho', ' w', 'as', ' o', 'bs', 'ti', 'na', 'te', ' a', 'nd', ' i', 'nq', 'ui', 'si', 'ti', 've', ',\\n', 'an', 'd ', 'wh', 'en', ' h', 'er', ' p', 'ar', 'en', 'ts', ' t', 'ol', 'd ', 'he', 'r ', 'to', ' d', 'o ']\n",
      "\n",
      "Processing file stories/033.txt\n",
      "Data size (characters) (Document 32) 3121\n",
      "sample string (Documents 32) ['a ', 'po', 'or', ' m', 'an', ' h', 'ad', ' t', 'we', 'lv', 'e ', 'ch', 'il', 'dr', 'en', ' a', 'nd', ' w', 'as', ' f', 'or', 'ce', 'd ', 'to', ' w', 'or', 'k ', 'ni', 'gh', 't ', 'an', 'd\\n', 'da', 'y ', 'to', ' g', 'iv', 'e ', 'th', 'em', ' e', 've', 'n ', 'br', 'ea', 'd.', '  ', 'wh', 'en', ' t']\n",
      "\n",
      "Processing file stories/034.txt\n",
      "Data size (characters) (Document 33) 4192\n",
      "sample string (Documents 33) ['a ', 'ce', 'rt', 'ai', 'n ', 'ta', 'il', 'or', ' h', 'ad', ' a', ' s', 'on', ', ', 'wh', 'o ', 'ha', 'pp', 'en', 'ed', ' t', 'o ', 'be', ' s', 'ma', 'll', ', ', 'an', 'd\\n', 'no', ' b', 'ig', 'ge', 'r ', 'th', 'an', ' a', ' t', 'hu', 'mb', ', ', 'an', 'd ', 'on', ' t', 'hi', 's ', 'ac', 'co', 'un']\n",
      "\n",
      "Processing file stories/035.txt\n",
      "Data size (characters) (Document 34) 3650\n",
      "sample string (Documents 34) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' w', 'iz', 'ar', 'd ', 'wh', 'o ', 'us', 'ed', ' t', 'o ', 'ta', 'ke', ' t', 'he', ' f', 'or', 'm ', 'of', ' a', ' p', 'oo', 'r\\n', 'ma', 'n,', ' a', 'nd', ' w', 'en', 't ', 'to', ' h', 'ou', 'se', 's ', 'an', 'd ', 'be', 'gg', 'ed', ', ', 'an', 'd ']\n",
      "\n",
      "Processing file stories/036.txt\n",
      "Data size (characters) (Document 35) 8219\n",
      "sample string (Documents 35) ['it', ' i', 's ', 'no', 'w ', 'lo', 'ng', ' a', 'go', ', ', 'qu', 'it', 'e ', 'tw', 'o ', 'th', 'ou', 'sa', 'nd', ' y', 'ea', 'rs', ', ', 'si', 'nc', 'e ', 'th', 'er', 'e ', 'wa', 's\\n', 'a ', 'ri', 'ch', ' m', 'an', ' w', 'ho', ' h', 'ad', ' a', ' b', 'ea', 'ut', 'if', 'ul', ' a', 'nd', ' p', 'io']\n",
      "\n",
      "Processing file stories/037.txt\n",
      "Data size (characters) (Document 36) 2151\n",
      "sample string (Documents 36) ['a ', 'fa', 'rm', 'er', ' o', 'nc', 'e ', 'ha', 'd ', 'a ', 'fa', 'it', 'hf', 'ul', ' d', 'og', ' c', 'al', 'le', 'd ', 'su', 'lt', 'an', ', ', 'wh', 'o ', 'ha', 'd ', 'gr', 'ow', 'n\\n', 'ol', 'd,', ' a', 'nd', ' l', 'os', 't ', 'al', 'l ', 'hi', 's ', 'te', 'et', 'h,', ' s', 'o ', 'th', 'at', ' h']\n",
      "\n",
      "Processing file stories/038.txt\n",
      "Data size (characters) (Document 37) 5129\n",
      "sample string (Documents 37) ['on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ', ', 'a ', 'ce', 'rt', 'ai', 'n ', 'ki', 'ng', ' w', 'as', ' h', 'un', 'ti', 'ng', ' i', 'n ', 'a ', 'gr', 'ea', 't ', 'fo', 're', 'st', ',\\n', 'an', 'd ', 'he', ' c', 'ha', 'se', 'd ', 'a ', 'wi', 'ld', ' b', 'ea', 'st', ' s', 'o ', 'ea', 'ge', 'rl']\n",
      "\n",
      "Processing file stories/039.txt\n",
      "Data size (characters) (Document 38) 3472\n",
      "sample string (Documents 38) ['\\tb', 'ri', 'ar', '-r', 'os', 'e\\n', '\\na', ' l', 'on', 'g ', 'ti', 'me', ' a', 'go', ' t', 'he', 're', ' w', 'er', 'e ', 'a ', 'ki', 'ng', ' a', 'nd', ' q', 'ue', 'en', ' w', 'ho', ' s', 'ai', 'd ', 'ev', 'er', 'y\\n', 'da', 'y,', ' a', 'h,', ' i', 'f ', 'on', 'ly', ' w', 'e ', 'ha', 'd ', 'a ', 'ch']\n",
      "\n",
      "Processing file stories/040.txt\n",
      "Data size (characters) (Document 39) 2490\n",
      "sample string (Documents 39) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' f', 'or', 'es', 'te', 'r ', 'wh', 'o ', 'we', 'nt', ' i', 'nt', 'o ', 'th', 'e ', 'fo', 're', 'st', ' t', 'o ', 'hu', 'nt', ',\\n', 'an', 'd ', 'as', ' h', 'e ', 'en', 'te', 're', 'd ', 'it', ' h', 'e ', 'he', 'ar', 'd ', 'a ', 'so', 'un', 'd ', 'of']\n",
      "\n",
      "Processing file stories/041.txt\n",
      "Data size (characters) (Document 40) 4273\n",
      "sample string (Documents 40) ['a ', 'ki', 'ng', ' h', 'ad', ' a', ' d', 'au', 'gh', 'te', 'r ', 'wh', 'o ', 'wa', 's ', 'be', 'au', 'ti', 'fu', 'l ', 'be', 'yo', 'nd', ' a', 'll', ' m', 'ea', 'su', 're', ',\\n', 'bu', 't ', 'so', ' p', 'ro', 'ud', ' a', 'nd', ' h', 'au', 'gh', 'ty', ' w', 'it', 'ha', 'l ', 'th', 'at', ' n', 'o ']\n",
      "\n",
      "Processing file stories/042.txt\n",
      "Data size (characters) (Document 41) 8327\n",
      "sample string (Documents 41) ['\\ts', 'no', 'w ', 'wh', 'it', 'e ', 'an', 'd ', 'th', 'e ', 'se', 've', 'n ', 'dw', 'ar', 'fs', '\\n\\n', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' i', 'n ', 'th', 'e ', 'mi', 'dd', 'le', ' o', 'f ', 'wi', 'nt', 'er', ', ', 'wh', 'en', ' t', 'he', ' f', 'la', 'ke', 's ', 'of', '\\ns', 'no', 'w ']\n",
      "\n",
      "Processing file stories/043.txt\n",
      "Data size (characters) (Document 42) 6128\n",
      "sample string (Documents 42) ['th', 'er', 'e ', 'we', 're', ' o', 'nc', 'e ', 'th', 're', 'e ', 'br', 'ot', 'he', 'rs', ' w', 'ho', ' h', 'ad', ' f', 'al', 'le', 'n ', 'de', 'ep', 'er', ' a', 'nd', ' d', 'ee', 'pe', 'r ', 'in', 'to', '\\np', 'ov', 'er', 'ty', ', ', 'an', 'd ', 'at', ' l', 'as', 't ', 'th', 'ei', 'r ', 'ne', 'ed']\n",
      "\n",
      "Processing file stories/044.txt\n",
      "Data size (characters) (Document 43) 2819\n",
      "sample string (Documents 43) ['\\tr', 'um', 'pe', 'ls', 'ti', 'lt', 'sk', 'in', '\\n\\n', 'on', 'ce', ' t', 'he', 're', ' w', 'as', ' a', ' m', 'il', 'le', 'r ', 'wh', 'o ', 'wa', 's ', 'po', 'or', ', ', 'bu', 't ', 'wh', 'o ', 'ha', 'd ', 'a ', 'be', 'au', 'ti', 'fu', 'l\\n', 'da', 'ug', 'ht', 'er', '. ', ' n', 'ow', ' i', 't ', 'ha']\n",
      "\n",
      "Processing file stories/045.txt\n",
      "Data size (characters) (Document 44) 3822\n",
      "sample string (Documents 44) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' w', 'om', 'an', ' w', 'ho', ' w', 'as', ' a', ' r', 'ea', 'l ', 'wi', 'tc', 'h ', 'an', 'd ', 'ha', 'd ', 'tw', 'o\\n', 'da', 'ug', 'ht', 'er', 's,', ' o', 'ne', ' u', 'gl', 'y ', 'an', 'd ', 'wi', 'ck', 'ed', ', ']\n",
      "\n",
      "Processing file stories/046.txt\n",
      "Data size (characters) (Document 45) 7772\n",
      "sample string (Documents 45) ['in', ' o', 'ld', 'en', ' t', 'im', 'es', ' t', 'he', 're', ' w', 'as', ' a', ' k', 'in', 'g,', ' w', 'ho', ' h', 'ad', ' b', 'eh', 'in', 'd ', 'hi', 's ', 'pa', 'la', 'ce', ' a', '\\nb', 'ea', 'ut', 'if', 'ul', ' p', 'le', 'as', 'ur', 'e-', 'ga', 'rd', 'en', ' i', 'n ', 'wh', 'ic', 'h ', 'th', 'er']\n",
      "\n",
      "Processing file stories/047.txt\n",
      "Data size (characters) (Document 46) 22158\n",
      "sample string (Documents 46) ['th', 'er', 'e ', 'we', 're', ' o', 'nc', 'e ', 'up', 'on', ' a', ' t', 'im', 'e ', 'tw', 'o ', 'br', 'ot', 'he', 'rs', ', ', 'on', 'e ', 'ri', 'ch', ' a', 'nd', ' t', 'he', ' o', 'th', 'er', '\\np', 'oo', 'r.', '  ', 'th', 'e ', 'ri', 'ch', ' o', 'ne', ' w', 'as', ' a', ' g', 'ol', 'ds', 'mi', 'th']\n",
      "\n",
      "Processing file stories/048.txt\n",
      "Data size (characters) (Document 47) 2169\n",
      "sample string (Documents 47) ['tw', 'o ', 'ki', 'ng', \"s'\", ' s', 'on', 's ', 'on', 'ce', ' w', 'en', 't ', 'ou', 't ', 'in', ' s', 'ea', 'rc', 'h ', 'of', ' a', 'dv', 'en', 'tu', 're', 's,', ' a', 'nd', ' f', 'el', 'l ', 'in', 'to', '\\na', ' w', 'il', 'd,', ' d', 'is', 'or', 'de', 'rl', 'y ', 'wa', 'y ', 'of', ' l', 'iv', 'in']\n",
      "\n",
      "Processing file stories/049.txt\n",
      "Data size (characters) (Document 48) 2822\n",
      "sample string (Documents 48) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' k', 'in', 'g ', 'wh', 'o ', 'ha', 'd ', 'th', 're', 'e ', 'so', 'ns', ', ', 'of', ' w', 'ho', 'm ', 'tw', 'o\\n', 'we', 're', ' c', 'le', 've', 'r ', 'an', 'd ', 'wi', 'se', ', ', 'bu', 't ', 'th', 'e ', 'th', 'ir']\n",
      "\n",
      "Processing file stories/050.txt\n",
      "Data size (characters) (Document 49) 4034\n",
      "sample string (Documents 49) ['th', 'er', 'e ', 'wa', 's ', 'a ', 'ma', 'n ', 'wh', 'o ', 'ha', 'd ', 'th', 're', 'e ', 'so', 'ns', ', ', 'th', 'e ', 'yo', 'un', 'ge', 'st', ' o', 'f ', 'wh', 'om', ' w', 'as', ' c', 'al', 'le', 'd\\n', 'du', 'mm', 'li', 'ng', ', ', 'an', 'd ', 'wa', 's ', 'de', 'sp', 'is', 'ed', ', ', 'mo', 'ck']\n",
      "\n",
      "Processing file stories/051.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size (characters) (Document 50) 5608\n",
      "sample string (Documents 50) ['\\ta', 'll', 'er', 'le', 'ir', 'au', 'h\\n', '\\nt', 'he', 're', ' w', 'as', ' o', 'nc', 'e ', 'up', 'on', ' a', ' t', 'im', 'e ', 'a ', 'ki', 'ng', ' w', 'ho', ' h', 'ad', ' a', ' w', 'if', 'e ', 'wi', 'th', ' g', 'ol', 'de', 'n ', 'ha', 'ir', ',\\n', 'an', 'd ', 'sh', 'e ', 'wa', 's ', 'so', ' b', 'ea']\n",
      "\n",
      "Processing file stories/052.txt\n",
      "Data size (characters) (Document 51) 1287\n",
      "sample string (Documents 51) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' w', 'om', 'an', ' a', 'nd', ' h', 'er', ' d', 'au', 'gh', 'te', 'r ', 'wh', 'o ', 'li', 've', 'd ', 'in', ' a', '\\np', 're', 'tt', 'y ', 'ga', 'rd', 'en', ' w', 'it', 'h ', 'ca', 'bb', 'ag', 'es', '. ', ' a', 'nd', ' a', ' l', 'it', 'tl', 'e ', 'ha']\n",
      "\n",
      "Processing file stories/053.txt\n",
      "Data size (characters) (Document 52) 2841\n",
      "sample string (Documents 52) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' k', 'in', \"g'\", 's ', 'so', 'n ', 'wh', 'o ', 'ha', 'd ', 'a ', 'br', 'id', 'e ', 'wh', 'om', ' h', 'e ', 'lo', 've', 'd ', 've', 'ry', ' m', 'uc', 'h.', '\\na', 'nd', ' w', 'he', 'n ', 'he', ' w', 'as', ' s', 'it', 'ti', 'ng', ' b', 'es', 'id', 'e ']\n",
      "\n",
      "Processing file stories/054.txt\n",
      "Data size (characters) (Document 53) 1922\n",
      "sample string (Documents 53) ['ha', 'ns', ' w', 'is', 'he', 'd ', 'to', ' p', 'ut', ' h', 'is', ' s', 'on', ' t', 'o ', 'le', 'ar', 'n ', 'a ', 'tr', 'ad', 'e,', ' s', 'o ', 'he', ' w', 'en', 't ', 'in', 'to', ' t', 'he', '\\nc', 'hu', 'rc', 'h ', 'an', 'd ', 'pr', 'ay', 'ed', ' t', 'o ', 'ou', 'r ', 'lo', 'rd', ' g', 'od', ' t']\n",
      "\n",
      "Processing file stories/055.txt\n",
      "Data size (characters) (Document 54) 2573\n",
      "sample string (Documents 54) ['a ', 'fa', 'th', 'er', ' o', 'nc', 'e ', 'ca', 'll', 'ed', ' h', 'is', ' t', 'hr', 'ee', ' s', 'on', 's ', 'be', 'fo', 're', ' h', 'im', ', ', 'an', 'd ', 'he', ' g', 'av', 'e ', 'to', ' t', 'he', '\\nf', 'ir', 'st', ' a', ' c', 'oc', 'k,', ' t', 'o ', 'th', 'e ', 'se', 'co', 'nd', ' a', ' s', 'cy']\n",
      "\n",
      "Processing file stories/056.txt\n",
      "Data size (characters) (Document 55) 5285\n",
      "sample string (Documents 55) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' m', 'an', ' w', 'ho', ' u', 'nd', 'er', 'st', 'oo', 'd ', 'al', 'l ', 'ki', 'nd', 's ', 'of', ' a', 'rt', 's.', '  ', 'he', ' s', 'er', 've', 'd ', 'in', '\\nw', 'ar', ', ', 'an', 'd ', 'be', 'ha', 've', 'd ', 'we', 'll', ' a', 'nd', ' b', 'ra', 've']\n",
      "\n",
      "Processing file stories/057.txt\n",
      "Data size (characters) (Document 56) 971\n",
      "sample string (Documents 56) ['th', 'e ', 'sh', 'e-', 'wo', 'lf', ' b', 'ro', 'ug', 'ht', ' i', 'nt', 'o ', 'th', 'e ', 'wo', 'rl', 'd ', 'a ', 'yo', 'un', 'g ', 'on', 'e,', ' a', 'nd', ' i', 'nv', 'it', 'ed', ' t', 'he', ' f', 'ox', '\\nt', 'o ', 'be', ' g', 'od', 'fa', 'th', 'er', '. ', ' a', 'ft', 'er', ' a', 'll', ', ', 'he']\n",
      "\n",
      "Processing file stories/058.txt\n",
      "Data size (characters) (Document 57) 4538\n",
      "sample string (Documents 57) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' q', 'ue', 'en', ' t', 'o ', 'wh', 'om', ' g', 'od', ' h', 'ad', ' g', 'iv', 'en', ' n', 'o ', 'ch', 'il', 'dr', 'en', '.\\n', 'ev', 'er', 'y ', 'mo', 'rn', 'in', 'g ', 'sh', 'e ', 'we', 'nt', ' i', 'nt', 'o ', 'th']\n",
      "\n",
      "Processing file stories/059.txt\n",
      "Data size (characters) (Document 58) 636\n",
      "sample string (Documents 58) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' v', 'er', 'y ', 'ol', 'd ', 'ma', 'n,', ' w', 'ho', 'se', ' e', 'ye', 's ', 'ha', 'd ', 'be', 'co', 'me', ' d', 'im', ', ', 'hi', 's ', 'ea', 'rs', '\\nd', 'ul', 'l ', 'of', ' h', 'ea', 'ri', 'ng', ', ', 'hi', 's ', 'kn', 'ee', 's ', 'tr', 'em', 'bl']\n",
      "\n",
      "Processing file stories/060.txt\n",
      "Data size (characters) (Document 59) 786\n",
      "sample string (Documents 59) ['a ', 'li', 'tt', 'le', ' b', 'ro', 'th', 'er', ' a', 'nd', ' s', 'is', 'te', 'r ', 'we', 're', ' o', 'nc', 'e ', 'pl', 'ay', 'in', 'g ', 'by', ' a', ' w', 'el', 'l,', ' a', 'nd', ' w', 'hi', 'le', '\\nt', 'he', 'y ', 'we', 're', ' t', 'hu', 's ', 'pl', 'ay', 'in', 'g,', ' t', 'he', 'y ', 'bo', 'th']\n",
      "\n",
      "Processing file stories/061.txt\n",
      "Data size (characters) (Document 60) 10687\n",
      "sample string (Documents 60) ['th', 'er', 'e ', 'wa', 's ', 'on', 'e ', 'up', 'on', ' a', ' t', 'im', 'e ', 'a ', 'gr', 'ea', 't ', 'wa', 'r,', ' a', 'nd', ' w', 'he', 'n ', 'it', ' c', 'am', 'e ', 'to', ' a', 'n ', 'en', 'd,', '\\nm', 'an', 'y ', 'so', 'ld', 'ie', 'rs', ' w', 'er', 'e ', 'di', 'sc', 'ha', 'rg', 'ed', '. ', ' t']\n",
      "\n",
      "Processing file stories/062.txt\n",
      "Data size (characters) (Document 61) 5105\n",
      "sample string (Documents 61) ['ha', 'ns', ' h', 'ad', ' s', 'er', 've', 'd ', 'hi', 's ', 'ma', 'st', 'er', ' f', 'or', ' s', 'ev', 'en', ' y', 'ea', 'rs', ', ', 'so', ' h', 'e ', 'sa', 'id', ' t', 'o ', 'hi', 'm,', '\\nm', 'as', 'te', 'r,', ' m', 'y ', 'ti', 'me', ' i', 's ', 'up', ', ', 'no', 'w ', 'i ', 'sh', 'ou', 'ld', ' b']\n",
      "\n",
      "Processing file stories/063.txt\n",
      "Data size (characters) (Document 62) 1127\n",
      "sample string (Documents 62) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' y', 'ou', 'ng', ' p', 'ea', 'sa', 'nt', ' n', 'am', 'ed', ' h', 'an', 's,', ' w', 'ho', 'se', ' u', 'nc', 'le', '\\nw', 'an', 'te', 'd ', 'to', ' f', 'in', 'd ', 'hi', 'm ', 'a ', 'ri', 'ch', ' w', 'if', 'e.', '  ']\n",
      "\n",
      "Processing file stories/064.txt\n",
      "Data size (characters) (Document 63) 4981\n",
      "sample string (Documents 63) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'ma', 'n ', 'an', 'd ', 'a ', 'po', 'or', ' w', 'om', 'an', ' w', 'ho', ' h', 'ad', ' n', 'ot', 'hi', 'ng', ' b', 'ut', ' a', '\\nl', 'it', 'tl', 'e ', 'co', 'tt', 'ag', 'e,', ' a', 'nd', ' w', 'ho', ' e', 'ar', 'ne', 'd ', 'th', 'ei']\n",
      "\n",
      "Processing file stories/065.txt\n",
      "Data size (characters) (Document 64) 6006\n",
      "sample string (Documents 64) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' m', 'an', ' w', 'ho', ' w', 'as', ' a', 'bo', 'ut', ' t', 'o ', 'se', 't ', 'ou', 't ', 'on', ' a', ' l', 'on', 'g\\n', 'jo', 'ur', 'ne', 'y,', ' a', 'nd', ' o', 'n ', 'pa', 'rt', 'in', 'g ', 'he', ' a', 'sk', 'ed']\n",
      "\n",
      "Processing file stories/066.txt\n",
      "Data size (characters) (Document 65) 5900\n",
      "sample string (Documents 65) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', 'n ', 'ol', 'd ', 'qu', 'ee', 'n ', 'wh', 'os', 'e ', 'hu', 'sb', 'an', 'd ', 'ha', 'd ', 'be', 'en', ' d', 'ea', 'd\\n', 'fo', 'r ', 'ma', 'ny', ' y', 'ea', 'rs', ', ', 'an', 'd ', 'sh', 'e ', 'ha', 'd ', 'a ', 'be']\n",
      "\n",
      "Processing file stories/067.txt\n",
      "Data size (characters) (Document 66) 7837\n",
      "sample string (Documents 66) ['on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' c', 'ou', 'nt', 'ry', 'ma', 'n ', 'ha', 'd ', 'a ', 'so', 'n ', 'wh', 'o ', 'wa', 's ', 'as', ' b', 'ig', ' a', 's ', 'a ', 'th', 'um', 'b,', '\\na', 'nd', ' d', 'id', ' n', 'ot', ' b', 'ec', 'om', 'e ', 'an', 'y ', 'bi', 'gg', 'er', ', ', 'an']\n",
      "\n",
      "Processing file stories/068.txt\n",
      "Data size (characters) (Document 67) 4717\n",
      "sample string (Documents 67) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' r', 'ic', 'h ', 'ki', 'ng', ' w', 'ho', ' h', 'ad', ' t', 'hr', 'ee', ' d', 'au', 'gh', 'te', 'rs', ', ', 'wh', 'o\\n', 'da', 'il', 'y ', 'we', 'nt', ' t', 'o ', 'wa', 'lk', ' i', 'n ', 'th', 'e ', 'pa', 'la', 'ce']\n",
      "\n",
      "Processing file stories/069.txt\n",
      "Data size (characters) (Document 68) 6233\n",
      "sample string (Documents 68) ['th', 'er', 'e ', 'wa', 's ', 'a ', 'ce', 'rt', 'ai', 'n ', 'me', 'rc', 'ha', 'nt', ' w', 'ho', ' h', 'ad', ' t', 'wo', ' c', 'hi', 'ld', 're', 'n,', ' a', ' b', 'oy', ' a', 'nd', ' a', ' g', 'ir', 'l,', '\\nt', 'he', 'y ', 'we', 're', ' b', 'ot', 'h ', 'yo', 'un', 'g,', ' a', 'nd', ' c', 'ou', 'ld']\n",
      "\n",
      "Processing file stories/070.txt\n",
      "Data size (characters) (Document 69) 5664\n",
      "sample string (Documents 69) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' q', 'ue', 'en', ' w', 'ho', ' h', 'ad', ' a', ' l', 'it', 'tl', 'e ', 'da', 'ug', 'ht', 'er', ' w', 'ho', '\\nw', 'as', ' s', 'ti', 'll', ' s', 'o ', 'yo', 'un', 'g ', 'th', 'at', ' s', 'he', ' h', 'ad', ' t', 'o ']\n",
      "\n",
      "Processing file stories/071.txt\n",
      "Data size (characters) (Document 70) 3569\n",
      "sample string (Documents 70) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'pe', 'as', 'an', 't ', 'wh', 'o ', 'ha', 'd ', 'no', ' l', 'an', 'd,', ' b', 'ut', ' o', 'nl', 'y ', 'a ', 'sm', 'al', 'l\\n', 'ho', 'us', 'e,', ' a', 'nd', ' o', 'ne', ' d', 'au', 'gh', 'te', 'r.', '  ', 'th', 'en', ' s', 'ai', 'd ']\n",
      "\n",
      "Processing file stories/072.txt\n",
      "Data size (characters) (Document 71) 3793\n",
      "sample string (Documents 71) ['ab', 'ou', 't ', 'a ', 'th', 'ou', 'sa', 'nd', ' o', 'r ', 'mo', 're', ' y', 'ea', 'rs', ' a', 'go', ', ', 'th', 'er', 'e ', 'we', 're', ' i', 'n ', 'th', 'is', '\\nc', 'ou', 'nt', 'ry', ' n', 'ot', 'hi', 'ng', ' b', 'ut', ' s', 'ma', 'll', ' k', 'in', 'gs', ', ', 'an', 'd ', 'on', 'e ', 'of', ' t']\n",
      "\n",
      "Processing file stories/073.txt\n",
      "Data size (characters) (Document 72) 5980\n",
      "sample string (Documents 72) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' k', 'in', 'g ', 'wh', 'o ', 'ha', 'd ', 'an', ' i', 'll', 'ne', 'ss', ', ', 'an', 'd ', 'no', ' o', 'ne', ' b', 'el', 'ie', 've', 'd ', 'th', 'at', ' h', 'e\\n', 'wo', 'ul', 'd ', 'co', 'me', ' o', 'ut', ' o', 'f ', 'it', ' w', 'it', 'h ', 'hi', 's ']\n",
      "\n",
      "Processing file stories/074.txt\n",
      "Data size (characters) (Document 73) 4518\n",
      "sample string (Documents 73) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'wo', 'od', 'cu', 'tt', 'er', ' w', 'ho', ' t', 'oi', 'le', 'd ', 'fr', 'om', ' e', 'ar', 'ly', '\\nm', 'or', 'ni', 'ng', ' t', 'il', 'l ', 'la', 'te', ' a', 't ', 'ni', 'gh', 't.', '  ', 'wh', 'en', ' a', 't ', 'la', 'st', ' h', 'e ']\n",
      "\n",
      "Processing file stories/075.txt\n",
      "Data size (characters) (Document 74) 3247\n",
      "sample string (Documents 74) ['a ', 'di', 'sc', 'ha', 'rg', 'ed', ' s', 'ol', 'di', 'er', ' h', 'ad', ' n', 'ot', 'hi', 'ng', ' t', 'o ', 'li', 've', ' o', 'n,', ' a', 'nd', ' d', 'id', ' n', 'ot', ' k', 'no', 'w ', 'ho', 'w ', 'to', '\\nm', 'ak', 'e ', 'hi', 's ', 'wa', 'y.', '  ', 'so', ' h', 'e ', 'we', 'nt', ' o', 'ut', ' i']\n",
      "\n",
      "Processing file stories/076.txt\n",
      "Data size (characters) (Document 75) 5130\n",
      "sample string (Documents 75) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' y', 'ou', 'ng', ' f', 'el', 'lo', 'w ', 'wh', 'o ', 'en', 'li', 'st', 'ed', ' a', 's ', 'a ', 'so', 'ld', 'ie', 'r,', ' c', 'on', 'du', 'ct', 'ed', '\\nh', 'im', 'se', 'lf', ' b', 'ra', 've', 'ly', ', ', 'an', 'd ', 'wa', 's ', 'al', 'wa', 'ys', ' t']\n",
      "\n",
      "Processing file stories/077.txt\n",
      "Data size (characters) (Document 76) 2401\n",
      "sample string (Documents 76) ['on', 'ce', ' i', 'n ', 'su', 'mm', 'er', '-t', 'im', 'e ', 'th', 'e ', 'be', 'ar', ' a', 'nd', ' t', 'he', ' w', 'ol', 'f ', 'we', 're', ' w', 'al', 'ki', 'ng', ' i', 'n ', 'th', 'e ', 'fo', 're', 'st', ',\\n', 'an', 'd ', 'th', 'e ', 'be', 'ar', ' h', 'ea', 'rd', ' a', ' b', 'ir', 'd ', 'si', 'ng']\n",
      "\n",
      "Processing file stories/078.txt\n",
      "Data size (characters) (Document 77) 624\n",
      "sample string (Documents 77) ['th', 'er', 'e ', 'wa', 's ', 'a ', 'po', 'or', ' b', 'ut', ' g', 'oo', 'd ', 'li', 'tt', 'le', ' g', 'ir', 'l ', 'wh', 'o ', 'li', 've', 'd ', 'al', 'on', 'e ', 'wi', 'th', ' h', 'er', '\\nm', 'ot', 'he', 'r,', ' a', 'nd', ' t', 'he', 'y ', 'no', ' l', 'on', 'ge', 'r ', 'ha', 'd ', 'an', 'yt', 'hi']\n",
      "\n",
      "Processing file stories/079.txt\n",
      "Data size (characters) (Document 78) 3991\n",
      "sample string (Documents 78) ['on', 'e ', 'da', 'y ', 'a ', 'pe', 'as', 'an', 't ', 'to', 'ok', ' h', 'is', ' g', 'oo', 'd ', 'ha', 'ze', 'l-', 'st', 'ic', 'k ', 'ou', 't ', 'of', ' t', 'he', ' c', 'or', 'ne', 'r\\n', 'an', 'd ', 'sa', 'id', ' t', 'o ', 'hi', 's ', 'wi', 'fe', ', ', 'tr', 'in', 'a,', ' i', ' a', 'm ', 'go', 'in']\n",
      "\n",
      "Processing file stories/080.txt\n",
      "Data size (characters) (Document 79) 1426\n",
      "sample string (Documents 79) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' l', 'it', 'tl', 'e ', 'ch', 'il', 'd ', 'wh', 'os', 'e ', 'mo', 'th', 'er', ' g', 'av', 'e ', 'he', 'r ', 'ev', 'er', 'y\\n', 'af', 'te', 'rn', 'oo', 'n ', 'a ', 'sm', 'al', 'l ', 'bo', 'wl', ' o', 'f ', 'mi', 'lk', ' a', 'nd', ' b', 're', 'ad', ', ']\n",
      "\n",
      "Processing file stories/081.txt\n",
      "Data size (characters) (Document 80) 3574\n",
      "sample string (Documents 80) ['in', ' a', ' c', 'er', 'ta', 'in', ' m', 'il', 'l ', 'li', 've', 'd ', 'an', ' o', 'ld', ' m', 'il', 'le', 'r ', 'wh', 'o ', 'ha', 'd ', 'ne', 'it', 'he', 'r ', 'wi', 'fe', ' n', 'or', ' c', 'hi', 'ld', ',\\n', 'an', 'd ', 'th', 're', 'e ', 'ap', 'pr', 'en', 'ti', 'ce', 's ', 'se', 'rv', 'ed', ' u']\n",
      "\n",
      "Processing file stories/082.txt\n",
      "Data size (characters) (Document 81) 10822\n",
      "sample string (Documents 81) ['hi', 'll', ' a', 'nd', ' v', 'al', 'e ', 'do', ' n', 'ot', ' m', 'ee', 't,', ' b', 'ut', ' t', 'he', ' c', 'hi', 'ld', 're', 'n ', 'of', ' m', 'en', ' d', 'o,', ' g', 'oo', 'd ', 'an', 'd ', 'ba', 'd.', '\\ni', 'n ', 'th', 'is', ' w', 'ay', ' a', ' s', 'ho', 'em', 'ak', 'er', ' a', 'nd', ' a', ' t']\n",
      "\n",
      "Processing file stories/083.txt\n",
      "Data size (characters) (Document 82) 5480\n",
      "sample string (Documents 82) ['\\th', 'an', 's ', 'th', 'e ', 'he', 'dg', 'eh', 'og', '\\n\\n', 'th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' c', 'ou', 'nt', 'ry', ' m', 'an', ' w', 'ho', ' h', 'ad', ' m', 'on', 'ey', ' a', 'nd', ' l', 'an', 'd ', 'in', ' p', 'le', 'nt', 'y,', ' b', 'ut', '\\nh', 'ow', 'ev', 'er', ' r', 'ic', 'h ']\n",
      "\n",
      "Processing file stories/084.txt\n",
      "Data size (characters) (Document 83) 658\n",
      "sample string (Documents 83) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' m', 'ot', 'he', 'r ', 'wh', 'o ', 'ha', 'd ', 'a ', 'li', 'tt', 'le', ' b', 'oy', ' o', 'f ', 'se', 've', 'n ', 'ye', 'ar', 's ', 'ol', 'd,', ' w', 'ho', '\\nw', 'as', ' s', 'o ', 'ha', 'nd', 'so', 'me', ' a', 'nd', ' l', 'ov', 'ab', 'le', ' t', 'ha']\n",
      "\n",
      "Processing file stories/085.txt\n",
      "Data size (characters) (Document 84) 5989\n",
      "sample string (Documents 84) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' y', 'ou', 'ng', ' f', 'el', 'lo', 'w ', 'wh', 'o ', 'ha', 'd ', 'le', 'ar', 'nt', ' t', 'he', ' t', 'ra', 'de', ' o', 'f ', 'lo', 'ck', 'sm', 'it', 'h,', '\\na', 'nd', ' t', 'ol', 'd ', 'hi', 's ', 'fa', 'th', 'er', ' h', 'e ', 'wo', 'ul', 'd ', 'no']\n",
      "\n",
      "Processing file stories/086.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size (characters) (Document 85) 8758\n",
      "sample string (Documents 85) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' k', 'in', 'g ', 'wh', 'o ', 'ha', 'd ', 'a ', 'li', 'tt', 'le', ' b', 'oy', ' i', 'n ', 'wh', 'os', 'e ', 'st', 'ar', 's\\n', 'it', ' h', 'ad', ' b', 'ee', 'n ', 'fo', 're', 'to', 'ld', ' t', 'ha', 't ', 'he', ' s']\n",
      "\n",
      "Processing file stories/087.txt\n",
      "Data size (characters) (Document 86) 3109\n",
      "sample string (Documents 86) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' p', 'ri', 'nc', 'es', 's ', 'wh', 'o ', 'wa', 's ', 'ex', 'tr', 'em', 'el', 'y ', 'pr', 'ou', 'd.', ' i', 'f ', 'a\\n', 'wo', 'oe', 'r ', 'ca', 'me', ' s', 'he', ' g', 'av', 'e ', 'hi', 'm ', 'so', 'me', ' r', 'id']\n",
      "\n",
      "Processing file stories/088.txt\n",
      "Data size (characters) (Document 87) 1365\n",
      "sample string (Documents 87) ['a ', 'ta', 'il', 'or', \"'s\", ' a', 'pp', 're', 'nt', 'ic', 'e ', 'wa', 's ', 'tr', 'av', 'el', 'in', 'g ', 'ab', 'ou', 't ', 'th', 'e ', 'wo', 'rl', 'd ', 'in', ' s', 'ea', 'rc', 'h ', 'of', '\\nw', 'or', 'k,', ' a', 'nd', ' a', 't ', 'on', 'e ', 'ti', 'me', ' h', 'e ', 'co', 'ul', 'd ', 'fi', 'nd']\n",
      "\n",
      "Processing file stories/089.txt\n",
      "Data size (characters) (Document 88) 4538\n",
      "sample string (Documents 88) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' o', 'n ', 'a ', 'ti', 'me', ' a', ' s', 'ol', 'di', 'er', ' w', 'ho', ' f', 'or', ' m', 'an', 'y ', 'ye', 'ar', 's ', 'ha', 'd ', 'se', 'rv', 'ed', ' t', 'he', '\\nk', 'in', 'g ', 'fa', 'it', 'hf', 'ul', 'ly', ', ', 'bu', 't ', 'wh', 'en', ' t', 'he', ' w']\n",
      "\n",
      "Processing file stories/090.txt\n",
      "Data size (characters) (Document 89) 345\n",
      "sample string (Documents 89) ['on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' t', 'he', 're', ' w', 'as', ' a', ' c', 'hi', 'ld', ' w', 'ho', ' w', 'as', ' w', 'il', 'lf', 'ul', ', ', 'an', 'd ', 'wo', 'ul', 'd ', 'no', 't ', 'do', '\\nw', 'ha', 't ', 'he', 'r ', 'mo', 'th', 'er', ' w', 'is', 'he', 'd.', '  ', 'fo', 'r ', 'th']\n",
      "\n",
      "Processing file stories/091.txt\n",
      "Data size (characters) (Document 90) 5460\n",
      "sample string (Documents 90) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' k', 'in', \"g'\", 's ', 'so', 'n,', ' w', 'ho', ' w', 'as', ' n', 'o ', 'lo', 'ng', 'er', ' c', 'on', 'te', 'nt', ' t', 'o ', 'st', 'ay', ' a', 't\\n', 'ho', 'me', ' i', 'n ', 'hi', 's ', 'fa', 'th', 'er', \"'s\", ' h', 'ou', 'se', ', ', 'an', 'd ', 'as']\n",
      "\n",
      "Processing file stories/092.txt\n",
      "Data size (characters) (Document 91) 6854\n",
      "sample string (Documents 91) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' y', 'ou', 'ng', ' h', 'un', 'ts', 'ma', 'n ', 'wh', 'o ', 'we', 'nt', ' i', 'nt', 'o ', 'th', 'e ', 'fo', 're', 'st', ' t', 'o ', 'li', 'e ', 'in', '\\nw', 'ai', 't.', '  ', 'he', ' h', 'ad', ' a', ' f', 're', 'sh', ' a', 'nd', ' j', 'oy', 'ou', 's ']\n",
      "\n",
      "Processing file stories/093.txt\n",
      "Data size (characters) (Document 92) 2314\n",
      "sample string (Documents 92) ['a ', 'po', 'or', ' s', 'er', 'va', 'nt', '-g', 'ir', 'l ', 'wa', 's ', 'on', 'ce', ' t', 'ra', 've', 'li', 'ng', ' w', 'it', 'h ', 'th', 'e ', 'fa', 'mi', 'ly', ' w', 'it', 'h ', 'wh', 'ic', 'h ', 'sh', 'e\\n', 'wa', 's ', 'in', ' s', 'er', 'vi', 'ce', ', ', 'th', 'ro', 'ug', 'h ', 'a ', 'gr', 'ea']\n",
      "\n",
      "Processing file stories/094.txt\n",
      "Data size (characters) (Document 93) 1706\n",
      "sample string (Documents 93) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' m', 'an', ' w', 'ho', ' h', 'ad', ' t', 'hr', 'ee', ' s', 'on', 's,', ' a', 'nd', ' n', 'ot', 'hi', 'ng', ' e', 'ls', 'e ', 'in', ' t', 'he', '\\nw', 'or', 'ld', ' b', 'ut', ' t', 'he', ' h', 'ou', 'se', ' i', 'n ', 'wh', 'ic', 'h ', 'he', ' l', 'iv']\n",
      "\n",
      "Processing file stories/095.txt\n",
      "Data size (characters) (Document 94) 3229\n",
      "sample string (Documents 94) ['th', 'er', 'e ', 'wa', 's ', 'a ', 'gr', 'ea', 't ', 'wa', 'r,', ' a', 'nd', ' t', 'he', ' k', 'in', 'g ', 'ha', 'd ', 'ma', 'ny', ' s', 'ol', 'di', 'er', 's,', ' b', 'ut', ' g', 'av', 'e ', 'th', 'em', '\\ns', 'ma', 'll', ' p', 'ay', ', ', 'so', ' s', 'ma', 'll', ' t', 'ha', 't ', 'th', 'ey', ' c']\n",
      "\n",
      "Processing file stories/096.txt\n",
      "Data size (characters) (Document 95) 4954\n",
      "sample string (Documents 95) ['on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' l', 'iv', 'ed', ' a', ' m', 'an', ' a', 'nd', ' a', ' w', 'om', 'an', ' w', 'ho', ' s', 'o ', 'lo', 'ng', ' a', 's ', 'th', 'ey', ' w', 'er', 'e\\n', 'ri', 'ch', ' h', 'ad', ' n', 'o ', 'ch', 'il', 'dr', 'en', ', ', 'bu', 't ', 'wh', 'en', ' t', 'he']\n",
      "\n",
      "Processing file stories/097.txt\n",
      "Data size (characters) (Document 96) 5732\n",
      "sample string (Documents 96) ['in', ' t', 'he', ' d', 'ay', 's ', 'wh', 'en', ' w', 'is', 'hi', 'ng', ' w', 'as', ' s', 'ti', 'll', ' o', 'f ', 'so', 'me', ' u', 'se', ', ', 'a ', 'ki', 'ng', \"'s\", ' s', 'on', ' w', 'as', '\\nb', 'ew', 'it', 'ch', 'ed', ' b', 'y ', 'an', ' o', 'ld', ' w', 'it', 'ch', ', ', 'an', 'd ', 'sh', 'ut']\n",
      "\n",
      "Processing file stories/098.txt\n",
      "Data size (characters) (Document 97) 4334\n",
      "sample string (Documents 97) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'ma', 'n ', 'wh', 'o ', 'ha', 'd ', 'fo', 'ur', ' s', 'on', 's,', ' a', 'nd', ' w', 'he', 'n ', 'th', 'ey', ' w', 'er', 'e ', 'gr', 'ow', 'n\\n', 'up', ', ', 'he', ' s', 'ai', 'd ', 'to', ' t', 'he', 'm,', ' \"', 'my', ' d', 'ea', 'r ']\n",
      "\n",
      "Processing file stories/099.txt\n",
      "Data size (characters) (Document 98) 7090\n",
      "sample string (Documents 98) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' w', 'om', 'an', ' w', 'ho', ' h', 'ad', ' t', 'hr', 'ee', ' d', 'au', 'gh', 'te', 'rs', ', ', 'th', 'e ', 'el', 'de', 'st', ' o', 'f ', 'wh', 'om', '\\nw', 'as', ' c', 'al', 'le', 'd ', 'on', 'e-', 'ey', 'e,', ' b', 'ec', 'au', 'se', ' s', 'he', ' h']\n",
      "\n",
      "Processing file stories/100.txt\n",
      "Data size (characters) (Document 99) 1007\n",
      "sample string (Documents 99) ['\"g', 'oo', 'd-', 'da', 'y,', ' f', 'at', 'he', 'r ', 'ho', 'll', 'en', 'th', 'e.', '\" ', '\"m', 'an', 'y ', 'th', 'an', 'ks', ', ', 'pi', 'f-', 'pa', 'f-', 'po', 'lt', 'ri', 'e.', '\" ', '\"m', 'ay', ' i', '\\nb', 'e ', 'al', 'lo', 'we', 'd ', 'to', ' h', 'av', 'e ', 'yo', 'ur', ' d', 'au', 'gh', 'te']\n",
      "\n",
      "Processing file stories/101.txt\n",
      "Data size (characters) (Document 100) 3891\n",
      "sample string (Documents 100) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' k', 'in', 'g ', 'wh', 'o ', 'ha', 'd ', 'tw', 'el', 've', ' d', 'au', 'gh', 'te', 'rs', ', ', 'ea', 'ch', ' o', 'ne', '\\nm', 'or', 'e ', 'be', 'au', 'ti', 'fu', 'l ', 'th', 'an', ' t', 'he', ' o', 'th', 'er', '. ']\n",
      "\n",
      "Processing file stories/102.txt\n",
      "Data size (characters) (Document 101) 6953\n",
      "sample string (Documents 101) ['in', ' o', 'ld', 'en', ' t', 'im', 'es', ' t', 'he', 're', ' l', 'iv', 'ed', ' a', 'n ', 'ag', 'ed', ' q', 'ue', 'en', ' w', 'ho', ' w', 'as', ' a', ' s', 'or', 'ce', 're', 'ss', ', ', 'an', 'd ', 'he', 'r\\n', 'da', 'ug', 'ht', 'er', ' w', 'as', ' t', 'he', ' m', 'os', 't ', 'be', 'au', 'ti', 'fu']\n",
      "\n",
      "Processing file stories/103.txt\n",
      "Data size (characters) (Document 102) 4093\n",
      "sample string (Documents 102) ['a ', 'wo', 'ma', 'n ', 'wa', 's ', 'wa', 'lk', 'in', 'g ', 'ab', 'ou', 't ', 'th', 'e ', 'fi', 'el', 'ds', ' w', 'it', 'h ', 'he', 'r ', 'da', 'ug', 'ht', 'er', ' a', 'nd', ' h', 'er', '\\ns', 'te', 'p-', 'da', 'ug', 'ht', 'er', ' c', 'ut', 'ti', 'ng', ' f', 'od', 'de', 'r,', ' w', 'he', 'n ', 'th']\n",
      "\n",
      "Processing file stories/104.txt\n",
      "Data size (characters) (Document 103) 8259\n",
      "sample string (Documents 103) ['**', '*t', 'he', 're', ' w', 'as', ' o', 'nc', 'e ', 'up', 'on', ' a', ' t', 'im', 'e ', 'a ', 'ki', 'ng', ' w', 'ho', ' h', 'ad', ' a', ' g', 're', 'at', ' f', 'or', 'es', 't ', 'ne', 'ar', '\\nh', 'is', ' p', 'al', 'ac', 'e,', ' f', 'ul', 'l ', 'of', ' a', 'll', ' k', 'in', 'ds', ' o', 'f ', 'wi']\n",
      "\n",
      "Processing file stories/105.txt\n",
      "Data size (characters) (Document 104) 1789\n",
      "sample string (Documents 104) ['ea', 'st', ' i', 'nd', 'ia', ' w', 'as', ' b', 'es', 'ie', 'ge', 'd ', 'by', ' a', 'n ', 'en', 'em', 'y ', 'wh', 'o ', 'wo', 'ul', 'd ', 'no', 't ', 're', 'ti', 're', ' u', 'nt', 'il', '\\nh', 'e ', 'ha', 'd ', 're', 'ce', 'iv', 'ed', ' s', 'ix', ' h', 'un', 'dr', 'ed', ' d', 'ol', 'la', 'rs', '. ']\n",
      "\n",
      "Processing file stories/106.txt\n",
      "Data size (characters) (Document 105) 393\n",
      "sample string (Documents 105) ['be', 'tw', 'ee', 'n ', 'we', 'rr', 'el', ' a', 'nd', ' s', 'oi', 'st', ' t', 'he', 're', ' l', 'iv', 'ed', ' a', ' m', 'an', ' w', 'ho', 'se', ' n', 'am', 'e ', 'wa', 's ', 'kn', 'oi', 'st', ',\\n', 'an', 'd ', 'he', ' h', 'ad', ' t', 'hr', 'ee', ' s', 'on', 's.', '  ', 'on', 'e ', 'wa', 's ', 'bl']\n",
      "\n",
      "Processing file stories/107.txt\n",
      "Data size (characters) (Document 106) 385\n",
      "sample string (Documents 106) ['a ', 'gi', 'rl', ' f', 'ro', 'm ', 'br', 'ak', 'el', ' o', 'nc', 'e ', 'we', 'nt', ' t', 'o ', 'st', '. ', 'an', 'ne', \"'s\", ' c', 'ha', 'pe', 'l ', 'at', ' t', 'he', ' f', 'oo', 't\\n', 'of', ' t', 'he', ' h', 'in', 'ne', 'nb', 'er', 'g,', ' a', 'nd', ' a', 's ', 'sh', 'e ', 'wa', 'nt', 'ed', ' t']\n",
      "\n",
      "Processing file stories/108.txt\n",
      "Data size (characters) (Document 107) 448\n",
      "sample string (Documents 107) ['wh', 'it', 'he', 'r ', 'do', ' y', 'ou', ' g', 'o.', '  ', 'to', ' w', 'al', 'pe', '. ', ' i', ' t', 'o ', 'wa', 'lp', 'e,', ' y', 'ou', ' t', 'o ', 'wa', 'lp', 'e,', ' s', 'o,', '\\ns', 'o,', ' t', 'og', 'et', 'he', 'r ', 'we', \"'l\", 'l ', 'go', '.\\n', 'ha', 've', ' y', 'ou', ' a', ' m', 'an', '. ']\n",
      "\n",
      "Processing file stories/109.txt\n",
      "Data size (characters) (Document 108) 1722\n",
      "sample string (Documents 108) ['th', 'er', 'e ', 'we', 're', ' o', 'nc', 'e ', 'a ', 'li', 'tt', 'le', ' b', 'ro', 'th', 'er', ' a', 'nd', ' a', ' l', 'it', 'tl', 'e ', 'si', 'st', 'er', ', ', 'wh', 'o ', 'lo', 've', 'd\\n', 'ea', 'ch', ' o', 'th', 'er', ' w', 'it', 'h ', 'al', 'l ', 'th', 'ei', 'r ', 'he', 'ar', 'ts', '. ', ' t']\n",
      "\n",
      "Processing file stories/110.txt\n",
      "Data size (characters) (Document 109) 2264\n",
      "sample string (Documents 109) ['th', 'er', 'e ', 'we', 're', ' o', 'nc', 'e ', 'tw', 'o ', 'br', 'ot', 'he', 'rs', ', ', 'th', 'e ', 'on', 'e ', 'ri', 'ch', ', ', 'th', 'e ', 'ot', 'he', 'r ', 'po', 'or', '.\\n', 'th', 'e ', 'ri', 'ch', ' o', 'ne', ', ', 'ho', 'we', 've', 'r,', ' g', 'av', 'e ', 'no', 'th', 'in', 'g ', 'to', ' t']\n",
      "\n",
      "Processing file stories/111.txt\n",
      "Data size (characters) (Document 110) 1018\n",
      "sample string (Documents 110) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'wo', 'ma', 'n ', 'wh', 'o ', 'ha', 'd ', 'a ', 'so', 'n,', ' w', 'ho', ' m', 'uc', 'h ', 'wi', 'sh', 'ed', ' t', 'o\\n', 'tr', 'av', 'el', ', ', 'bu', 't ', 'hi', 's ', 'mo', 'th', 'er', ' s', 'ai', 'd,', ' h', 'ow', ' c', 'an', ' y']\n",
      "\n",
      "Processing file stories/112.txt\n",
      "Data size (characters) (Document 111) 3286\n",
      "sample string (Documents 111) ['on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' t', 'he', 're', ' l', 'iv', 'ed', ' a', ' k', 'in', 'g ', 'an', 'd ', 'a ', 'qu', 'ee', 'n,', ' w', 'ho', ' w', 'er', 'e ', 'ri', 'ch', ',\\n', 'an', 'd ', 'ha', 'd ', 'ev', 'er', 'yt', 'hi', 'ng', ' t', 'he', 'y ', 'wa', 'nt', 'ed', ', ', 'bu', 't ']\n",
      "\n",
      "Processing file stories/113.txt\n",
      "Data size (characters) (Document 112) 405\n",
      "sample string (Documents 112) ['a ', 'ma', 'n ', 'an', 'd ', 'hi', 's ', 'wi', 'fe', ' w', 'er', 'e ', 'on', 'ce', ' s', 'it', 'ti', 'ng', ' b', 'y ', 'th', 'e ', 'do', 'or', ' o', 'f ', 'th', 'ei', 'r ', 'ho', 'us', 'e,', '\\na', 'nd', ' t', 'he', 'y ', 'ha', 'd ', 'a ', 'ro', 'as', 'te', 'd ', 'ch', 'ic', 'ke', 'n ', 'se', 't ']\n",
      "\n",
      "Processing file stories/114.txt\n",
      "Data size (characters) (Document 113) 2914\n",
      "sample string (Documents 113) ['th', 'er', 'e ', 'we', 're', ' o', 'nc', 'e ', 'tw', 'o ', 'br', 'ot', 'he', 'rs', ' w', 'ho', ' b', 'ot', 'h ', 'se', 'rv', 'ed', ' a', 's ', 'so', 'ld', 'ie', 'rs', ', ', 'on', 'e ', 'of', '\\nt', 'he', 'm ', 'wa', 's ', 'ri', 'ch', ', ', 'an', 'd ', 'th', 'e ', 'ot', 'he', 'r ', 'po', 'or', '. ']\n",
      "\n",
      "Processing file stories/115.txt\n",
      "Data size (characters) (Document 114) 1433\n",
      "sample string (Documents 114) ['at', ' t', 'he', ' t', 'im', 'e ', 'wh', 'en', ' o', 'ur', ' l', 'or', 'd ', 'st', 'il', 'l ', 'wa', 'lk', 'ed', ' t', 'hi', 's ', 'ea', 'rt', 'h,', ' h', 'e ', 'an', 'd ', 'st', '.\\n', 'pe', 'te', 'r ', 'st', 'op', 'pe', 'd ', 'on', 'e ', 'ev', 'en', 'in', 'g ', 'at', ' a', ' s', 'mi', 'th', \"'s\"]\n",
      "\n",
      "Processing file stories/116.txt\n",
      "Data size (characters) (Document 115) 545\n",
      "sample string (Documents 115) ['a ', 'ce', 'rt', 'ai', 'n ', 'ki', 'ng', ' h', 'ad', ' t', 'hr', 'ee', ' s', 'on', 's ', 'wh', 'o ', 'we', 're', ' a', 'll', ' e', 'qu', 'al', 'ly', ' d', 'ea', 'r ', 'to', '\\nh', 'im', ', ', 'an', 'd ', 'he', ' d', 'id', ' n', 'ot', ' k', 'no', 'w ', 'wh', 'ic', 'h ', 'of', ' t', 'he', 'm ', 'to']\n",
      "\n",
      "Processing file stories/117.txt\n",
      "Data size (characters) (Document 116) 877\n",
      "sample string (Documents 116) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' s', 'he', 'ph', 'er', 'd ', 'bo', 'y ', 'wh', 'os', 'e ', 'fa', 'me', ' s', 'pr', 'ea', 'd\\n', 'fa', 'r ', 'an', 'd ', 'wi', 'de', ' b', 'ec', 'au', 'se', ' o', 'f ', 'th', 'e ', 'wi', 'se', ' a', 'ns', 'we', 'rs']\n",
      "\n",
      "Processing file stories/118.txt\n",
      "Data size (characters) (Document 117) 801\n",
      "sample string (Documents 117) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' l', 'it', 'tl', 'e ', 'gi', 'rl', ' w', 'ho', 'se', ' f', 'at', 'he', 'r ', 'an', 'd ', 'mo', 'th', 'er', '\\nw', 'er', 'e ', 'de', 'ad', ', ', 'an', 'd ', 'sh', 'e ', 'wa', 's ', 'so', ' p', 'oo', 'r ', 'th', 'at']\n",
      "\n",
      "Processing file stories/119.txt\n",
      "Data size (characters) (Document 118) 889\n",
      "sample string (Documents 118) ['a ', 'fa', 'th', 'er', ' w', 'as', ' o', 'ne', ' d', 'ay', ' s', 'it', 'ti', 'ng', ' a', 't ', 'di', 'nn', 'er', ' w', 'it', 'h ', 'hi', 's ', 'wi', 'fe', ' a', 'nd', ' h', 'is', '\\nc', 'hi', 'ld', 're', 'n,', ' a', 'nd', ' a', ' g', 'oo', 'd ', 'fr', 'ie', 'nd', ' w', 'ho', ' h', 'ad', ' c', 'om']\n",
      "\n",
      "Processing file stories/120.txt\n",
      "Data size (characters) (Document 119) 404\n",
      "sample string (Documents 119) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' y', 'ou', 'ng', ' s', 'he', 'ph', 'er', 'd ', 'wh', 'o ', 'wa', 'nt', 'ed', ' v', 'er', 'y ', 'mu', 'ch', ' t', 'o ', 'ma', 'rr', 'y,', '\\na', 'nd', ' w', 'as', ' a', 'cq', 'ua', 'in', 'te', 'd ', 'wi', 'th', ' t', 'hr', 'ee', ' s', 'is', 'te', 'rs']\n",
      "\n",
      "Processing file stories/121.txt\n",
      "Data size (characters) (Document 120) 2545\n",
      "sample string (Documents 120) ['a ', 'sp', 'ar', 'ro', 'w ', 'ha', 'd ', 'fo', 'ur', ' y', 'ou', 'ng', ' o', 'ne', 's ', 'in', ' a', ' s', 'wa', 'll', 'ow', \"'s\", ' n', 'es', 't.', '  ', 'wh', 'en', ' t', 'he', 'y\\n', 'we', 're', ' f', 'le', 'dg', 'ed', ', ', 'so', 'me', ' n', 'au', 'gh', 'ty', ' b', 'oy', 's ', 'pu', 'll', 'ed']\n",
      "\n",
      "Processing file stories/122.txt\n",
      "Data size (characters) (Document 121) 6387\n",
      "sample string (Documents 121) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'wi', 'do', 'w ', 'wh', 'o ', 'li', 've', 'd ', 'in', ' a', ' l', 'on', 'el', 'y ', 'co', 'tt', 'ag', 'e.', '  ', 'in', '\\nf', 'ro', 'nt', ' o', 'f ', 'th', 'e ', 'co', 'tt', 'ag', 'e ', 'wa', 's ', 'a ', 'ga', 'rd', 'en', ' w', 'he']\n",
      "\n",
      "Processing file stories/123.txt\n",
      "Data size (characters) (Document 122) 6215\n",
      "sample string (Documents 122) ['le', 't ', 'no', ' o', 'ne', ' e', 've', 'r ', 'sa', 'y ', 'th', 'at', ' a', ' p', 'oo', 'r ', 'ta', 'il', 'or', ' c', 'an', 'no', 't ', 'do', ' g', 're', 'at', ' t', 'hi', 'ng', 's\\n', 'an', 'd ', 'wi', 'n ', 'hi', 'gh', ' h', 'on', 'or', 's.', '  ', 'al', 'l ', 'th', 'at', ' i', 's ', 'ne', 'ed']\n",
      "\n",
      "Processing file stories/124.txt\n",
      "Data size (characters) (Document 123) 2567\n",
      "sample string (Documents 123) ['ha', 'rr', 'y ', 'wa', 's ', 'la', 'zy', ', ', 'an', 'd ', 'al', 'th', 'ou', 'gh', ' h', 'e ', 'ha', 'd ', 'no', 'th', 'in', 'g ', 'el', 'se', ' t', 'o ', 'do', ' b', 'ut', '\\nd', 'ri', 've', ' h', 'is', ' g', 'oa', 't ', 'da', 'il', 'y ', 'to', ' p', 'as', 'tu', 're', ', ', 'he', ' n', 'ev', 'er']\n",
      "\n",
      "Processing file stories/125.txt\n",
      "Data size (characters) (Document 124) 7174\n",
      "sample string (Documents 124) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' k', 'in', 'g,', ' b', 'ut', ' w', 'he', 're', ' h', 'e ', 're', 'ig', 'ne', 'd ', 'an', 'd ', 'wh', 'at', '\\nh', 'e ', 'wa', 's ', 'ca', 'll', 'ed', ', ', 'i ', 'do', ' n', 'ot', ' k', 'no', 'w.', '  ', 'he', ' h']\n",
      "\n",
      "Processing file stories/126.txt\n",
      "Data size (characters) (Document 125) 6823\n",
      "sample string (Documents 125) ['th', 'er', 'e ', 'we', 're', ' o', 'nc', 'e ', 'a ', 'ma', 'n ', 'an', 'd ', 'a ', 'wo', 'ma', 'n ', 'wh', 'o ', 'ha', 'd ', 'an', ' o', 'nl', 'y ', 'ch', 'il', 'd,', ' a', 'nd', '\\nl', 'iv', 'ed', ' q', 'ui', 'te', ' a', 'lo', 'ne', ' i', 'n ', 'a ', 'so', 'li', 'ta', 'ry', ' v', 'al', 'le', 'y.']\n",
      "\n",
      "Processing file stories/127.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size (characters) (Document 126) 5064\n",
      "sample string (Documents 126) ['a ', 'po', 'or', ' w', 'oo', 'd-', 'cu', 'tt', 'er', ' l', 'iv', 'ed', ' w', 'it', 'h ', 'hi', 's ', 'wi', 'fe', ' a', 'nd', ' t', 'hr', 'ee', ' d', 'au', 'gh', 'te', 'rs', ' i', 'n\\n', 'a ', 'li', 'tt', 'le', ' h', 'ut', ' o', 'n ', 'th', 'e ', 'ed', 'ge', ' o', 'f ', 'a ', 'lo', 'ne', 'ly', ' f']\n",
      "\n",
      "Processing file stories/128.txt\n",
      "Data size (characters) (Document 127) 10096\n",
      "sample string (Documents 127) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' v', 'er', 'y ', 'ol', 'd ', 'wo', 'ma', 'n,', ' w', 'ho', ' l', 'iv', 'ed', ' w', 'it', 'h ', 'he', 'r\\n', 'fl', 'oc', 'k ', 'of', ' g', 'ee', 'se', ' i', 'n ', 'a ', 're', 'mo', 'te', ' c', 'le', 'ar', 'in', 'g ']\n",
      "\n",
      "Processing file stories/129.txt\n",
      "Data size (characters) (Document 128) 1845\n",
      "sample string (Documents 128) ['wh', 'en', ' a', 'da', 'm ', 'an', 'd ', 'ev', 'e ', 'we', 're', ' d', 'ri', 've', 'n ', 'ou', 't ', 'of', ' p', 'ar', 'ad', 'is', 'e,', ' t', 'he', 'y ', 'we', 're', '\\nc', 'om', 'pe', 'll', 'ed', ' t', 'o ', 'bu', 'il', 'd ', 'a ', 'ho', 'us', 'e ', 'fo', 'r ', 'th', 'em', 'se', 'lv', 'es', ' o']\n",
      "\n",
      "Processing file stories/130.txt\n",
      "Data size (characters) (Document 129) 5643\n",
      "sample string (Documents 129) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' m', 'il', 'le', 'r ', 'wh', 'o ', 'li', 've', 'd ', 'wi', 'th', ' h', 'is', ' w', 'if', 'e\\n', 'in', ' g', 're', 'at', ' c', 'on', 'te', 'nt', 'me', 'nt', '. ', ' t', 'he', 'y ', 'ha', 'd ', 'mo', 'ne', 'y ', 'an']\n",
      "\n",
      "Processing file stories/131.txt\n",
      "Data size (characters) (Document 130) 3172\n",
      "sample string (Documents 130) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'sh', 'ep', 'he', 'rd', '-b', 'oy', ' w', 'ho', 'se', ' f', 'at', 'he', 'r ', 'an', 'd ', 'mo', 'th', 'er', '\\nw', 'er', 'e ', 'de', 'ad', ', ', 'an', 'd ', 'he', ' w', 'as', ' p', 'la', 'ce', 'd ', 'by', ' t', 'he', ' a', 'ut', 'ho']\n",
      "\n",
      "Processing file stories/132.txt\n",
      "Data size (characters) (Document 131) 7003\n",
      "sample string (Documents 131) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' g', 'ir', 'l ', 'wh', 'o ', 'wa', 's ', 'yo', 'un', 'g ', 'an', 'd ', 'be', 'au', 'ti', 'fu', 'l,', ' b', 'ut', '\\ns', 'he', ' h', 'ad', ' l', 'os', 't ', 'he', 'r ', 'mo', 'th', 'er', ' w', 'he', 'n ', 'sh', 'e ']\n",
      "\n",
      "Processing file stories/133.txt\n",
      "Data size (characters) (Document 132) 2910\n",
      "sample string (Documents 132) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' g', 'ir', 'l ', 'wh', 'os', 'e ', 'fa', 'th', 'er', ' a', 'nd', ' m', 'ot', 'he', 'r ', 'di', 'ed', ' w', 'hi', 'le', '\\ns', 'he', ' w', 'as', ' s', 'ti', 'll', ' a', ' l', 'it', 'tl', 'e ', 'ch', 'il', 'd.', '  ', 'al', 'l ', 'al', 'on', 'e,', ' i']\n",
      "\n",
      "Processing file stories/134.txt\n",
      "Data size (characters) (Document 133) 3316\n",
      "sample string (Documents 133) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' p', 'ri', 'nc', 'es', 's,', ' w', 'ho', ', ', 'hi', 'gh', ' u', 'nd', 'er', ' t', 'he', '\\nb', 'at', 'tl', 'em', 'en', 'ts', ' i', 'n ', 'he', 'r ', 'ca', 'st', 'le', ', ', 'ha', 'd ', 'an', ' a', 'pa', 'rt', 'me']\n",
      "\n",
      "Processing file stories/135.txt\n",
      "Data size (characters) (Document 134) 7859\n",
      "sample string (Documents 134) ['on', 'e ', 'da', 'y ', 'an', ' o', 'ld', ' m', 'an', ' a', 'nd', ' h', 'is', ' w', 'if', 'e ', 'we', 're', ' s', 'it', 'ti', 'ng', ' i', 'n ', 'fr', 'on', 't ', 'of', ' a', '\\nm', 'is', 'er', 'ab', 'le', ' h', 'ou', 'se', ' r', 'es', 'ti', 'ng', ' a', ' w', 'hi', 'le', ' f', 'ro', 'm ', 'th', 'ei']\n",
      "\n",
      "Processing file stories/136.txt\n",
      "Data size (characters) (Document 135) 9842\n",
      "sample string (Documents 135) ['a ', 'yo', 'un', 'g ', 'dr', 'um', 'me', 'r ', 'we', 'nt', ' o', 'ut', ' q', 'ui', 'te', ' a', 'lo', 'ne', ' o', 'ne', ' e', 've', 'ni', 'ng', ' i', 'nt', 'o ', 'th', 'e ', 'co', 'un', 'tr', 'y,', '\\na', 'nd', ' c', 'am', 'e ', 'to', ' a', ' l', 'ak', 'e ', 'on', ' t', 'he', ' s', 'ho', 're', ' o']\n",
      "\n",
      "Processing file stories/137.txt\n",
      "Data size (characters) (Document 136) 637\n",
      "sample string (Documents 136) ['in', ' f', 'or', 'me', 'r ', 'ti', 'me', 's,', ' w', 'he', 'n ', 'go', 'd ', 'hi', 'ms', 'el', 'f ', 'st', 'il', 'l ', 'wa', 'lk', 'ed', ' t', 'he', ' e', 'ar', 'th', ', ', 'th', 'e\\n', 'fr', 'ui', 'tf', 'ul', 'ne', 'ss', ' o', 'f ', 'th', 'e ', 'so', 'il', ' w', 'as', ' m', 'uc', 'h ', 'gr', 'ea']\n",
      "\n",
      "Processing file stories/138.txt\n",
      "Data size (characters) (Document 137) 1984\n",
      "sample string (Documents 137) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' k', 'in', 'g ', 'wh', 'o ', 'ha', 'd ', 'a ', 'da', 'ug', 'ht', 'er', ',\\n', 'an', 'd ', 'he', ' c', 'au', 'se', 'd ', 'a ', 'gl', 'as', 's ', 'mo', 'un', 'ta', 'in', ' t', 'o ', 'be', ' m', 'ad', 'e,', ' a', 'nd']\n",
      "\n",
      "Processing file stories/139.txt\n",
      "Data size (characters) (Document 138) 2746\n",
      "sample string (Documents 138) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', 'n ', 'en', 'ch', 'an', 'tr', 'es', 's,', ' w', 'ho', ' h', 'ad', ' t', 'hr', 'ee', ' s', 'on', 's ', 'wh', 'o ', 'lo', 've', 'd ', 'ea', 'ch', '\\no', 'th', 'er', ' a', 's ', 'br', 'ot', 'he', 'rs', ', ', 'bu', 't ', 'th', 'e ', 'ol', 'd ', 'wo', 'ma']\n",
      "\n",
      "Processing file stories/140.txt\n",
      "Data size (characters) (Document 139) 5265\n",
      "sample string (Documents 139) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' k', 'in', 'g ', 'wh', 'o ', 'ha', 'd ', 'a ', 'so', 'n ', 'wh', 'o ', 'as', 'ke', 'd ', 'in', ' m', 'ar', 'ri', 'ag', 'e ', 'th', 'e\\n', 'da', 'ug', 'ht', 'er', ' o', 'f ', 'a ', 'mi', 'gh', 'ty', ' k', 'in', 'g,', ' s', 'he', ' w', 'as', ' c', 'al']\n",
      "\n",
      "Processing file stories/141.txt\n",
      "Data size (characters) (Document 140) 2943\n",
      "sample string (Documents 140) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' o', 'n ', 'a ', 'ti', 'me', ' a', ' m', 'ot', 'he', 'r ', 'wh', 'o ', 'ha', 'd ', 'th', 're', 'e ', 'da', 'ug', 'ht', 'er', 's,', '\\nt', 'he', ' e', 'ld', 'es', 't ', 'of', ' w', 'ho', 'm ', 'wa', 's ', 'ru', 'de', ' a', 'nd', ' w', 'ic', 'ke', 'd,', ' t']\n",
      "\n",
      "Processing file stories/142.txt\n",
      "Data size (characters) (Document 141) 1053\n",
      "sample string (Documents 141) ['th', 're', 'e ', 'hu', 'nd', 're', 'd ', 'ye', 'ar', 's ', 'be', 'fo', 're', ' t', 'he', ' b', 'ir', 'th', ' o', 'f ', 'th', 'e ', 'lo', 'rd', ' c', 'hr', 'is', 't,', ' t', 'he', 're', '\\nl', 'iv', 'ed', ' a', ' m', 'ot', 'he', 'r ', 'wh', 'o ', 'ha', 'd ', 'tw', 'el', 've', ' s', 'on', 's,', ' b']\n",
      "\n",
      "Processing file stories/143.txt\n",
      "Data size (characters) (Document 142) 398\n",
      "sample string (Documents 142) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'wo', 'ma', 'n ', 'wh', 'o ', 'ha', 'd ', 'tw', 'o ', 'ch', 'il', 'dr', 'en', '. ', ' t', 'he', ' y', 'ou', 'ng', 'es', 't\\n', 'ha', 'd ', 'to', ' g', 'o ', 'ev', 'er', 'y ', 'da', 'y ', 'in', 'to', ' t', 'he', ' f', 'or', 'es', 't ']\n",
      "\n",
      "Processing file stories/144.txt\n",
      "Data size (characters) (Document 143) 1251\n",
      "sample string (Documents 143) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' k', 'in', \"g'\", 's ', 'so', 'n ', 'wh', 'o ', 'we', 'nt', ' o', 'ut', ' i', 'nt', 'o ', 'th', 'e ', 'wo', 'rl', 'd,', ' a', 'nd', '\\nh', 'e ', 'wa', 's ', 'fu', 'll', ' o', 'f ', 'th', 'ou', 'gh', 't ', 'an', 'd ', 'sa', 'd.', '  ', 'he', ' l', 'oo']\n",
      "\n",
      "Processing file stories/145.txt\n",
      "Data size (characters) (Document 144) 639\n",
      "sample string (Documents 144) ['th', 'er', 'e ', 'we', 're', ' o', 'nc', 'e ', 'up', 'on', ' a', ' t', 'im', 'e ', 'tw', 'o ', 'si', 'st', 'er', 's,', ' o', 'ne', ' o', 'f ', 'wh', 'om', ' h', 'ad', ' n', 'o\\n', 'ch', 'il', 'dr', 'en', ' a', 'nd', ' w', 'as', ' r', 'ic', 'h,', ' a', 'nd', ' t', 'he', ' o', 'th', 'er', ' h', 'ad']\n",
      "\n",
      "Processing file stories/146.txt\n",
      "Data size (characters) (Document 145) 2451\n",
      "sample string (Documents 145) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' h', 'er', 'mi', 't ', 'wh', 'o ', 'li', 've', 'd ', 'in', ' a', ' f', 'or', 'es', 't ', 'at', ' t', 'he', '\\nf', 'oo', 't ', 'of', ' a', ' m', 'ou', 'nt', 'ai', 'n,', ' a', 'nd', ' p', 'as', 'se', 'd ', 'hi', 's ']\n",
      "\n",
      "Processing file stories/147.txt\n",
      "Data size (characters) (Document 146) 887\n",
      "sample string (Documents 146) ['in', ' a', ' l', 'ar', 'ge', ' t', 'ow', 'n ', 'th', 'er', 'e ', 'wa', 's ', 'an', ' o', 'ld', ' w', 'om', 'an', ' w', 'ho', ' s', 'at', ' i', 'n ', 'th', 'e ', 'ev', 'en', 'in', 'g ', 'al', 'on', 'e\\n', 'in', ' h', 'er', ' r', 'oo', 'm ', 'th', 'in', 'ki', 'ng', ' h', 'ow', ' s', 'he', ' h', 'ad']\n",
      "\n",
      "Processing file stories/148.txt\n",
      "Data size (characters) (Document 147) 566\n",
      "sample string (Documents 147) ['on', 'e ', 'af', 'te', 'rn', 'oo', 'n ', 'th', 'e ', 'ch', 'ri', 'st', '-c', 'hi', 'ld', ' h', 'ad', ' l', 'ai', 'd ', 'hi', 'ms', 'el', 'f ', 'in', ' h', 'is', ' c', 'ra', 'dl', 'e-', 'be', 'd\\n', 'an', 'd ', 'ha', 'd ', 'fa', 'll', 'en', ' a', 'sl', 'ee', 'p.', '  ', 'th', 'en', ' h', 'is', ' m']\n",
      "\n",
      "Processing file stories/149.txt\n",
      "Data size (characters) (Document 148) 2517\n",
      "sample string (Documents 148) ['a ', 'ce', 'rt', 'ai', 'n ', 'ca', 't ', 'ha', 'd ', 'ma', 'de', ' t', 'he', ' a', 'cq', 'ua', 'in', 'ta', 'nc', 'e ', 'of', ' a', ' m', 'ou', 'se', ', ', 'an', 'd ', 'ha', 'd\\n', 'sa', 'id', ' s', 'o ', 'mu', 'ch', ' t', 'o ', 'he', 'r ', 'ab', 'ou', 't ', 'th', 'e ', 'gr', 'ea', 't ', 'lo', 've']\n",
      "\n",
      "Processing file stories/150.txt\n",
      "Data size (characters) (Document 149) 2765\n",
      "sample string (Documents 149) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' w', 'on', 'de', 'rf', 'ul', ' m', 'us', 'ic', 'ia', 'n,', ' w', 'ho', ' w', 'en', 't ', 'qu', 'it', 'e ', 'fo', 'rl', 'or', 'n\\n', 'th', 'ro', 'ug', 'h ', 'a ', 'fo', 're', 'st', ' a', 'nd', ' t', 'ho', 'ug', 'ht', ' o', 'f ', 'al', 'l ', 'ma', 'nn']\n",
      "\n",
      "Processing file stories/151.txt\n",
      "Data size (characters) (Document 150) 2060\n",
      "sample string (Documents 150) ['th', 'e ', 'co', 'ck', ' o', 'nc', 'e ', 'sa', 'id', ' t', 'o ', 'th', 'e ', 'he', 'n,', ' i', 't ', 'is', ' n', 'ow', ' t', 'he', ' t', 'im', 'e ', 'wh', 'en', ' t', 'he', ' n', 'ut', 's\\n', 'ar', 'e ', 'ri', 'pe', ', ', 'so', ' l', 'et', ' u', 's ', 'go', ' t', 'o ', 'th', 'e ', 'hi', 'll', ' t']\n",
      "\n",
      "Processing file stories/152.txt\n",
      "Data size (characters) (Document 151) 1331\n",
      "sample string (Documents 151) ['in', ' a', ' v', 'il', 'la', 'ge', ' d', 'we', 'lt', ' a', ' p', 'oo', 'r ', 'ol', 'd ', 'wo', 'ma', 'n,', ' w', 'ho', ' h', 'ad', ' g', 'at', 'he', 're', 'd ', 'to', 'ge', 'th', 'er', ' a', '\\nd', 'is', 'h ', 'of', ' b', 'ea', 'ns', ' a', 'nd', ' w', 'an', 'te', 'd ', 'to', ' c', 'oo', 'k ', 'th']\n",
      "\n",
      "Processing file stories/153.txt\n",
      "Data size (characters) (Document 152) 7907\n",
      "sample string (Documents 152) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' f', 'is', 'he', 'rm', 'an', ' w', 'ho', ' l', 'iv', 'ed', ' w', 'it', 'h ', 'hi', 's ', 'wi', 'fe', '\\ni', 'n ', 'a ', 'pi', 'g-', 'st', 'ye', ' c', 'lo', 'se', ' b', 'y ', 'th', 'e ', 'se', 'a,', ' a', 'nd', ' e']\n",
      "\n",
      "Processing file stories/154.txt\n",
      "Data size (characters) (Document 153) 1611\n",
      "sample string (Documents 153) ['on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' m', 'ou', 'se', ', ', 'a ', 'bi', 'rd', ', ', 'an', 'd ', 'a ', 'sa', 'us', 'ag', 'e ', 'be', 'ca', 'me', '\\nc', 'om', 'pa', 'ni', 'on', 's,', ' k', 'ep', 't ', 'ho', 'us', 'e ', 'to', 'ge', 'th', 'er', ', ', 'li', 've', 'd ', 'we', 'll', ' a']\n",
      "\n",
      "Processing file stories/155.txt\n",
      "Data size (characters) (Document 154) 3244\n",
      "sample string (Documents 154) ['a ', 'ce', 'rt', 'ai', 'n ', 'ma', 'n ', 'ha', 'd ', 'a ', 'do', 'nk', 'ey', ', ', 'wh', 'ic', 'h ', 'ha', 'd ', 'ca', 'rr', 'ie', 'd ', 'th', 'e ', 'co', 'rn', '-s', 'ac', 'ks', '\\nt', 'o ', 'th', 'e ', 'mi', 'll', ' i', 'nd', 'ef', 'at', 'ig', 'ab', 'ly', ' f', 'or', ' m', 'an', 'y ', 'a ', 'lo']\n",
      "\n",
      "Processing file stories/156.txt\n",
      "Data size (characters) (Document 155) 1407\n",
      "sample string (Documents 155) ['a ', 'lo', 'us', 'e ', 'an', 'd ', 'a ', 'fl', 'ea', ' k', 'ep', 't ', 'ho', 'us', 'e ', 'to', 'ge', 'th', 'er', ' a', 'nd', ' w', 'er', 'e ', 'br', 'ew', 'in', 'g ', 'be', 'er', '\\ni', 'n ', 'an', ' e', 'gg', '-s', 'he', 'll', '. ', ' t', 'he', 'n ', 'th', 'e ', 'li', 'tt', 'le', ' l', 'ou', 'se']\n",
      "\n",
      "Processing file stories/157.txt\n",
      "Data size (characters) (Document 156) 2016\n",
      "sample string (Documents 156) ['on', 'e ', 've', 'ry', ' f', 'in', 'e ', 'da', 'y ', 'it', ' c', 'am', 'e ', 'to', ' p', 'as', 's ', 'th', 'at', ' t', 'he', ' g', 'oo', 'd ', 'go', 'd ', 'wi', 'sh', 'ed', ' t', 'o ', 'en', 'jo', 'y\\n', 'hi', 'ms', 'el', 'f ', 'in', ' t', 'he', ' h', 'ea', 've', 'nl', 'y ', 'ga', 'rd', 'en', ', ']\n",
      "\n",
      "Processing file stories/158.txt\n",
      "Data size (characters) (Document 157) 2073\n",
      "sample string (Documents 157) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', 'n ', 'ol', 'd ', 'fo', 'x ', 'wi', 'th', ' n', 'in', 'e ', 'ta', 'il', 's,', ' w', 'ho', '\\nb', 'el', 'ie', 've', 'd ', 'th', 'at', ' h', 'is', ' w', 'if', 'e ', 'wa', 's ', 'no', 't ', 'fa', 'it', 'hf', 'ul', ' t']\n",
      "\n",
      "Processing file stories/159.txt\n",
      "Data size (characters) (Document 158) 1700\n",
      "sample string (Documents 158) ['a ', 'sh', 'oe', 'ma', 'ke', 'r,', ' b', 'y ', 'no', ' f', 'au', 'lt', ' o', 'f ', 'hi', 's ', 'ow', 'n,', ' h', 'ad', ' b', 'ec', 'om', 'e ', 'so', ' p', 'oo', 'r ', 'th', 'at', ' a', 't\\n', 'la', 'st', ' h', 'e ', 'ha', 'd ', 'no', 'th', 'in', 'g ', 'le', 'ft', ' b', 'ut', ' l', 'ea', 'th', 'er']\n",
      "\n",
      "Processing file stories/160.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size (characters) (Document 159) 989\n",
      "sample string (Documents 159) ['th', 'er', 'e ', 'we', 're', ' o', 'nc', 'e ', 'a ', 'co', 'ck', ' a', 'nd', ' a', ' h', 'en', ' w', 'ho', ' w', 'an', 'te', 'd ', 'to', ' t', 'ak', 'e ', 'a ', 'jo', 'ur', 'ne', 'y\\n', 'to', 'ge', 'th', 'er', '. ', ' s', 'o ', 'th', 'e ', 'co', 'ck', ' b', 'ui', 'lt', ' a', ' b', 'ea', 'ut', 'if']\n",
      "\n",
      "Processing file stories/161.txt\n",
      "Data size (characters) (Document 160) 3114\n",
      "sample string (Documents 160) ['a ', 'sh', 'ee', 'p-', 'do', 'g ', 'ha', 'd ', 'no', 't ', 'a ', 'go', 'od', ' m', 'as', 'te', 'r,', ' b', 'ut', ', ', 'on', ' t', 'he', ' c', 'on', 'tr', 'ar', 'y,', ' o', 'ne', ' w', 'ho', '\\nl', 'et', ' h', 'im', ' s', 'uf', 'fe', 'r ', 'hu', 'ng', 'er', '. ', ' a', 's ', 'he', ' c', 'ou', 'ld']\n",
      "\n",
      "Processing file stories/162.txt\n",
      "Data size (characters) (Document 161) 6731\n",
      "sample string (Documents 161) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' m', 'an', ' w', 'ho', ' w', 'as', ' c', 'al', 'le', 'd ', 'fr', 'ed', 'er', 'ic', 'k\\n', 'an', 'd ', 'a ', 'wo', 'ma', 'n ', 'ca', 'll', 'ed', ' c', 'at', 'he', 'ri', 'ne', ', ', 'wh', 'o ', 'ha', 'd ', 'ma', 'rr']\n",
      "\n",
      "Processing file stories/163.txt\n",
      "Data size (characters) (Document 162) 5357\n",
      "sample string (Documents 162) ['th', 'er', 'e ', 'wa', 's ', 'a ', 'ce', 'rt', 'ai', 'n ', 'vi', 'll', 'ag', 'e ', 'wh', 'er', 'ei', 'n ', 'no', ' o', 'ne', ' l', 'iv', 'ed', ' b', 'ut', ' r', 'ea', 'll', 'y ', 'ri', 'ch', '\\np', 'ea', 'sa', 'nt', 's,', ' a', 'nd', ' j', 'us', 't ', 'on', 'e ', 'po', 'or', ' o', 'ne', ', ', 'wh']\n",
      "\n",
      "Processing file stories/164.txt\n",
      "Data size (characters) (Document 163) 2546\n",
      "sample string (Documents 163) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', 'n ', 'ol', 'd ', 'ca', 'st', 'le', ' i', 'n ', 'th', 'e ', 'mi', 'ds', 't ', 'of', ' a', ' l', 'ar', 'ge', ' a', 'nd', ' d', 'en', 'se', '\\nf', 'or', 'es', 't,', ' a', 'nd', ' i', 'n ', 'it', ' a', 'n ', 'ol', 'd ', 'wo', 'ma', 'n ', 'wh', 'o ', 'wa']\n",
      "\n",
      "Processing file stories/165.txt\n",
      "Data size (characters) (Document 164) 1018\n",
      "sample string (Documents 164) ['on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' t', 'he', ' f', 'ox', ' w', 'as', ' t', 'al', 'ki', 'ng', ' t', 'o ', 'th', 'e ', 'wo', 'lf', ' o', 'f ', 'th', 'e\\n', 'st', 're', 'ng', 'th', ' o', 'f ', 'ma', 'n.', '  ', 'ho', 'w ', 'no', ' a', 'ni', 'ma', 'l ', 'co', 'ul', 'd ', 'wi', 'th', 'st']\n",
      "\n",
      "Processing file stories/166.txt\n",
      "Data size (characters) (Document 165) 1827\n",
      "sample string (Documents 165) ['th', 'e ', 'wo', 'lf', ' h', 'ad', ' t', 'he', ' f', 'ox', ' w', 'it', 'h ', 'hi', 'm,', ' a', 'nd', ' w', 'ha', 'ts', 'oe', 've', 'r ', 'th', 'e ', 'wo', 'lf', '\\nw', 'is', 'he', 'd,', ' t', 'ha', 't ', 'th', 'e ', 'fo', 'x ', 'wa', 's ', 'co', 'mp', 'el', 'le', 'd ', 'to', ' d', 'o,', ' f', 'or']\n",
      "\n",
      "Processing file stories/167.txt\n",
      "Data size (characters) (Document 166) 734\n",
      "sample string (Documents 166) ['it', ' h', 'ap', 'pe', 'ne', 'd ', 'th', 'at', ' t', 'he', ' c', 'at', ' m', 'et', ' t', 'he', ' f', 'ox', ' i', 'n ', 'a ', 'fo', 're', 'st', ', ', 'an', 'd ', 'as', ' s', 'he', '\\nt', 'ho', 'ug', 'ht', ' t', 'o ', 'he', 'rs', 'el', 'f,', ' h', 'e ', 'is', ' c', 'le', 've', 'r ', 'an', 'd ', 'fu']\n",
      "\n",
      "Processing file stories/168.txt\n",
      "Data size (characters) (Document 167) 2408\n",
      "sample string (Documents 167) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' c', 'oo', 'k ', 'na', 'me', 'd ', 'gr', 'et', 'el', ', ', 'wh', 'o ', 'wo', 're', ' s', 'ho', 'es', ' w', 'it', 'h\\n', 're', 'd ', 'he', 'el', 's,', ' a', 'nd', ' w', 'he', 'n ', 'sh', 'e ', 'wa', 'lk', 'ed', ' o', 'ut', ' w', 'it', 'h ', 'th', 'em']\n",
      "\n",
      "Processing file stories/169.txt\n",
      "Data size (characters) (Document 168) 1644\n",
      "sample string (Documents 168) ['on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' t', 'he', ' l', 'it', 'tl', 'e ', 'he', 'n ', 'we', 'nt', ' w', 'it', 'h ', 'th', 'e ', 'li', 'tt', 'le', ' c', 'oc', 'k\\n', 'to', ' t', 'he', ' n', 'ut', '-h', 'il', 'l,', ' a', 'nd', ' t', 'he', 'y ', 'ag', 're', 'ed', ' t', 'og', 'et', 'he', 'r ']\n",
      "\n",
      "Processing file stories/170.txt\n",
      "Data size (characters) (Document 169) 2544\n",
      "sample string (Documents 169) ['on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' t', 'he', 're', ' w', 'as', ' a', ' m', 'an', ' w', 'ho', ' d', 'id', ' n', 'ot', 'hi', 'ng', ' b', 'ut', ' g', 'am', 'bl', 'e,', ' a', 'nd', '\\nf', 'or', ' t', 'ha', 't ', 're', 'as', 'on', ' p', 'eo', 'pl', 'e ', 'ne', 've', 'r ', 'ca', 'll', 'ed']\n",
      "\n",
      "Processing file stories/171.txt\n",
      "Data size (characters) (Document 170) 575\n",
      "sample string (Documents 170) ['th', 'e ', 'fo', 'x ', 'on', 'ce', ' c', 'am', 'e ', 'to', ' a', ' m', 'ea', 'do', 'w ', 'in', ' w', 'hi', 'ch', ' s', 'at', ' a', ' f', 'lo', 'ck', ' o', 'f ', 'fi', 'ne', ' f', 'at', '\\ng', 'ee', 'se', ', ', 'on', ' w', 'hi', 'ch', ' h', 'e ', 'sm', 'il', 'ed', ' a', 'nd', ' s', 'ai', 'd,', ' i']\n",
      "\n",
      "Processing file stories/172.txt\n",
      "Data size (characters) (Document 171) 4341\n",
      "sample string (Documents 171) ['in', ' o', 'ld', 'en', ' t', 'im', 'es', ', ', 'wh', 'en', ' t', 'he', ' l', 'or', 'd ', 'hi', 'ms', 'el', 'f ', 'st', 'il', 'l ', 'us', 'ed', ' t', 'o ', 'wa', 'lk', ' a', 'bo', 'ut', ' o', 'n\\n', 'th', 'is', ' e', 'ar', 'th', ' a', 'mo', 'ng', 'st', ' m', 'en', ', ', 'it', ' o', 'nc', 'e ', 'ha']\n",
      "\n",
      "Processing file stories/173.txt\n",
      "Data size (characters) (Document 172) 3480\n",
      "sample string (Documents 172) ['on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' l', 'iv', 'ed', ' a', ' p', 'ea', 'sa', 'nt', ' a', 'nd', ' h', 'is', ' w', 'if', 'e,', ' a', 'nd', ' t', 'he', ' p', 'ar', 'so', 'n\\n', 'of', ' t', 'he', ' v', 'il', 'la', 'ge', ' h', 'ad', ' a', ' f', 'an', 'cy', ' f', 'or', ' t', 'he', ' w', 'if']\n",
      "\n",
      "Processing file stories/174.txt\n",
      "Data size (characters) (Document 173) 2003\n",
      "sample string (Documents 173) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' p', 'oo', 'r ', 'pe', 'as', 'an', 't ', 'ca', 'll', 'ed', ' c', 'ra', 'bb', ', ', 'wh', 'o ', 'dr', 'ov', 'e\\n', 'wi', 'th', ' t', 'wo', ' o', 'xe', 'n ', 'a ', 'lo', 'ad', ' o', 'f ', 'wo', 'od', ' t', 'o ', 'th']\n",
      "\n",
      "Processing file stories/175.txt\n",
      "Data size (characters) (Document 174) 4494\n",
      "sample string (Documents 174) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' r', 'ic', 'h ', 'ma', 'n,', ' w', 'ho', ' h', 'ad', ' a', ' s', 'er', 'va', 'nt', ' w', 'ho', ' s', 'er', 've', 'd ', 'hi', 'm\\n', 'di', 'li', 'ge', 'nt', 'ly', ' a', 'nd', ' h', 'on', 'es', 'tl', 'y.', '  ', 'he', ' w', 'as', ' e', 've', 'ry', ' m']\n",
      "\n",
      "Processing file stories/176.txt\n",
      "Data size (characters) (Document 175) 932\n",
      "sample string (Documents 175) ['a ', 'co', 'un', 'tr', 'ym', 'an', ' w', 'as', ' o', 'nc', 'e ', 'go', 'in', 'g ', 'ou', 't ', 'to', ' p', 'lo', 'ug', 'h ', 'wi', 'th', ' a', ' p', 'ai', 'r ', 'of', ' o', 'xe', 'n.', '\\nw', 'he', 'n ', 'he', ' g', 'ot', ' t', 'o ', 'th', 'e ', 'fi', 'el', 'd,', ' b', 'ot', 'h ', 'th', 'e ', 'an']\n",
      "\n",
      "Processing file stories/177.txt\n",
      "Data size (characters) (Document 176) 2833\n",
      "sample string (Documents 176) ['th', 're', 'e ', 'ar', 'my', ' s', 'ur', 'ge', 'on', 's ', 'wh', 'o ', 'th', 'ou', 'gh', 't ', 'th', 'ey', ' k', 'ne', 'w ', 'th', 'ei', 'r ', 'ar', 't ', 'pe', 'rf', 'ec', 'tl', 'y\\n', 'we', 're', ' t', 'ra', 've', 'li', 'ng', ' a', 'bo', 'ut', ' t', 'he', ' w', 'or', 'ld', ', ', 'an', 'd ', 'th']\n",
      "\n",
      "Processing file stories/178.txt\n",
      "Data size (characters) (Document 177) 2821\n",
      "sample string (Documents 177) ['se', 've', 'n ', 'sw', 'ab', 'ia', 'ns', ' w', 'er', 'e ', 'on', 'ce', ' t', 'og', 'et', 'he', 'r.', '  ', 'th', 'e ', 'fi', 'rs', 't ', 'wa', 's ', 'ma', 'st', 'er', '\\ns', 'ch', 'ul', 'z,', ' t', 'he', ' s', 'ec', 'on', 'd,', ' j', 'ac', 'kl', 'i,', ' t', 'he', ' t', 'hi', 'rd', ', ', 'ma', 'rl']\n",
      "\n",
      "Processing file stories/179.txt\n",
      "Data size (characters) (Document 178) 3010\n",
      "sample string (Documents 178) ['th', 'er', 'e ', 'we', 're', ' o', 'nc', 'e ', 'th', 're', 'e ', 'ap', 'pr', 'en', 'ti', 'ce', 's,', ' w', 'ho', ' h', 'ad', ' a', 'gr', 'ee', 'd ', 'to', ' k', 'ee', 'p ', 'al', 'wa', 'ys', '\\nt', 'og', 'et', 'he', 'r ', 'wh', 'il', 'e ', 'tr', 'av', 'el', 'in', 'g,', ' a', 'nd', ' a', 'lw', 'ay']\n",
      "\n",
      "Processing file stories/180.txt\n",
      "Data size (characters) (Document 179) 1904\n",
      "sample string (Documents 179) ['in', ' a', ' c', 'er', 'ta', 'in', ' v', 'il', 'la', 'ge', ' t', 'he', 're', ' o', 'nc', 'e ', 'li', 've', 'd ', 'a ', 'ma', 'n ', 'an', 'd ', 'hi', 's ', 'wi', 'fe', ', ', 'an', 'd\\n', 'th', 'e ', 'wi', 'fe', ' w', 'as', ' s', 'o ', 'la', 'zy', ' t', 'ha', 't ', 'sh', 'e ', 'wo', 'ul', 'd ', 'ne']\n",
      "\n",
      "Processing file stories/181.txt\n",
      "Data size (characters) (Document 180) 1185\n",
      "sample string (Documents 180) ['a ', 'pe', 'as', 'an', 't ', 'ha', 'd ', 'a ', 'fa', 'it', 'hf', 'ul', ' h', 'or', 'se', ' w', 'hi', 'ch', ' h', 'ad', ' g', 'ro', 'wn', ' o', 'ld', ' a', 'nd', ' c', 'ou', 'ld', '\\nd', 'o ', 'no', ' m', 'or', 'e ', 'wo', 'rk', ', ', 'so', ' h', 'is', ' m', 'as', 'te', 'r ', 'wo', 'ul', 'd ', 'no']\n",
      "\n",
      "Processing file stories/182.txt\n",
      "Data size (characters) (Document 181) 955\n",
      "sample string (Documents 181) ['th', 'e ', 'lo', 'rd', ' g', 'od', ' h', 'ad', ' c', 're', 'at', 'ed', ' a', 'll', ' a', 'ni', 'ma', 'ls', ', ', 'an', 'd ', 'ha', 'd ', 'ch', 'os', 'en', ' o', 'ut', ' t', 'he', ' w', 'ol', 'f ', 'to', '\\nb', 'e ', 'hi', 's ', 'do', 'g,', ' b', 'ut', ' h', 'e ', 'ha', 'd ', 'fo', 'rg', 'ot', 'te']\n",
      "\n",
      "Processing file stories/183.txt\n",
      "Data size (characters) (Document 182) 723\n",
      "sample string (Documents 182) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' s', 'or', 'ce', 're', 'r ', 'wh', 'o ', 'wa', 's ', 'st', 'an', 'di', 'ng', ' i', 'n ', 'th', 'e ', 'mi', 'ds', 't ', 'of', ' a', '\\ng', 're', 'at', ' c', 'ro', 'wd', ' o', 'f ', 'pe', 'op', 'le', ' p', 'er', 'fo', 'rm', 'in', 'g ', 'hi', 's ', 'wo']\n",
      "\n",
      "Processing file stories/184.txt\n",
      "Data size (characters) (Document 183) 411\n",
      "sample string (Documents 183) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', 'n ', 'ol', 'd ', 'wo', 'ma', 'n,', ' b', 'ut', ' y', 'ou', ' h', 'av', 'e ', 'su', 're', 'ly', ' s', 'ee', 'n ', 'an', ' o', 'ld', '\\nw', 'om', 'an', ' g', 'o ', 'a-', 'be', 'gg', 'in', 'g ', 'be', 'fo', 're', ' n', 'ow', '. ', ' t', 'hi', 's ', 'wo']\n",
      "\n",
      "Processing file stories/185.txt\n",
      "Data size (characters) (Document 184) 509\n",
      "sample string (Documents 184) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' o', 'n ', 'a ', 'ti', 'me', ' a', ' m', 'ai', 'de', 'n ', 'wh', 'o ', 'wa', 's ', 'pr', 'et', 'ty', ', ', 'bu', 't ', 'id', 'le', '\\na', 'nd', ' n', 'eg', 'li', 'ge', 'nt', '. ', ' w', 'he', 'n ', 'sh', 'e ', 'ha', 'd ', 'to', ' s', 'pi', 'n ', 'sh', 'e ']\n",
      "\n",
      "Processing file stories/186.txt\n",
      "Data size (characters) (Document 185) 877\n",
      "sample string (Documents 185) ['in', ' t', 'he', ' t', 'im', 'e ', 'of', ' s', 'ch', 'la', 'ur', 'af', 'fe', 'n ', 'i ', 'we', 'nt', ' f', 'or', 'th', ' a', 'nd', ' s', 'aw', ' r', 'om', 'e ', 'an', 'd ', 'th', 'e\\n', 'la', 'te', 'ra', 'n ', 'ha', 'ng', 'in', 'g ', 'by', ' a', ' s', 'ma', 'll', ' s', 'il', 'ke', 'n ', 'th', 're']\n",
      "\n",
      "Processing file stories/187.txt\n",
      "Data size (characters) (Document 186) 534\n",
      "sample string (Documents 186) ['i ', 'wi', 'll', ' t', 'el', 'l ', 'yo', 'u ', 'so', 'me', 'th', 'in', 'g.', '  ', 'i ', 'sa', 'w ', 'tw', 'o ', 'ro', 'as', 'te', 'd ', 'fo', 'wl', 's ', 'fl', 'yi', 'ng', ',\\n', 'th', 'ey', ' f', 'le', 'w ', 'qu', 'ic', 'kl', 'y ', 'an', 'd ', 'ha', 'd ', 'th', 'ei', 'r ', 'br', 'ea', 'st', 's ']\n",
      "\n",
      "Processing file stories/188.txt\n",
      "Data size (characters) (Document 187) 327\n",
      "sample string (Documents 187) ['th', 're', 'e ', 'wo', 'me', 'n ', 'we', 're', ' t', 'ra', 'ns', 'fo', 'rm', 'ed', ' i', 'nt', 'o ', 'fl', 'ow', 'er', 's ', 'wh', 'ic', 'h ', 'gr', 'ew', ' i', 'n\\n', 'th', 'e ', 'fi', 'el', 'd,', ' b', 'ut', ' o', 'ne', ' o', 'f ', 'th', 'em', ' w', 'as', ' a', 'll', 'ow', 'ed', ' t', 'o ', 'be']\n",
      "\n",
      "Processing file stories/189.txt\n",
      "Data size (characters) (Document 188) 603\n",
      "sample string (Documents 188) ['ho', 'w ', 'fo', 'rt', 'un', 'at', 'e ', 'is', ' t', 'he', ' m', 'as', 'te', 'r,', ' a', 'nd', ' h', 'ow', ' w', 'el', 'l ', 'al', 'l ', 'go', 'es', ' i', 'n ', 'hi', 's\\n', 'ho', 'us', 'e,', ' w', 'he', 'n ', 'he', ' h', 'as', ' a', ' w', 'is', 'e ', 'se', 'rv', 'an', 't ', 'wh', 'o ', 'li', 'st']\n",
      "\n",
      "Processing file stories/190.txt\n",
      "Data size (characters) (Document 189) 655\n",
      "sample string (Documents 189) ['on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' p', 'oo', 'r ', 'pi', 'ou', 's ', 'pe', 'as', 'an', 't ', 'di', 'ed', ', ', 'an', 'd ', 'ar', 'ri', 've', 'd ', 'be', 'fo', 're', '\\nt', 'he', ' g', 'at', 'e ', 'of', ' h', 'ea', 've', 'n.', '  ', 'at', ' t', 'he', ' s', 'am', 'e ', 'ti', 'me']\n",
      "\n",
      "Processing file stories/191.txt\n",
      "Data size (characters) (Document 190) 1131\n",
      "sample string (Documents 190) ['le', 'an', ' l', 'is', 'a ', 'wa', 's ', 'of', ' a', ' v', 'er', 'y ', 'di', 'ff', 'er', 'en', 't ', 'wa', 'y ', 'of', ' t', 'hi', 'nk', 'in', 'g ', 'fr', 'om', ' l', 'az', 'y\\n', 'ha', 'rr', 'y ', 'an', 'd ', 'fa', 't ', 'tr', 'in', 'a,', ' w', 'ho', ' n', 'ev', 'er', ' l', 'et', ' a', 'ny', 'th']\n",
      "\n",
      "Processing file stories/192.txt\n",
      "Data size (characters) (Document 191) 983\n",
      "sample string (Documents 191) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' t', 'ai', 'lo', 'r,', ' w', 'ho', ' w', 'as', ' a', ' q', 'ua', 'rr', 'el', 'so', 'me', ' f', 'el', 'lo', 'w,', ' a', 'nd', '\\nh', 'is', ' w', 'if', 'e,', ' w', 'ho', ' w', 'as', ' g', 'oo', 'd,', ' i', 'nd', 'us', 'tr', 'io', 'us', ', ', 'an', 'd ']\n",
      "\n",
      "Processing file stories/193.txt\n",
      "Data size (characters) (Document 192) 3027\n",
      "sample string (Documents 192) ['in', ' o', 'ld', 'en', ' t', 'im', 'es', ' e', 've', 'ry', ' s', 'ou', 'nd', ' s', 'ti', 'll', ' h', 'ad', ' i', 'ts', ' m', 'ea', 'ni', 'ng', ' a', 'nd', ' s', 'ig', 'ni', 'fi', 'ca', 'nc', 'e.', '\\nw', 'he', 'n ', 'th', 'e ', 'sm', 'it', \"h'\", 's ', 'ha', 'mm', 'er', ' r', 'es', 'ou', 'nd', 'ed']\n",
      "\n",
      "Processing file stories/194.txt\n",
      "Data size (characters) (Document 193) 648\n",
      "sample string (Documents 193) ['th', 'e ', 'fi', 'sh', 'es', ' h', 'ad', ' f', 'or', ' a', ' l', 'on', 'g ', 'ti', 'me', ' b', 'ee', 'n ', 'di', 'sc', 'on', 'te', 'nt', 'ed', ' b', 'ec', 'au', 'se', ' n', 'o\\n', 'or', 'de', 'r ', 'pr', 'ev', 'ai', 'le', 'd ', 'in', ' t', 'he', 'ir', ' k', 'in', 'gd', 'om', '. ', ' n', 'on', 'e ']\n",
      "\n",
      "Processing file stories/195.txt\n",
      "Data size (characters) (Document 194) 620\n",
      "sample string (Documents 194) ['wh', 'er', 'e ', 'do', ' y', 'ou', ' l', 'ik', 'e ', 'be', 'st', ' t', 'o ', 'fe', 'ed', ' y', 'ou', 'r ', 'fl', 'oc', 'ks', ', ', 'sa', 'id', ' a', ' m', 'an', '\\nt', 'o ', 'an', ' o', 'ld', ' c', 'ow', 'he', 'rd', '. ', ' h', 'er', 'e,', ' s', 'ir', ', ', 'wh', 'er', 'e ', 'th', 'e ', 'gr', 'as']\n",
      "\n",
      "Processing file stories/196.txt\n",
      "Data size (characters) (Document 195) 2246\n",
      "sample string (Documents 195) ['tw', 'o ', 'or', ' t', 'hr', 'ee', ' h', 'un', 'dr', 'ed', ' y', 'ea', 'rs', ' a', 'go', ', ', 'wh', 'en', ' p', 'eo', 'pl', 'e ', 'we', 're', ' f', 'ar', ' f', 'ro', 'm ', 'be', 'in', 'g ', 'so', '\\nc', 'ra', 'ft', 'y ', 'an', 'd ', 'cu', 'nn', 'in', 'g ', 'as', ' t', 'he', 'y ', 'ar', 'e ', 'no']\n",
      "\n",
      "Processing file stories/197.txt\n",
      "Data size (characters) (Document 196) 2081\n",
      "sample string (Documents 196) ['in', ' d', 'ay', 's ', 'go', 'ne', ' b', 'y ', 'th', 'er', 'e ', 'wa', 's ', 'a ', 'la', 'nd', ' w', 'he', 're', ' t', 'he', ' n', 'ig', 'ht', 's ', 'we', 're', ' a', 'lw', 'ay', 's ', 'da', 'rk', ',\\n', 'an', 'd ', 'th', 'e ', 'sk', 'y ', 'sp', 're', 'ad', ' o', 've', 'r ', 'it', ' l', 'ik', 'e ']\n",
      "\n",
      "Processing file stories/198.txt\n",
      "Data size (characters) (Document 197) 1488\n",
      "sample string (Documents 197) ['wh', 'en', ' g', 'od', ' c', 're', 'at', 'ed', ' t', 'he', ' w', 'or', 'ld', ' a', 'nd', ' w', 'as', ' a', 'bo', 'ut', ' t', 'o ', 'fi', 'x ', 'th', 'e ', 'le', 'ng', 'th', ' o', 'f\\n', 'ea', 'ch', ' c', 're', 'at', 'ur', \"e'\", 's ', 'li', 'fe', ', ', 'th', 'e ', 'as', 's ', 'ca', 'me', ' a', 'nd']\n",
      "\n",
      "Processing file stories/199.txt\n",
      "Data size (characters) (Document 198) 1529\n",
      "sample string (Documents 198) ['in', ' a', 'nc', 'ie', 'nt', ' t', 'im', 'es', ' a', ' g', 'ia', 'nt', ' w', 'as', ' o', 'nc', 'e ', 'tr', 'av', 'el', 'in', 'g ', 'on', ' a', ' g', 're', 'at', ' h', 'ig', 'hw', 'ay', ',\\n', 'wh', 'en', ' s', 'ud', 'de', 'nl', 'y ', 'an', ' u', 'nk', 'no', 'wn', ' m', 'an', ' s', 'pr', 'an', 'g ']\n",
      "\n",
      "Processing file stories/200.txt\n",
      "Data size (characters) (Document 199) 3745\n",
      "sample string (Documents 199) ['ma', 'st', 'er', ' p', 'fr', 'ie', 'm ', 'wa', 's ', 'a ', 'sh', 'or', 't,', ' t', 'hi', 'n,', ' b', 'ut', ' l', 'iv', 'el', 'y ', 'ma', 'n,', ' w', 'ho', ' n', 'ev', 'er', '\\nr', 'es', 'te', 'd ', 'a ', 'mo', 'me', 'nt', '. ', ' h', 'is', ' f', 'ac', 'e,', ' o', 'f ', 'wh', 'ic', 'h ', 'hi', 's ']\n",
      "\n",
      "Processing file stories/201.txt\n",
      "Data size (characters) (Document 200) 2599\n",
      "sample string (Documents 200) ['a ', 'ta', 'il', 'or', ' a', 'nd', ' a', ' g', 'ol', 'ds', 'mi', 'th', ' w', 'er', 'e ', 'tr', 'av', 'el', 'in', 'g ', 'to', 'ge', 'th', 'er', ', ', 'an', 'd ', 'on', 'e ', 'ev', 'en', 'in', 'g\\n', 'wh', 'en', ' t', 'he', ' s', 'un', ' h', 'ad', ' s', 'un', 'k ', 'be', 'hi', 'nd', ' t', 'he', ' m']\n",
      "\n",
      "Processing file stories/202.txt\n",
      "Data size (characters) (Document 201) 1783\n",
      "sample string (Documents 201) ['a ', 'ce', 'rt', 'ai', 'n ', 'ta', 'il', 'or', ' w', 'ho', ' w', 'as', ' g', 're', 'at', ' a', 't ', 'bo', 'as', 'ti', 'ng', ' b', 'ut', ' i', 'll', ' a', 't ', 'do', 'in', 'g,', '\\nt', 'oo', 'k ', 'it', ' i', 'nt', 'o ', 'hi', 's ', 'he', 'ad', ' t', 'o ', 'go', ' a', 'br', 'oa', 'd ', 'fo', 'r ']\n",
      "\n",
      "Processing file stories/203.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size (characters) (Document 202) 681\n",
      "sample string (Documents 202) ['a ', 'me', 'rc', 'ha', 'nt', ' h', 'ad', ' d', 'on', 'e ', 'go', 'od', ' b', 'us', 'in', 'es', 's ', 'at', ' t', 'he', ' f', 'ai', 'r.', '  ', 'he', ' h', 'ad', ' s', 'ol', 'd ', 'hi', 's\\n', 'wa', 're', 's,', ' a', 'nd', ' l', 'in', 'ed', ' h', 'is', ' m', 'on', 'ey', '-b', 'ag', 's ', 'wi', 'th']\n",
      "\n",
      "Processing file stories/204.txt\n",
      "Data size (characters) (Document 203) 3501\n",
      "sample string (Documents 203) ['th', 'is', ' s', 'to', 'ry', ', ', 'my', ' d', 'ea', 'r ', 'yo', 'un', 'g ', 'fo', 'lk', 's,', ' s', 'ee', 'ms', ' t', 'o ', 'be', ' f', 'al', 'se', ', ', 'bu', 't ', 'it', ' r', 'ea', 'll', 'y\\n', 'is', ' t', 'ru', 'e,', ' f', 'or', ' m', 'y ', 'gr', 'an', 'df', 'at', 'he', 'r,', ' f', 'ro', 'm ']\n",
      "\n",
      "Processing file stories/205.txt\n",
      "Data size (characters) (Document 204) 978\n",
      "sample string (Documents 204) ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' f', 'ar', '-s', 'ig', 'ht', 'ed', ', ', 'cr', 'af', 'ty', ' p', 'ea', 'sa', 'nt', ' w', 'ho', 'se', '\\nt', 'ri', 'ck', 's ', 'we', 're', ' m', 'uc', 'h ', 'ta', 'lk', 'ed', ' a', 'bo', 'ut', '. ', ' t', 'he', ' b']\n",
      "\n",
      "Processing file stories/206.txt\n",
      "Data size (characters) (Document 205) 457\n",
      "sample string (Documents 205) ['ge', 'or', 'ge', ' o', 'ne', ' d', 'ay', ' s', 'ai', 'd ', 'to', ' h', 'is', ' l', 'it', 'tl', 'e ', 'ch', 'ic', 'ke', 'ns', ', ', 'co', 'me', ' i', 'nt', 'o ', 'th', 'e\\n', 'pa', 'rl', 'or', ' a', 'nd', ' e', 'nj', 'oy', ' y', 'ou', 'rs', 'el', 've', 's,', ' a', 'nd', ' p', 'ic', 'k ', 'up', ' t']\n",
      "\n",
      "Processing file stories/207.txt\n",
      "Data size (characters) (Document 206) 3822\n",
      "sample string (Documents 206) ['a ', 'ri', 'ch', ' f', 'ar', 'me', 'r ', 'wa', 's ', 'on', 'e ', 'da', 'y ', 'st', 'an', 'di', 'ng', ' i', 'n ', 'hi', 's ', 'ya', 'rd', ' i', 'ns', 'pe', 'ct', 'in', 'g ', 'hi', 's\\n', 'fi', 'el', 'ds', ' a', 'nd', ' g', 'ar', 'de', 'ns', '. ', ' t', 'he', ' c', 'or', 'n ', 'wa', 's ', 'gr', 'ow']\n",
      "\n",
      "Processing file stories/208.txt\n",
      "Data size (characters) (Document 207) 3783\n",
      "sample string (Documents 207) ['a ', 'so', 'ld', 'ie', 'r ', 'wh', 'o ', 'is', ' a', 'fr', 'ai', 'd ', 'of', ' n', 'ot', 'hi', 'ng', ', ', 'tr', 'ou', 'bl', 'es', ' h', 'im', 'se', 'lf', ' a', 'bo', 'ut', '\\nn', 'ot', 'hi', 'ng', '. ', ' o', 'ne', ' o', 'f ', 'th', 'is', ' k', 'in', 'd ', 'ha', 'd ', 're', 'ce', 'iv', 'ed', ' h']\n",
      "\n",
      "Processing file stories/209.txt\n",
      "Data size (characters) (Document 208) 461\n",
      "sample string (Documents 208) ['\\ni', 'n ', 'th', 'e ', 'wi', 'nt', 'er', ' t', 'im', 'e,', ' w', 'he', 'n ', 'de', 'ep', ' s', 'no', 'w ', 'la', 'y ', 'on', ' t', 'he', ' g', 'ro', 'un', 'd,', ' a', ' p', 'oo', 'r ', 'bo', 'y ', '\\nw', 'as', ' f', 'or', 'ce', 'd ', 'to', ' g', 'o ', 'ou', 't ', 'on', ' a', ' s', 'le', 'dg', 'e ']\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    \n",
    "    with open(filename) as f:\n",
    "        data = tf.compat.as_str(f.read())\n",
    "        #make all the words lowercaser\n",
    "        data = data.lower()\n",
    "        data = list(data)\n",
    "    return data\n",
    "\n",
    "documents = []\n",
    "global documents\n",
    "for i in range(num_files):\n",
    "    print('\\nProcessing file %s' %os.path.join(dir_name, filenames[i]))\n",
    "    chars = read_data(os.path.join(dir_name, filenames[i]))\n",
    "    \n",
    "    #break the data into bigrams\n",
    "    two_grams = [''.join(chars[ch_i:ch_i+2]) for ch_i in range(0, len(chars)-2, 2)]\n",
    "    #create a list of lists with bigrams\n",
    "    documents.append(two_grams)\n",
    "    print('Data size (characters) (Document %d) %d' %(i, len(two_grams)))\n",
    "    print('sample string (Documents %d) %s' %(i, two_grams[:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Dictionaries (Bigrams)\n",
    "Build the following. to understand each of these elements, let use also assume the test \"I like to go to school\"\n",
    "- dictionary : maps a string word to an ID . ({I:O, like:1, to:2, go:3, school:4})\n",
    "- reverse_dictionary: maps ID to words({0:I, 1:like, 2:to, 3:go, 4:school})\n",
    "- count: list of list of (word, frequency) elements (e.g [(I,1), (to,2), (go,1), (school,1)])\n",
    "- data: Contain the string of text we read, where string words are replaced with word IDs e.g [0, 1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "727505 Characters found.\n",
      "Most common words (+UNK) [('e ', 24554), ('he', 24203), (' t', 21726), ('th', 21031), ('d ', 16995)]\n",
      "Least common words (+UNK) [('md', 1), ('dt', 1), ('xu', 1), ('x-', 1), ('-.', 1), ('tp', 1), ('-j', 1), ('lg', 1), ('uj', 1), ('kd', 1), ('z.', 1), ('kt', 1), ('oj', 1), ('c-', 1), ('!\"', 1)]\n",
      "Sample data [16, 27, 88, 26, 3, 96, 72, 11, 2, 17]\n",
      "Sample data [23, 157, 25, 36, 78, 183, 42, 9, 87, 19]\n",
      "Vocabulary:  572\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(documents):\n",
    "    chars = []\n",
    "    # This is going to be a list of lists\n",
    "    # Where the outer list denote each document\n",
    "    # and the inner lists denote words in a given document\n",
    "    data_list = []\n",
    "  \n",
    "    for d in documents:\n",
    "        chars.extend(d)\n",
    "    print('%d Characters found.'%len(chars))\n",
    "    count = []\n",
    "    # Get the bigram sorted by their frequency (Highest comes first)\n",
    "    count.extend(collections.Counter(chars).most_common())\n",
    "    \n",
    "    # Create an ID for each bigram by giving the current length of the dictionary\n",
    "    # And adding that item to the dictionary\n",
    "    # Start with 'UNK' that is assigned to too rare words\n",
    "    dictionary = dict({'UNK':0})\n",
    "    for char, c in count:\n",
    "        # Only add a bigram to dictionary if its frequency is more than 10\n",
    "        if c > 10:\n",
    "            dictionary[char] = len(dictionary)    \n",
    "    \n",
    "    unk_count = 0\n",
    "    # Traverse through all the text we have\n",
    "    # to replace each string word with the ID of the word\n",
    "    for d in documents:\n",
    "        data = list()\n",
    "        for char in d:\n",
    "            # If word is in the dictionary use the word ID,\n",
    "            # else use the ID of the special token \"UNK\"\n",
    "            if char in dictionary:\n",
    "                index = dictionary[char]        \n",
    "            else:\n",
    "                index = dictionary['UNK']\n",
    "                unk_count += 1\n",
    "            data.append(index)\n",
    "            \n",
    "        data_list.append(data)\n",
    "        \n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "    return data_list, count, dictionary, reverse_dictionary\n",
    "\n",
    "global data_list, count, dictionary, reverse_dictionary,vocabulary_size\n",
    "\n",
    "# Print some statistics about data\n",
    "data_list, count, dictionary, reverse_dictionary = build_dataset(documents)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Least common words (+UNK)', count[-15:])\n",
    "print('Sample data', data_list[0][:10])\n",
    "print('Sample data', data_list[1][:10])\n",
    "print('Vocabulary: ',len(dictionary))\n",
    "vocabulary_size = len(dictionary)\n",
    "del documents  # To reduce memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Batches of Data\n",
    "The following object generates a batch of data which will be used to train the LSTM. More specifically the generator breaks a given sequence of words into batch_size segments. we also maintain a cursor of each segment. so whenever we create a bacth of data, we sample one item from each segment and update the cursor of each segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Unrolled index 0\n",
      "\tInputs:\n",
      "\te  (1), \tki (152), \t d (49), \t w (11), \tbe (69), \n",
      "\tOutput:\n",
      "\tli (98), \tng (34), \tau (215), \ter (13), \tau (215), \n",
      "\n",
      "Unrolled index 1\n",
      "\tInputs:\n",
      "\tli (98), \tng (34), \tau (215), \ter (13), \tau (215), \n",
      "\tOutput:\n",
      "\tve (43), \t\n",
      "w (167), \tgh (109), \te  (1), \tti (112), \n",
      "\n",
      "Unrolled index 2\n",
      "\tInputs:\n",
      "\tve (43), \t\n",
      "w (167), \tgh (109), \te  (1), \tti (112), \n",
      "\tOutput:\n",
      "\td  (5), \tho (61), \tte (62), \tal (80), \tfu (235), \n",
      "\n",
      "Unrolled index 3\n",
      "\tInputs:\n",
      "\td  (5), \tho (61), \tte (62), \tal (80), \tfu (235), \n",
      "\tOutput:\n",
      "\ta  (78), \tse (56), \trs (138), \tl  (57), \tl, (260), \n",
      "\n",
      "Unrolled index 4\n",
      "\tInputs:\n",
      "\ta  (78), \tse (56), \trs (138), \tl  (57), \tbe (69), \n",
      "\tOutput:\n",
      "\tki (152), \t d (49), \t w (11), \tbe (69), \tau (215), "
     ]
    }
   ],
   "source": [
    "class DataGeneratorOHE(object):\n",
    "    \n",
    "    def __init__(self, text, batch_size, num_unroll):\n",
    "        #Text where a bigram is denoted by its ID\n",
    "        self._text = text\n",
    "        #Number of bigrams in the text\n",
    "        self._text_size = len(self._text)\n",
    "        #number of data points ina batch of data\n",
    "        self._batch_size = batch_size\n",
    "        #Num unroll is the number of steps we unroll the RNN in a single training step\n",
    "        self._num_unroll = num_unroll\n",
    "        #we break the text into several segments and the batch of data is sampled by samplying a single item from a single segemnt\n",
    "        self._segments = self._text_size // self._batch_size\n",
    "        self._cursor = [offset * self._segments for offset in range(self._batch_size)]\n",
    "    \n",
    "    def next_batch(self):\n",
    "        '''Generates a single batch of data'''\n",
    "        #train inputs (one-hot-encoded)  and train outputs (one-hot-encode)\n",
    "        batch_data = np.zeros((self._batch_size, vocabulary_size), dtype=np.float32)\n",
    "        batch_labels = np.zeros((self._batch_size, vocabulary_size), dtype=np.float32)\n",
    "        \n",
    "        #fill in the batch datapoint by datapoint\n",
    "        for b in range(self._batch_size):\n",
    "            #if the cursor of a given segment exceeds the segment length\n",
    "            #we reset the cursor back to the beginning of that segment\n",
    "            if self._cursor[b]+1 >= self._text_size:\n",
    "                self._cursor[b] = b * self._segments\n",
    "                \n",
    "            #add the text at the cursor as the input\n",
    "            batch_data[b, self._text[self._cursor[b]]] = 1.0\n",
    "            #add the preceeding bigram as the label to be predicted\n",
    "            batch_labels[b, self._text[self._cursor[b]+1]] = 1.0\n",
    "            #update the cursor\n",
    "            self._cursor[b] = (self._cursor[b]+1)%self._text_size\n",
    "            \n",
    "        return batch_data, batch_labels\n",
    "    \n",
    "    def unroll_batches(self):\n",
    "        '''This produces a list of num_unroll batches as required by a single step of training the RNN'''\n",
    "        \n",
    "        unroll_data, unroll_labels = [], []\n",
    "        for ui in range(self._num_unroll):\n",
    "            data, labels= self.next_batch()\n",
    "            unroll_data.append(data)\n",
    "            unroll_labels.append(labels)\n",
    "            \n",
    "        return unroll_data, unroll_labels\n",
    "    \n",
    "    def reset_indices(self):\n",
    "        '''used to reset all the cursors if needed'''\n",
    "        self._cursor = [offset * self._segments for offset in range(self._batch_size)]\n",
    "        \n",
    "# running a tiny set to see if things are correct\n",
    "dg = DataGeneratorOHE(data_list[0][25:50], 5, 5)\n",
    "u_data, u_labels =dg.unroll_batches()\n",
    "\n",
    "# Iterate through each data batch in the unrolled set of batches\n",
    "for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):   \n",
    "    print('\\n\\nUnrolled index %d'%ui)\n",
    "    dat_ind = np.argmax(dat,axis=1)\n",
    "    lbl_ind = np.argmax(lbl,axis=1)\n",
    "    print('\\tInputs:')\n",
    "    for sing_dat in dat_ind:\n",
    "        print('\\t%s (%d)'%(reverse_dictionary[sing_dat],sing_dat),end=\", \")\n",
    "    print('\\n\\tOutput:')\n",
    "    for sing_lbl in lbl_ind:        \n",
    "        print('\\t%s (%d)'%(reverse_dictionary[sing_lbl],sing_lbl),end=\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the LSTM\n",
    "This is a standard LSTM. the LSTM has 5 main component\n",
    "- Cell state\n",
    "- Hidden state\n",
    "- Input gate\n",
    "- Forget gate\n",
    "- output gate\n",
    "\n",
    "Each gate has three sets of weights (1 set for the current input, 1 set for the previous hidden state and 1 bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining hyperparameters\n",
    "Here we define several hyperparameters. However additionally we use dropout; a technique that helps to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of neurons in the hidden state varibles\n",
    "num_nodes = 128\n",
    "\n",
    "#number of data points in a batch we proces\n",
    "batch_size = 64\n",
    "\n",
    "#number of times steps we unroll for during optimization\n",
    "num_unrollings = 50\n",
    "\n",
    "dropout = 0.0 \n",
    "\n",
    "#use this in the scv filename when saving \n",
    "filename_extension = ''\n",
    "if dropout > 0.0:\n",
    "    filename_extension = '_dropout'\n",
    "\n",
    "filename_to_save = 'lstm'+filename_extension+'.csv' # use to save perplexity values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Inputs and Outputs\n",
    "\n",
    "In the code we define two different types of inputs. \n",
    "* Training inputs (The stories we downloaded) (batch_size > 1 with unrolling)\n",
    "* Validation inputs (An unseen validation dataset) (bach_size =1, no unrolling)\n",
    "* Test input (New story we are going to generate) (batch_size=1, no unrolling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#training input data.\n",
    "train_inputs, train_labels = [],[]\n",
    "\n",
    "#defining unrolled training inputs\n",
    "for ui in range(num_unrollings):\n",
    "    train_inputs.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size], name='train_inputs_%d'%ui))\n",
    "    train_labels.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size], name='train_labels_%d'%ui))\n",
    "    \n",
    "    #validation data placeholders\n",
    "    valid_inputs = tf.placeholder(tf.float32, shape=[1, vocabulary_size], name='valid_inputs')\n",
    "    valid_labels = tf.placeholder(tf.float32, shape=[1, vocabulary_size], name='valid_labels')\n",
    "    \n",
    "    #text generation: batch 1, no unrolling\n",
    "    test_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size], name='test_input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Model Parameters\n",
    "\n",
    "Now we define model parameters. Compared to RNNs, LSTMs have a large number of parameters. Each gate (input, forget, memory and output) has three different sets of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input gate (i_t) - How much memory to write to cell state\n",
    "#connect the current input to the input gate\n",
    "ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.02))\n",
    "#connects the prvious hidden state to the input gate\n",
    "im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.02))\n",
    "#bais of the input gate\n",
    "ib = tf.Variable(tf.random_uniform([1, num_nodes], -0.02, 0.02))\n",
    "\n",
    "#Forget gate (f_t) - How much memoery is discard from the cell state\n",
    "# connect the current input to the forget gate\n",
    "fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.02))\n",
    "#connects the prvious hidden state to the forget gate\n",
    "fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.02))\n",
    "#bais of the forget gate\n",
    "fb = tf.Variable(tf.random_uniform([1, num_nodes], -0.02, 0.02))\n",
    "\n",
    "#Candidate value (c~_t) - used to compute the current cell state\n",
    "# connect the current input to the candidate\n",
    "cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.02))\n",
    "#connects the prvious hidden state to the candidate\n",
    "cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.02))\n",
    "#bais of the candidate\n",
    "cb = tf.Variable(tf.random_uniform([1, num_nodes], -0.02, 0.02))\n",
    "\n",
    "\n",
    "# Output gate (o_t) - How much memory to output from the cell state\n",
    "# Connects the current input to the output gate\n",
    "ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.02))\n",
    "# Connects the previous hidden state to the output gate\n",
    "om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.02))\n",
    "# Bias of the output gate\n",
    "ob = tf.Variable(tf.random_uniform([1, num_nodes],-0.02,0.02))\n",
    "\n",
    "#softmax Classifier weights and biases.\n",
    "w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], stddev=0.02))\n",
    "b = tf.Variable(tf.random_uniform([1, num_nodes], -0.02, 0.02))\n",
    "\n",
    "# Variables saving state across unrollings.\n",
    "# Hidden state\n",
    "saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False, name='train_hidden')\n",
    "# Cell state\n",
    "saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False, name='train_cell')\n",
    "\n",
    "# Same variables for validation phase\n",
    "saved_valid_output = tf.Variable(tf.zeros([1, num_nodes]),trainable=False, name='valid_hidden')\n",
    "saved_valid_state = tf.Variable(tf.zeros([1, num_nodes]),trainable=False, name='valid_cell')\n",
    "\n",
    "# Same variables for testing phase\n",
    "saved_test_output = tf.Variable(tf.zeros([1, num_nodes]),trainable=False, name='test_hidden')\n",
    "saved_test_state = tf.Variable(tf.zeros([1, num_nodes]),trainable=False, name='test_cell')\n",
    "\n",
    "\n",
    "# Softmax Classifier weights and biases.\n",
    "w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], stddev=0.02)) \n",
    "b = tf.Variable(tf.random_uniform([vocabulary_size],-0.02,0.02))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining LSTM Computations\n",
    "Here first we define the LSTM cell computations as a consice function. Then we use this function to define training and test-time inference logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definition of the cell computation\n",
    "def lstm_cell(i, o, state):\n",
    "    \"\"\"create an lstm cell\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i,ox) + tf.matmul(o, om) + ob)\n",
    "    \n",
    "    return output_gate * tf.tanh(state), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Softmax_16:0\", shape=(1, 572), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "#Training related inference logic\n",
    "\n",
    "# Keeps the calculated state outputs in all the unrollings\n",
    "# Used to calculate loss\n",
    "outputs = list()\n",
    "\n",
    "#These two python variables are iteratively updated at each step of unrolling\n",
    "output = saved_output\n",
    "state = saved_state\n",
    "\n",
    "#compute the hidden state (output) and cell state (state)\n",
    "# recursively for all the teps in unrolling\n",
    "for i in train_inputs: \n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    output = tf.nn.dropout(output, keep_prob=1.0-dropout)\n",
    "    #append eachcomputed output vale\n",
    "    outputs.append(output)\n",
    "    \n",
    "#calculate the score values\n",
    "logits = tf.matmul(tf.concat(axis=0, values=outputs), w) + b\n",
    "\n",
    "# Compute predictions.\n",
    "train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Compute training perplexity\n",
    "train_perplexity_without_exp = tf.reduce_sum(tf.concat(train_labels,0)*-tf.log(tf.concat(train_prediction,0)+1e-10))/(num_unrollings*batch_size)\n",
    "\n",
    "# =====================================================================\n",
    "# validation phase related inference logic\n",
    "\n",
    "#compute the LSTM cell output for validation data\n",
    "valid_output, valid_state = lstm_cell(valid_inputs, saved_valid_output, saved_valid_state)\n",
    "\n",
    "#compute the logits\n",
    "valid_logits = tf.nn.xw_plus_b(valid_output, w, b)\n",
    "\n",
    "#compute training perplexity \n",
    "train_perplexity_without_exp = tf.reduce_sum(tf.concat(train_labels,0)*-tf.log(tf.concat(train_prediction,0)+1e-10))/(num_unrollings*batch_size)\n",
    "\n",
    "#Make sure that the state variables are updated\n",
    "#before moving on to the next iteration of generation\n",
    "with tf.control_dependencies([saved_valid_output.assign(valid_output), \n",
    "                            saved_valid_state.assign(valid_state)]):\n",
    "    valid_prediction = tf.nn.softmax(valid_logits)\n",
    "\n",
    "\n",
    "# Compute validation perplexity\n",
    "valid_perplexity_without_exp = tf.reduce_sum(valid_labels*-tf.log(valid_prediction+1e-10))\n",
    "\n",
    "# ========================================================================\n",
    "# Testing phase related inference logic\n",
    "\n",
    "# Compute the LSTM cell output for testing data\n",
    "test_output, test_state = lstm_cell(test_input, saved_test_output, saved_test_state)\n",
    "\n",
    "#compute test logit\n",
    "test_logits = tf.nn.xw_plus_b(test_output, w, b)\n",
    "\n",
    "#Make sure that the state variables are updated\n",
    "#before moving on to the next iteration of generation\n",
    "with tf.control_dependencies([saved_test_output.assign(test_output),\n",
    "                            saved_test_state.assign(test_state)]):\n",
    "    test_prediction = tf.nn.softmax(test_logits)\n",
    "print(test_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating LSTM Loss\n",
    "we calculate the training loss of the LSTM. It's a typical cross entropy loss calculated over all the scores we obtained for training data(loss) '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add_3259:0\", shape=(3200, 572), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# before calculating the training loss,\n",
    "#save the hidden state and the cell state to their respective Tensorflow variables\n",
    "with tf.control_dependencies([saved_output.assign(output), \n",
    "                              saved_state.assign(state)]):\n",
    "    #calculate the training loss by concatenation the results form all the unrolled time steps\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits = logits, labels=tf.concat(axis=0, values=train_labels))\n",
    "    )\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Learning Rate and the Optimizer with Gradient Clipping\n",
    "Here we define the learning rate and the optimizer we're going to use. We will be using Adam optimizer. We use gradient clipping to prevent any gradient explosions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"Adam_1\"\n",
      "op: \"NoOp\"\n",
      "input: \"^Adam_1/update_Variable_16/ApplyAdam\"\n",
      "input: \"^Adam_1/update_Variable_17/ApplyAdam\"\n",
      "input: \"^Adam_1/update_Variable_18/ApplyAdam\"\n",
      "input: \"^Adam_1/update_Variable_19/ApplyAdam\"\n",
      "input: \"^Adam_1/update_Variable_20/ApplyAdam\"\n",
      "input: \"^Adam_1/update_Variable_21/ApplyAdam\"\n",
      "input: \"^Adam_1/update_Variable_22/ApplyAdam\"\n",
      "input: \"^Adam_1/update_Variable_23/ApplyAdam\"\n",
      "input: \"^Adam_1/update_Variable_24/ApplyAdam\"\n",
      "input: \"^Adam_1/update_Variable_25/ApplyAdam\"\n",
      "input: \"^Adam_1/update_Variable_26/ApplyAdam\"\n",
      "input: \"^Adam_1/update_Variable_27/ApplyAdam\"\n",
      "input: \"^Adam_1/update_Variable_30/ApplyAdam\"\n",
      "input: \"^Adam_1/update_Variable_31/ApplyAdam\"\n",
      "input: \"^Adam_1/Assign\"\n",
      "input: \"^Adam_1/Assign_1\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#learning rate decay\n",
    "gstep  = tf.Variable(0, trainable=False, name='global_step')\n",
    "\n",
    "#Running this operation will cause the value of gstep to increase, while in turn reducing the learning rate\n",
    "inc_gstep = tf.assign(gstep, gstep+1)\n",
    "\n",
    "# decays learning rate everytime the gstep increase\n",
    "tf_learning_rate = tf.train.exponential_decay(0.001, gstep,\n",
    "                                             decay_steps=1, decay_rate=0.5)\n",
    "#Adam optimizer and gradient clipping\n",
    "optimizer = tf.train.AdamOptimizer(tf_learning_rate)\n",
    "gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "#clipping gradients\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "\n",
    "optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v)\n",
    ")\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resetting Operations for Resetting Hidden States\n",
    "Sometimes the state variable needs to be reset (e.g. when starting predictions at a beginning of a new epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset train state\n",
    "reset_train_state = tf.group(tf.assign(saved_state, tf.zeros([batch_size, num_nodes])),\n",
    "                            tf.assign(saved_output, tf.zeros([batch_size, num_nodes])))\n",
    "\n",
    "#Reset valid state\n",
    "reset_valid_state = tf.group(tf.assign(saved_valid_state, tf.zeros([1, num_nodes])),\n",
    "                            tf.assign(saved_valid_output, tf.zeros([1, num_nodes])))\n",
    "# Reset test state\n",
    "reset_test_state = tf.group(tf.assign(saved_test_output.assign(tf.random_normal([1, num_nodes], stddev=0.05)),\n",
    "                                     saved_test_state.assign(tf.random_normal([1, num_nodes], stddev=0.05))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy Sampling to Break the Repetition\n",
    "Here we write some simple logic to break the repetition in text. Specifically instead of always getting the word that gave this highest prediction probability, we sample randomly where the probability of being selected given by their prediction probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(distribution):\n",
    "    '''Greedy sampling\n",
    "    We pick the three best predictions given by the LSTM and sample one of them with very high \n",
    "    probability of picking the best one'''\n",
    "    \n",
    "    best_inds = np.argsort(distribution)[-3:]\n",
    "    best_probs = distribution[best_inds] / np.sum(distribution[best_inds])\n",
    "    best_idx = np.random.choice(best_inds, p=best_probs)\n",
    "    return best_idx\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the LSTM to Generate Text\n",
    "Here we train the LSTM on the available data and generate text using the trained LSTM for several steps. From each document we extract text for steps_per_document steps to train the LSTM on. We also report the train perplexity at the end of each step. Finally we test the LSTM by asking it to generate some new text starting from a randomly picked bigram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate Decay Logic\n",
    "Here we define the logic to decrease learning rate whenever the validation perplexity does not decrease\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate decay related\n",
    "# If valid perpelxity does not decrease\n",
    "# continuously for this many epochs\n",
    "# decrease the learning rate\n",
    "decay_threshold = 5\n",
    "# Keep counting perplexity increases\n",
    "decay_count = 0\n",
    "\n",
    "min_perplexity = 1e10\n",
    "\n",
    "# Learning rate decay logic\n",
    "def decay_learning_rate(session, v_perplexity):\n",
    "    global decay_threshold, decay_count, min_perplexity  \n",
    "    # Decay learning rate\n",
    "    if v_perplexity < min_perplexity:\n",
    "        decay_count = 0\n",
    "        min_perplexity= v_perplexity\n",
    "    else: \n",
    "        decay_count += 1\n",
    "    \n",
    "    if decay_count >= decay_threshold:\n",
    "        print('\\t Reducing learning rate')\n",
    "        decay_count = 0\n",
    "        \n",
    "        session.run(inc_gstep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/online1/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py:1714: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Global Variables \n",
      "Training (Step: 0) (36).(98).(73).(94).(61).(88).(47).(45).(21).(24).\n",
      "Average loss at step 1: 4.407770\n",
      "\tPerplexity at step 1: 82.086177\n",
      "\n",
      "Valid Perplexity: 67.70\n",
      "\n",
      "Generated Text after epoch 0 ... \n",
      "======================== New text Segment ==========================\n",
      "\t irstard ther, and the to then the poped said, and the pome, and the came, and ther, but he was he pome to be had the did, and now he had the dredersed the was he pome the was his was they the poped they his whould the took the was his his was the was he pome he the pome, and there the poped him the pope, which the pope, and the pord, and that then he whoulders, what his his he pome the was then he had to the pome that the to to then his his had he took to be to be to the to to be the was the do the to as the pord, and not the to the was the dide he was his to the hen he was his the don, and the pome then the pome his to the pord of he his was he the to as the\n",
      "dere, and said, and the pope.  and did, but the poped the pome he the to the was his had the was he hen in to then, what he poped the poped the hereing the card, and\n",
      "him the pome, and said him what his in he pome, and said the was he the ders, and the pord, and not the poped the pome, and that it the pord, and said he pope, and the \n",
      "====================================================================\n",
      "\n",
      "Training (Step: 1) (29).(18).(1).(61).(2).(72).(34).(21).(5).(90).\n",
      "Average loss at step 2: 3.167796\n",
      "\tPerplexity at step 2: 23.755081\n",
      "\n",
      "Valid Perplexity: 42.11\n",
      "\n",
      "Generated Text after epoch 1 ... \n",
      "======================== New text Segment ==========================\n",
      "\t nd him, said him, and wants the ded had his little and the wasant, and and then the devil into beat have her and staked the king they to to be bece of she staing the was some for thereat then the\n",
      "devils.  they day daughtered him and and as daughter, and when the wast light it was set with reat and there, and when the was standing was the great him, and then the king to the king him, said, and when his had but the will that he devil that still said that his had and said the king himsever the wated him to him to the king then him the greek that she him ans son thered him, and they ing the great him little was stained, and the ded way of thereing the greet they saway, when he was stlet that his thereed, and wanted that the great hall and to the greet that the waster that his to the king to on, and then he said the will that said the great reme threw he was solemned, and when he devil the devils whim to they daugh, and and the said, \"then stime and said his son the great rejoicing the king t\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 2) (45).(36).(14).(44).(2).(21).(77).(30).(22).(81).\n",
      "Average loss at step 3: 2.534515\n",
      "\tPerplexity at step 3: 12.610317\n",
      "\n",
      "Valid Perplexity: 39.69\n",
      "\n",
      "Generated Text after epoch 2 ... \n",
      "======================== New text Segment ==========================\n",
      "\t , saw a beof shole, and\n",
      "rough the king tim and thereat the sto to your bade for that into the made cansed it, the killed the stork came to the canded to the god and of the mento it.  and were was to the there, and asked the king and there was forest, and tailor to his forse.  and the sailor, into the king and that was a was into the town to to be all stailor to the king as one of her, and not the king had eat the thim to him, and the killed him, and the king and thereater thered back, and said,\n",
      "he, the\n",
      "home to ough he has now his head so once that the king and stork whole were disticked the little tailor to the whild on his with the should not was a find and time.\n",
      "\n",
      "he could had back, the whole was the stown he was not ing ass that of the men the king, who the tailor had wile begarrow, and that the the stailor, and not was head back hered the\n",
      "tailor, and not the tailor, and now him and said the said to him.  he was to time the tailor hade took he had not the tailor, and the tailor the sho\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 3) (45).(13).(52).(9).(81).(88).(63).(94).(10).(11).\n",
      "Average loss at step 4: 2.279176\n",
      "\tPerplexity at step 4: 9.768623\n",
      "\n",
      "Valid Perplexity: 38.57\n",
      "\n",
      "Generated Text after epoch 3 ... \n",
      "======================== New text Segment ==========================\n",
      "\t read.  they were no bettle they were of it, she hansel.  when she weved now stit, but\n",
      "they had night, she were a but in the ground, answered to children or fait, the woman, and head nother, and now that leat her walked, and\n",
      "they threw away sat doore.  then she cood to her which apple, she\n",
      "was in the old woman which awas and gretel, and they had no more perce and my were to the gretel which which it is spinning beards, therried to the room, and now the hansel, and they had to me, witch the with the food for father, and whosoever not in and foress when a she wittle duck and greated him all good ther.  they were herself of a light of the old woman, and\n",
      "tone her pear it oncers nessh in they the witch's forest forest.  she was about in she old woman, they arept nother, and hansel it hearls is then him.  now what said, in help i agan her in that do be the with, should not but of the wicked, that is alkeep and was all said, how do a great forest, there hansel, she had to\n",
      "the litter to the good \n",
      "====================================================================\n",
      "\n",
      "Training (Step: 4) (79).(42).(70).(69).(3).(32).(26).(11).(90).(54).\n",
      "Average loss at step 5: 2.109303\n",
      "\tPerplexity at step 5: 8.242492\n",
      "\n",
      "Valid Perplexity: 54.48\n",
      "\n",
      "Generated Text after epoch 4 ... \n",
      "======================== New text Segment ==========================\n",
      "\t on that misfortune we are accustomed,\n",
      "than give up our beger to misfortune we arem, and sund hersed the still of longers to the misfortune home will.  at lace was she were rom than there was as were, he sproting third and the palace, and killed that shound as if you care alred the\n",
      "casted were stirst.  in the\n",
      "mings.  that she would herself serting, she sprang safely out of\n",
      "the window, but the besiegers did not leave out to that, and killed sit so hery torthand the madelished so counly man, she sprancerve had promiced up, and\n",
      "that she was song force. that the still belet the councillers he could herse.  but the besiege did not leave off until the whole\n",
      "paracacd stood to leat the king and it made to the man, and sood the griend, and up she sicend to the of them.  the gonest\n",
      "dece out of mes such as once out, for the condof them the still, and lainster her come muse, and cathers athey couse not the ground.  when in he shall yield bearle.  and when the fire\n",
      "reached the room and the cat uncerpe\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 5) (34).(76).(55).(79).(64).(90).(14).(62).(42).(92).\n",
      "Average loss at step 6: 1.763157\n",
      "\tPerplexity at step 6: 5.830819\n",
      "\n",
      "Valid Perplexity: 51.97\n",
      "\n",
      "Generated Text after epoch 5 ... \n",
      "======================== New text Segment ==========================\n",
      "\t econd a little\n",
      "cone gold\n",
      "with it.  then he was a king's son, and away the dond still by with the sole.   and event i will give houted into the roon, and she hanseself, fore\n",
      "int them, and he said, but the married the god took the dooson, and when she raided\n",
      "how, and had he lived\n",
      "hand took could king's day more there, and the mong and like to her against to her liket of get to so, and set a little taid, and said me, and head twould be and lose this she chanted it that it the ring, who is as if the seven he comented to may of to trees, and lose and chant came to passin bacson, and the took to her with a birds, and i will ented to a beate\n",
      "on a tree, and so the was away into the room.\n",
      "\n",
      "now the ring, but the given shind that and\n",
      "that so litking the ther do, which as stood bead against it.  and looked for a chea, which is so goroad, whic soon as she will form with it.\" but an she thankly sack, and said, \"o sill said,\n",
      "\"you have delivered the charch mant see the brant, and wisteded, liked them fo\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 6) (70).(0).(64).(88).(14).(76).(74).(6).(47).(82).\n",
      "Average loss at step 7: 1.637778\n",
      "\tPerplexity at step 7: 5.143727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valid Perplexity: 44.27\n",
      "\n",
      "Generated Text after epoch 6 ... \n",
      "======================== New text Segment ==========================\n",
      "\t t of the fat nothingen herwas and cried, it in the miden from white way prand indow and lighted that the heard that she toged to her that the king's spikes, and she was\n",
      "at last her came, and the king ast once had bedpick that he had goitting the heard kingdom from much in thek down in the king's peachack them dow, and\n",
      "heard the for her lorst to great for\n",
      "him, they that he shall\n",
      "defive his was as, and then the hedgehog's skin the redgehog's daughter would conformed.  the bate and was to be there by which oner to the royal pangeraced to him his so may only to betting there was a lastone, and the hedgehog hans the hedgehog, however, by that in the father was\n",
      "ate and to by the fidger the hedgesiant told she came to the royal palace.  but the king camed him with the king and celle it and life a mangewas to be to\n",
      "him, the isling a cone by\n",
      "himself on the kingdom from the age.  then she reventme coulded his sleep.  the king asked his wards, hant see shorls, but thesself a butside her canted to h\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 7) (0).(88).(23).(60).(34).(66).(84).(74).(48).(7).\n",
      "Average loss at step 8: 1.693258\n",
      "\tPerplexity at step 8: 5.437168\n",
      "\n",
      "Valid Perplexity: 48.38\n",
      "\n",
      "Generated Text after epoch 7 ... \n",
      "======================== New text Segment ==========================\n",
      "\t  shall witch oppery had gone this head and that had been hand, shat i can'\n",
      "he of got in, and so disceldes only a leat to feath.  then the king was the king that he had met walked the king said, he time to see hersome for the watch beautiful come. i cell not to the air, and said, i with the joyelt man, and be door whiched but, he went into the king where it in was the batten that the bar iffeather of her, and said, that had been was to the borned.\n",
      "and she deverything said, you could not me.ught her daughter was\n",
      "bed.\n",
      "the queen beard then to to her,\n",
      "and they that the king who haveled it to\n",
      " such as had and shen shall beautiful bovery shall, she saw it must be\n",
      "quick, and said, i have been in\n",
      "then the king could not come.  but the quieut was wict this the king who havel it into a strece.  the nowf came and sawn the hundled out the hunging, and she answented.  when the king was the king into the hunden the king been was\n",
      "gladinto the fire quickly with the bird,\n",
      "and said, you spromis mould not g\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 8) (11).(95).(84).(38).(66).(60).(93).(15).(72).(47).\n",
      "Average loss at step 9: 1.684774\n",
      "\tPerplexity at step 9: 5.391234\n",
      "\n",
      "Valid Perplexity: 56.55\n",
      "\n",
      "\t Reducing learning rate\n",
      "Generated Text after epoch 8 ... \n",
      "======================== New text Segment ==========================\n",
      "\t  was suffer a swelf.  when they all that is at\n",
      "lone came from sleep, and those who had been turned to\n",
      "stone received once more their natural forms.\n",
      "\n",
      "simpleton married the youngest and sweetest princess, and after her\n",
      "father's death became king, and his two brothers received the two\n",
      "othe call of the had been right for thrdi dound the two brothers, the bearn looked\n",
      "at a sonce complacdid him onde with the pearls, and he gother exactly, what out sitting in the meat\n",
      "sibe away toave it to the heor was wask to his bed.\n",
      "\n",
      "as the most.  the went up the second duttle was the enchanted into him the mosted to before the encesnted the birst lit, in the watch, they cried him to became to hiy the elsed on the leaves were so a coof sone of the right palked on there again, the he came to the two brothers, he came king, and his tas and the dexght day, but should be know to be two morey, and after hon lettle came turt the bein would be from the manted insidestered brothers, and they were shappy smith fell, \n",
      "====================================================================\n",
      "\n",
      "Training (Step: 9) (96).(89).(40).(87).(32).(65).(68).(43).(71).(48).\n",
      "Average loss at step 10: 1.980322\n",
      "\tPerplexity at step 10: 7.245073\n",
      "\n",
      "Valid Perplexity: 41.10\n",
      "\n",
      "Generated Text after epoch 9 ... \n",
      "======================== New text Segment ==========================\n",
      "\t  father into the little old killed he with his two one and heard of a ring count out of the water.  the quieut of the came at insimpleto as that he said, \"i woman they flew the king together about in a piece their death of dress heads on the some cann.  the\n",
      "finderlace worse.\n",
      "    s not\n",
      "with at soon at the that the two at and brought that, coulsesed that the two brothers wered ners, it whose that the kingdom has now that they with all the one want to are in the mostleg, he said, from heard the road of the wathe whom he had grown, and led her to chaved he had grown to no one who had been led to you.  when he had grown in hers and the those of the kingdom to bright and could that, so all and said the two elde of that, she said to her, and lood wife again, and said,\n",
      "ell sight what he that it in all on her into then the two peasanting the out one out on the water of the maining the kingdoms, but a reconey, and the turnle he said, have whon he who the king for you must belf and the face's daugh\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 10) (98).(85).(14).(73).(66).(91).(16).(89).(2).(76).\n",
      "Average loss at step 11: 2.146448\n",
      "\tPerplexity at step 11: 8.554419\n",
      "\n",
      "Valid Perplexity: 35.06\n",
      "\n",
      "Generated Text after epoch 10 ... \n",
      "======================== New text Segment ==========================\n",
      "\t r before and was to seekin, and then sent ward, and all at live that the willor coved ther walld, lew at has angest, will sideed, and at last the childrke, and saw that in you have pleased her the forner the unnine bother the will bent, and they sat all the said, i will chambed it a first to be a huntsman, and said,\n",
      "UNKuse that,\n",
      "but the three rave\n",
      "should stree to to leavor, and then at the chanh had he benear a chardo, and hast beneached the are and had beg them at armantle\n",
      "the restied, and wher and was all that,\n",
      "but he had three\n",
      "searth a coares, and thread it to no purposed from her in the morning the fox on the son and cried, \"the\n",
      "dost by home with all and cries wered annon sat that in you alartly howevered in his anger was to being\n",
      "intoe and remained preceiven again.  that is sent and saw that, and they remained to everything at the beards with to betten, and said,\n",
      " for you, and then rushen was a maid,\n",
      "UNKust they, said he, and struck it, but he said, at one ar forth their pardon.  and \n",
      "====================================================================\n",
      "\n",
      "Training (Step: 11) (22).(48).(57).(7).(65).(68).(14).(27).(6).(82).\n",
      "Average loss at step 12: 1.982770\n",
      "\tPerplexity at step 12: 7.262831\n",
      "\n",
      "Valid Perplexity: 28.31\n",
      "\n",
      "Generated Text after epoch 11 ... \n",
      "======================== New text Segment ==========================\n",
      "\t o with hers nought.  they tore is he whole was lay, and then he was to\n",
      "himself the way, has not not only to the first had bring only no becomed him to a head that the wedding was.  then she sat no me.  but he had gone men\n",
      "and came in his\n",
      "hedgehold in her mares of his\n",
      "ban had brinder, should not have.  that thushed the king had come and happened of her to go with\n",
      "her, and reathe hannow, that he was welonly, that the forest and come thought her some form of the cock and wither, what is whe courtyard on that he wanted it into the cantle, that he would likewise thought with her arm\n",
      "as he was again.  then she was to stably together,\n",
      "why are\n",
      "and onged they would ran as hans, i had soon recame and\n",
      "never had been the wholeften her father, i have not\n",
      "become a hunger.  when the redgehog as if on her resened one the way, how was\n",
      "celebrather was havereat how how his pocking, and the\n",
      "fishtions would been shors, and that he theye, she sswed, and he would.  shave you cones, but on the hedge with his pi\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 12) (58).(35).(96).(79).(22).(52).(56).(3).(18).(66).\n",
      "Average loss at step 13: 1.715380\n",
      "\tPerplexity at step 13: 5.558785\n",
      "\n",
      "Valid Perplexity: 32.14\n",
      "\n",
      "Generated Text after epoch 12 ... \n",
      "======================== New text Segment ==========================\n",
      "\t nto whole said, i am no lighted her husband they ring-cleaten about the hearn saw have again, and drink, but he was only a shall every said, and when\n",
      "the\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bird by the bailiff.  what was shut bride long come to the box on the ears, and put as if he would but when the king's ate wilk.  when she have her beautiful drins went into the mill-stouchen heard it out, and went out of it, he tred on the second time, he will was to the birst.\n",
      "\n",
      "at the wedding outside.  then they wed them, he was standing about him the have had grown, said if\n",
      "not, was away to them, and have him not no longer and bed, how he\n",
      "had a little, and\n",
      "ther, but reat of her fare of the mill,\n",
      "too, he\n",
      "said, no, no on her own form and ring four-footed the heares, the had one your father in some\n",
      "he, said, i have mading he at last bire arest back for hards, and began to the night, for the others, and said, that they were hon walk to him, but, i have along of whom to gare him again she was to anythen she which and her which was away in\n",
      "====================================================================\n",
      "\n",
      "Training (Step: 13) (57).(20).(62).(13).(18).(72).(19).(26).(43).(71).\n",
      "Average loss at step 14: 1.873695\n",
      "\tPerplexity at step 14: 6.512316\n",
      "\n",
      "Valid Perplexity: 32.58\n",
      "\n",
      "Generated Text after epoch 13 ... \n",
      "======================== New text Segment ==========================\n",
      "\t , thanchaid, the striking is the water, and then she him, and he did not by a wolf to the fishertaken to them of them to her back again, when the maiden went to the other, and then she tood the fish, and she was to the forched to the king's daughted, and said to furest, who was quige.  when she\n",
      "was going to the great.  the old wome\n",
      "into his head.  and he strench again him.\n",
      "  then in the daughters of the world to the fisherman and the gread stretched out\n",
      "to a great nothing, and thought the children want with him the king was and was streaced to a stone of the little was steaped to drittened the driUNK            your nowever, and that in her son and\n",
      "that, it was so to be a little house, and with killever a conders with his grown, and the children happig out of the forest, went in the forest, and when she thought the water, he wates and befored to the three of you will not some ind the fish went to turse.  the old woman told himself, be with the mout was too, be sides of the old not somep, \n",
      "====================================================================\n",
      "\n",
      "Training (Step: 14) (73).(63).(88).(82)."
     ]
    }
   ],
   "source": [
    "# Some hyperparameters needed for the training process\n",
    "\n",
    "num_steps = 26\n",
    "steps_per_document = 100\n",
    "valid_summary = 1\n",
    "train_doc_count = 100\n",
    "docs_per_step = 10\n",
    "\n",
    "# Capture the behavior of train perplexity over time\n",
    "train_perplexity_ot = []\n",
    "valid_perplexity_ot = []\n",
    "\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "# Initializing variables\n",
    "tf.global_variables_initializer().run()\n",
    "print('Initialized Global Variables ')\n",
    "\n",
    "average_loss = 0 # Calculates the average loss ever few steps\n",
    "\n",
    "# We use the first 10 documents that has \n",
    "# more than 10*steps_per_document bigrams for creating the validation dataset\n",
    "\n",
    "# Identify the first 10 documents following the above condition\n",
    "long_doc_ids = []\n",
    "for di in range(num_files):\n",
    "  if len(data_list[di])>10*steps_per_document:\n",
    "    long_doc_ids.append(di)\n",
    "  if len(long_doc_ids)==10:\n",
    "    break\n",
    "    \n",
    "# Generating validation data\n",
    "data_gens = []\n",
    "valid_gens = []\n",
    "for fi in range(num_files):\n",
    "  # Get all the bigrams if the document id is not in the validation document ids\n",
    "  if fi not in long_doc_ids:\n",
    "    data_gens.append(DataGeneratorOHE(data_list[fi],batch_size,num_unrollings))\n",
    "  # if the document is in the validation doc ids, only get up to the \n",
    "  # last steps_per_document bigrams and use the last steps_per_document bigrams as validation data\n",
    "  else:\n",
    "    data_gens.append(DataGeneratorOHE(data_list[fi][:-steps_per_document],batch_size,num_unrollings))\n",
    "    # Defining the validation data generator\n",
    "    valid_gens.append(DataGeneratorOHE(data_list[fi][-steps_per_document:],1,1))\n",
    "\n",
    "\n",
    "feed_dict = {}\n",
    "for step in range(num_steps):\n",
    "    \n",
    "    print('Training (Step: %d)'%step,end=' ')\n",
    "    for di in np.random.permutation(train_doc_count)[:docs_per_step]:            \n",
    "        doc_perplexity = 0\n",
    "        for doc_step_id in range(steps_per_document):\n",
    "            \n",
    "            # Get a set of unrolled batches\n",
    "            u_data, u_labels = data_gens[di].unroll_batches()\n",
    "            \n",
    "            # Populate the feed dict by using each of the data batches\n",
    "            # present in the unrolled data\n",
    "            for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):            \n",
    "                feed_dict[train_inputs[ui]] = dat\n",
    "                feed_dict[train_labels[ui]] = lbl\n",
    "            \n",
    "            # Running the TensorFlow operations\n",
    "            _, l, step_perplexity = session.run([optimizer, loss, train_perplexity_without_exp], \n",
    "                                                       feed_dict=feed_dict)\n",
    "            # Update doc_perpelxity variable\n",
    "            doc_perplexity += step_perplexity \n",
    "            \n",
    "            # Update the average_loss variable\n",
    "            average_loss += step_perplexity \n",
    "            \n",
    "        # Show the printing progress <train_doc_id_1>.<train_doc_id_2>. ...\n",
    "        print('(%d).'%di,end='') \n",
    "    \n",
    "        # resetting hidden state after processing a single document\n",
    "        # It's still questionable if this adds value in terms of learning\n",
    "        # One one hand it's intuitive to reset the state when learning a new document\n",
    "        # On the other hand this approach creates a bias for the state to be zero\n",
    "        # We encourage the reader to investigate further the effect of resetting the state\n",
    "        #session.run(reset_train_state) # resetting hidden state for each document\n",
    "    print('')\n",
    "    \n",
    "    # Generate new samples\n",
    "    if (step+1) % valid_summary == 0:\n",
    "      \n",
    "      # Compute average loss\n",
    "      average_loss = average_loss / (valid_summary*docs_per_step*steps_per_document)\n",
    "      \n",
    "      # Print losses\n",
    "      print('Average loss at step %d: %f' % (step+1, average_loss))\n",
    "      print('\\tPerplexity at step %d: %f' %(step+1, np.exp(average_loss)))\n",
    "      train_perplexity_ot.append(np.exp(average_loss))\n",
    "        \n",
    "      average_loss = 0 # reset loss\n",
    "      valid_loss = 0 # reset loss\n",
    "        \n",
    "      # calculate valid perplexity\n",
    "      for v_doc_id in range(10):\n",
    "          # Remember we process things as bigrams\n",
    "          # So need to divide by 2\n",
    "          for v_step in range(steps_per_document//2):\n",
    "            uvalid_data,uvalid_labels = valid_gens[v_doc_id].unroll_batches()        \n",
    "\n",
    "            # Run validation phase related TensorFlow operations       \n",
    "            v_perp = session.run(\n",
    "                valid_perplexity_without_exp,\n",
    "                feed_dict = {valid_inputs:uvalid_data[0],valid_labels: uvalid_labels[0]}\n",
    "            )\n",
    "\n",
    "            valid_loss += v_perp\n",
    "            \n",
    "          session.run(reset_valid_state)\n",
    "      \n",
    "          # Reset validation data generator cursor\n",
    "          valid_gens[v_doc_id].reset_indices()      \n",
    "    \n",
    "      print()\n",
    "      v_perplexity = np.exp(valid_loss/(steps_per_document*10.0//2))\n",
    "      print(\"Valid Perplexity: %.2f\\n\"%v_perplexity)\n",
    "      valid_perplexity_ot.append(v_perplexity)\n",
    "          \n",
    "      decay_learning_rate(session, v_perplexity)\n",
    "        \n",
    "      # Generating new text ...\n",
    "      # We will be generating one segment having 500 bigrams\n",
    "      # Feel free to generate several segments by changing\n",
    "      # the value of segments_to_generate\n",
    "      print('Generated Text after epoch %d ... '%step)  \n",
    "      segments_to_generate = 1\n",
    "      chars_in_segment = 500\n",
    "    \n",
    "      for _ in range(segments_to_generate):\n",
    "        print('======================== New text Segment ==========================')\n",
    "        \n",
    "        # Start with a random word\n",
    "        test_word = np.zeros((1,vocabulary_size),dtype=np.float32)\n",
    "        rand_doc = data_list[np.random.randint(0,num_files)]\n",
    "        test_word[0,rand_doc[np.random.randint(0,len(rand_doc))]] = 1.0\n",
    "        print(\"\\t\",reverse_dictionary[np.argmax(test_word[0])],end='')\n",
    "        \n",
    "        # Generating words within a segment by feeding in the previous prediction\n",
    "        # as the current input in a recursive manner\n",
    "        for _ in range(chars_in_segment):    \n",
    "          sample_pred = session.run(test_prediction, feed_dict = {test_input:test_word})  \n",
    "          next_ind = sample(sample_pred.ravel())\n",
    "          test_word = np.zeros((1,vocabulary_size),dtype=np.float32)\n",
    "          test_word[0,next_ind] = 1.0\n",
    "          print(reverse_dictionary[next_ind],end='')\n",
    "        print(\"\")\n",
    "        \n",
    "        # Reset train state\n",
    "        session.run(reset_test_state)\n",
    "        print('====================================================================')\n",
    "      print(\"\")\n",
    "\n",
    "session.close()\n",
    "\n",
    "# Write training and validation perplexities to a csv file\n",
    "with open(filename_to_save, 'wt') as f:\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    writer.writerow(train_perplexity_ot)\n",
    "    writer.writerow(valid_perplexity_ot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM with Beam-Search\n",
    "Here we alter the previously defined prediction related TensorFlow operations to employ beam-search. Beam search is a way of predicting several time steps ahead. Concretely instead of predicting the best prediction we have at a given time step, we get predictions for several time steps and get the sequence of highest joint probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_length = 5 # number of steps to look ahead\n",
    "beam_neighbors = 5 # number of neighbors to compare to at each step\n",
    "\n",
    "# We redefine the sample generation with beam search\n",
    "sample_beam_inputs = [tf.placeholder(tf.float32, shape=[1, vocabulary_size]) for _ in range(beam_neighbors)]\n",
    "\n",
    "best_beam_index = tf.placeholder(shape=None, dtype=tf.int32)\n",
    "best_neighbor_beam_indices = tf.placeholder(shape=[beam_neighbors], dtype=tf.int32)\n",
    "\n",
    "# Maintains output of each beam\n",
    "saved_sample_beam_output = [tf.Variable(tf.zeros([1, num_nodes])) for _ in range(beam_neighbors)]\n",
    "# Maintains the state of each beam\n",
    "saved_sample_beam_state = [tf.Variable(tf.zeros([1, num_nodes])) for _ in range(beam_neighbors)]\n",
    "\n",
    "# Resetting the sample beam states (should be done at the beginning of each text snippet generation)\n",
    "reset_sample_beam_state = tf.group(\n",
    "    *[saved_sample_beam_output[vi].assign(tf.zeros([1, num_nodes])) for vi in range(beam_neighbors)],\n",
    "    *[saved_sample_beam_state[vi].assign(tf.zeros([1, num_nodes])) for vi in range(beam_neighbors)]\n",
    ")\n",
    "\n",
    "# We stack them to perform gather operation below\n",
    "stacked_beam_outputs = tf.stack(saved_sample_beam_output)\n",
    "stacked_beam_states = tf.stack(saved_sample_beam_state)\n",
    "\n",
    "# The beam states for each beam (there are beam_neighbor-many beams) needs to be updated at every depth of tree\n",
    "# Consider an example where you have 3 classes where we get the best two neighbors (marked with star)\n",
    "#     a`      b*       c  \n",
    "#   / | \\   / | \\    / | \\\n",
    "#  a  b c  a* b` c  a  b  c\n",
    "# Since both the candidates from level 2 comes from the parent b\n",
    "# We need to update both states/outputs from saved_sample_beam_state/output to have index 1 (corresponding to parent b)\n",
    "update_sample_beam_state = tf.group(\n",
    "    *[saved_sample_beam_output[vi].assign(tf.gather_nd(stacked_beam_outputs,[best_neighbor_beam_indices[vi]])) for vi in range(beam_neighbors)],\n",
    "    *[saved_sample_beam_state[vi].assign(tf.gather_nd(stacked_beam_states,[best_neighbor_beam_indices[vi]])) for vi in range(beam_neighbors)]\n",
    ")\n",
    "\n",
    "# We calculate lstm_cell state and output for each beam\n",
    "sample_beam_outputs, sample_beam_states = [],[] \n",
    "for vi in range(beam_neighbors):\n",
    "    tmp_output, tmp_state = lstm_cell(\n",
    "        sample_beam_inputs[vi], saved_sample_beam_output[vi], saved_sample_beam_state[vi]\n",
    "    )\n",
    "    sample_beam_outputs.append(tmp_output)\n",
    "    sample_beam_states.append(tmp_state)\n",
    "\n",
    "# For a given set of beams, outputs a list of prediction vectors of size beam_neighbors\n",
    "# each beam having the predictions for full vocabulary\n",
    "sample_beam_predictions = []\n",
    "for vi in range(beam_neighbors):\n",
    "    with tf.control_dependencies([saved_sample_beam_output[vi].assign(sample_beam_outputs[vi]),\n",
    "                                saved_sample_beam_state[vi].assign(sample_beam_states[vi])]):\n",
    "        sample_beam_predictions.append(tf.nn.softmax(tf.nn.xw_plus_b(sample_beam_outputs[vi], w, b)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the LSTM with Beam Search to Generate Text\n",
    "Here we train the LSTM on the available data and generate text using the trained LSTM for several steps. From each document we extract text for steps_per_document steps to train the LSTM on. We also report the train perplexity at the end of each step. Finally we test the LSTM by asking it to generate some new text with beam search starting from a randomly picked bigram.\n",
    "\n",
    "### Learning rate Decay Logic\n",
    "Here we define the logic to decrease learning rate whenever the validation perplexity does not decrease\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate decay related\n",
    "# If valid perpelxity does not decrease\n",
    "# continuously for this many epochs\n",
    "# decrease the learning rate\n",
    "decay_threshold = 5\n",
    "# Keep counting perplexity increases\n",
    "decay_count = 0\n",
    "\n",
    "min_perplexity = 1e10\n",
    "\n",
    "# Learning rate decay logic\n",
    "def decay_learning_rate(session, v_perplexity):\n",
    "  global decay_threshold, decay_count, min_perplexity  \n",
    "  # Decay learning rate\n",
    "  if v_perplexity < min_perplexity:\n",
    "    decay_count = 0\n",
    "    min_perplexity= v_perplexity\n",
    "  else:\n",
    "    decay_count += 1\n",
    "\n",
    "  if decay_count >= decay_threshold:\n",
    "    print('\\t Reducing learning rate')\n",
    "    decay_count = 0\n",
    "    session.run(inc_gstep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Beam Prediction Logic\n",
    "Here we define function that takes in the session as an argument and output a beam of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_word = None\n",
    "\n",
    "def get_beam_prediction(session):\n",
    "    \n",
    "    # Generating words within a segment with Beam Search\n",
    "    # To make some calculations clearer, we use the example as follows\n",
    "    # We have three classes with beam_neighbors=2 (best candidate denoted by *, second best candidate denoted by `)\n",
    "    # For simplicity we assume best candidate always have probability of 0.5 in output prediction\n",
    "    # second best has 0.2 output prediction\n",
    "    #           a`                   b*                   c                <--- root level\n",
    "    #    /     |     \\         /     |     \\        /     |     \\   \n",
    "    #   a      b      c       a*     b`     c      a      b      c         <--- depth 1\n",
    "    # / | \\  / | \\  / | \\   / | \\  / | \\  / | \\  / | \\  / | \\  / | \\\n",
    "    # a b c  a b c  a b c   a*b c  a`b c  a b c  a b c  a b c  a b c       <--- depth 2\n",
    "    # So the best beams at depth 2 would be\n",
    "    # b-a-a and b-b-a\n",
    "        \n",
    "    global test_word\n",
    "    global sample_beam_predictions\n",
    "    global update_sample_beam_state\n",
    "    \n",
    "    # Calculate the candidates at the root level\n",
    "    feed_dict = {}\n",
    "    for b_n_i in range(beam_neighbors):\n",
    "        feed_dict.update({sample_beam_inputs[b_n_i]: test_word})\n",
    "\n",
    "    # We calculate sample predictions for all neighbors with the same starting word/character\n",
    "    # This is important to update the state for all instances of beam search\n",
    "    sample_preds_root = session.run(sample_beam_predictions, feed_dict = feed_dict)  \n",
    "    sample_preds_root = sample_preds_root[0]\n",
    "\n",
    "    # indices of top-k candidates\n",
    "    # b and a in our example (root level)\n",
    "    this_level_candidates =  (np.argsort(sample_preds_root,axis=1).ravel()[::-1])[:beam_neighbors].tolist() \n",
    "\n",
    "    # probabilities of top-k candidates\n",
    "    # 0.5 and 0.2\n",
    "    this_level_probs = sample_preds_root[0,this_level_candidates] \n",
    "\n",
    "    # Update test sequence produced by each beam from the root level calculation\n",
    "    # Test sequence looks like for our example (at root)\n",
    "    # [b,a]\n",
    "    test_sequences = ['' for _ in range(beam_neighbors)]\n",
    "    for b_n_i in range(beam_neighbors):\n",
    "        test_sequences[b_n_i] += reverse_dictionary[this_level_candidates[b_n_i]]\n",
    "\n",
    "    # Make the calculations for the rest of the depth of the beam search tree\n",
    "    for b_i in range(beam_length-1):\n",
    "        test_words = [] # candidate words for each beam\n",
    "        pred_words = [] # Predicted words of each beam\n",
    "\n",
    "        # computing feed_dict for the beam search (except root)\n",
    "        # feed dict should contain the best words/chars/bigrams found by the previous level of search\n",
    "\n",
    "        # For level 1 in our example this would be\n",
    "        # sample_beam_inputs[0]: b, sample_beam_inputs[1]:a\n",
    "        feed_dict = {}\n",
    "        for p_idx, pred_i in enumerate(this_level_candidates):                    \n",
    "            # Updating the feed_dict for getting next predictions\n",
    "            test_words.append(np.zeros((1,vocabulary_size),dtype=np.float32))\n",
    "            test_words[p_idx][0,this_level_candidates[p_idx]] = 1.0\n",
    "\n",
    "            feed_dict.update({sample_beam_inputs[p_idx]:test_words[p_idx]})\n",
    "\n",
    "        # Calculating predictions for all neighbors in beams\n",
    "        # This is a list of vectors where each vector is the prediction vector for a certain beam\n",
    "        # For level 1 in our example, the prediction values for \n",
    "        #      b             a  (previous beam search results)\n",
    "        # [a,  b,  c],  [a,  b,  c] (current level predictions) would be\n",
    "        # [0.1,0.1,0.1],[0.5,0.2,0]\n",
    "        sample_preds_all_neighbors = session.run(sample_beam_predictions, feed_dict=feed_dict)\n",
    "\n",
    "        # Create a single vector with \n",
    "        # Making our example [0.1,0.1,0.1,0.5,0.2,0] \n",
    "        sample_preds_all_neighbors_concat = np.concatenate(sample_preds_all_neighbors,axis=1)\n",
    "\n",
    "        # Update this_level_candidates to be used for the next iteration\n",
    "        # And update the probabilities for each beam\n",
    "        # In our example these would be [3,4] (indices with maximum value from above vector)\n",
    "        this_level_candidates = np.argsort(sample_preds_all_neighbors_concat.ravel())[::-1][:beam_neighbors]\n",
    "\n",
    "        # In the example this would be [1,1]\n",
    "        parent_beam_indices = this_level_candidates//vocabulary_size\n",
    "\n",
    "        # normalize this_level_candidates to fall between [0,vocabulary_size]\n",
    "        # In this example this would be [0,1]\n",
    "        this_level_candidates = (this_level_candidates%vocabulary_size).tolist()\n",
    "\n",
    "        # Here we update the final state of each beam to be\n",
    "        # the state that was at the index 1. Because for both the candidates at this level the parent is \n",
    "        # at index 1 (that is b from root level)\n",
    "        session.run(update_sample_beam_state, feed_dict={best_neighbor_beam_indices: parent_beam_indices})\n",
    "\n",
    "        # Here we update the joint probabilities of each beam and add the newly found candidates to the sequence\n",
    "        tmp_this_level_probs = np.asarray(this_level_probs) #This is currently [0.5,0.2]\n",
    "        tmp_test_sequences = list(test_sequences) # This is currently [b,a]\n",
    "\n",
    "        for b_n_i in range(beam_neighbors):\n",
    "            # We make the b_n_i element of this_level_probs to be the probability of parents\n",
    "            # In the example the parent indices are [1,1]\n",
    "            # So this_level_probs become [0.5,0.5]\n",
    "            this_level_probs[b_n_i] = tmp_this_level_probs[parent_beam_indices[b_n_i]]\n",
    "\n",
    "            # Next we multipyle these by the probabilities of the best candidates from current level \n",
    "            # [0.5*0.5, 0.5*0.2] = [0.25,0.1]\n",
    "            this_level_probs[b_n_i] *= sample_preds_all_neighbors[parent_beam_indices[b_n_i]][0,this_level_candidates[b_n_i]]\n",
    "\n",
    "            # Make the b_n_i element of test_sequences to be the correct parent of the current best candidates\n",
    "            # In the example this becomes [b, b]\n",
    "            test_sequences[b_n_i] = tmp_test_sequences[parent_beam_indices[b_n_i]]\n",
    "\n",
    "            # Now we append the current best candidates\n",
    "            # In this example this becomes [ba,bb]\n",
    "            test_sequences[b_n_i] += reverse_dictionary[this_level_candidates[b_n_i]]\n",
    "\n",
    "            # Create one-hot-encoded representation for each candidate\n",
    "            pred_words.append(np.zeros((1,vocabulary_size),dtype=np.float32))\n",
    "            pred_words[b_n_i][0,this_level_candidates[b_n_i]] = 1.0\n",
    "\n",
    "    # Calculate best beam id based on the highest beam probability\n",
    "    # Using the highest beam probability always lead to very monotonic text\n",
    "    # Let us sample one randomly where one being sampled is decided by the likelihood of that beam\n",
    "    rand_cand_ids = np.argsort(this_level_probs)[-3:]\n",
    "    rand_cand_probs = this_level_probs[rand_cand_ids]/np.sum(this_level_probs[rand_cand_ids])\n",
    "    random_id = np.random.choice(rand_cand_ids,p=rand_cand_probs)\n",
    "\n",
    "    best_beam_id = parent_beam_indices[random_id]\n",
    "\n",
    "    # Update state and output variables for test prediction\n",
    "    session.run(update_sample_beam_state,feed_dict={best_neighbor_beam_indices:[best_beam_id for _ in range(beam_neighbors)]})\n",
    "\n",
    "    # Make the last word/character/bigram from the best beam\n",
    "    test_word = pred_words[best_beam_id]\n",
    "    \n",
    "    return test_sequences[best_beam_id]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Training, Validation and Generation\n",
    "We traing the LSTM on existing training data, check the validaiton perplexity on an unseen chunk of text and generate a fresh segment of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_to_save = 'lstm_beam_search_dropout'\n",
    "\n",
    "# Some hyperparameters needed for the training process\n",
    "\n",
    "num_steps = 26\n",
    "steps_per_document = 100\n",
    "valid_summary = 1\n",
    "train_doc_count = 100\n",
    "docs_per_step = 10\n",
    "\n",
    "\n",
    "beam_nodes = []\n",
    "\n",
    "beam_train_perplexity_ot = []\n",
    "beam_valid_perplexity_ot = []\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "print('Initialized')\n",
    "average_loss = 0\n",
    "\n",
    "# We use the first 10 documents that has \n",
    "# more than 10*steps_per_document bigrams for creating the validation dataset\n",
    "\n",
    "# Identify the first 10 documents following the above condition\n",
    "long_doc_ids = []\n",
    "for di in range(num_files):\n",
    "  if len(data_list[di])>10*steps_per_document:\n",
    "    long_doc_ids.append(di)\n",
    "  if len(long_doc_ids)==10:\n",
    "    break\n",
    "    \n",
    "# Generating validation data\n",
    "data_gens = []\n",
    "valid_gens = []\n",
    "for fi in range(num_files):\n",
    "  # Get all the bigrams if the document id is not in the validation document ids\n",
    "  if fi not in long_doc_ids:\n",
    "    data_gens.append(DataGeneratorOHE(data_list[fi],batch_size,num_unrollings))\n",
    "  # if the document is in the validation doc ids, only get up to the \n",
    "  # last steps_per_document bigrams and use the last steps_per_document bigrams as validation data\n",
    "  else:\n",
    "    data_gens.append(DataGeneratorOHE(data_list[fi][:-steps_per_document],batch_size,num_unrollings))\n",
    "    # Defining the validation data generator\n",
    "    valid_gens.append(DataGeneratorOHE(data_list[fi][-steps_per_document:],1,1))\n",
    "\n",
    "\n",
    "feed_dict = {}\n",
    "for step in range(num_steps):\n",
    "    \n",
    "    for di in np.random.permutation(train_doc_count)[:docs_per_step]:            \n",
    "        doc_perplexity = 0\n",
    "        for doc_step_id in range(steps_per_document):\n",
    "            \n",
    "            # Get a set of unrolled batches\n",
    "            u_data, u_labels = data_gens[di].unroll_batches()\n",
    "            \n",
    "            # Populate the feed dict by using each of the data batches\n",
    "            # present in the unrolled data\n",
    "            for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):            \n",
    "                feed_dict[train_inputs[ui]] = dat\n",
    "                feed_dict[train_labels[ui]] = lbl\n",
    "            \n",
    "            # Running the TensorFlow operations\n",
    "            _, l, step_perplexity = session.run([optimizer, loss, train_perplexity_without_exp], \n",
    "                                                       feed_dict=feed_dict)\n",
    "            # Update doc_perpelxity variable\n",
    "            doc_perplexity += step_perplexity \n",
    "            \n",
    "            # Update the average_loss variable\n",
    "            average_loss += step_perplexity \n",
    "            \n",
    "        # Show the printing progress <train_doc_id_1>.<train_doc_id_2>. ...\n",
    "        print('(%d).'%di,end='') \n",
    "    \n",
    "    # resetting hidden state after processing a single document\n",
    "    # It's still questionable if this adds value in terms of learning\n",
    "    # One one hand it's intuitive to reset the state when learning a new document\n",
    "    # On the other hand this approach creates a bias for the state to be zero\n",
    "    # We encourage the reader to investigate further the effect of resetting the state\n",
    "    #session.run(reset_train_state) # resetting hidden state for each document\n",
    "    print('')\n",
    "    \n",
    "    if (step+1) % valid_summary == 0:\n",
    "      \n",
    "      # Compute average loss\n",
    "      average_loss = average_loss / (docs_per_step*steps_per_document*valid_summary)\n",
    "      \n",
    "      # Print loss\n",
    "      print('Average loss at step %d: %f' % (step+1, average_loss))\n",
    "      print('\\tPerplexity at step %d: %f' %(step+1, np.exp(average_loss)))\n",
    "      beam_train_perplexity_ot.append(np.exp(average_loss))\n",
    "    \n",
    "      average_loss = 0 # reset loss\n",
    "        \n",
    "      valid_loss = 0 # reset loss\n",
    "        \n",
    "      # calculate valid perplexity\n",
    "      for v_doc_id in range(10):\n",
    "          # Remember we process things as bigrams\n",
    "          # So need to divide by 2\n",
    "          for v_step in range(steps_per_document//2):\n",
    "            uvalid_data,uvalid_labels = valid_gens[v_doc_id].unroll_batches()        \n",
    "\n",
    "            # Run validation phase related TensorFlow operations       \n",
    "            v_perp = session.run(\n",
    "                valid_perplexity_without_exp,\n",
    "                feed_dict = {valid_inputs:uvalid_data[0],valid_labels: uvalid_labels[0]}\n",
    "            )\n",
    "\n",
    "            valid_loss += v_perp\n",
    "            \n",
    "          session.run(reset_valid_state)\n",
    "      \n",
    "          # Reset validation data generator cursor\n",
    "          valid_gens[v_doc_id].reset_indices()      \n",
    "    \n",
    "      print()\n",
    "      v_perplexity = np.exp(valid_loss/(steps_per_document*10.0//2))\n",
    "      print(\"Valid Perplexity: %.2f\\n\"%v_perplexity)\n",
    "      beam_valid_perplexity_ot.append(v_perplexity)\n",
    "      \n",
    "      # Decay learning rate\n",
    "      decay_learning_rate(session, v_perplexity)\n",
    "    \n",
    "      # Generating new text ...\n",
    "      # We will be generating one segment having 500 bigrams\n",
    "      # Feel free to generate several segments by changing\n",
    "      # the value of segments_to_generate\n",
    "      print('Generated Text after epoch %d ... '%step)  \n",
    "      segments_to_generate = 1\n",
    "      chars_in_segment = 500//beam_length\n",
    "    \n",
    "      for _ in range(segments_to_generate):\n",
    "        print('======================== New text Segment ==========================')\n",
    "        # first word randomly generated\n",
    "        test_word = np.zeros((1,vocabulary_size),dtype=np.float32)\n",
    "        rand_doc = data_list[np.random.randint(0,num_files)]\n",
    "        test_word[0,rand_doc[np.random.randint(0,len(rand_doc))]] = 1.0\n",
    "        print(\"\",reverse_dictionary[np.argmax(test_word[0])],end='')\n",
    "        \n",
    "        for _ in range(chars_in_segment):\n",
    "            \n",
    "            test_sequence = get_beam_prediction(session)\n",
    "            print(test_sequence,end='')\n",
    "            \n",
    "        print(\"\")\n",
    "        session.run([reset_sample_beam_state])\n",
    "        \n",
    "        print('====================================================================')\n",
    "      print(\"\")\n",
    "\n",
    "session.close()\n",
    "    \n",
    "with open(filename_to_save, 'wt') as f:\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    writer.writerow(beam_train_perplexity_ot)\n",
    "    writer.writerow(beam_valid_perplexity_ot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
